{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "iris k-fold & best model 찾기(효진).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1dR7UWB0AOgmXIgnf1VkLUcaFSATlrisd",
      "authorship_tag": "ABX9TyPlRypIASgrWdASAFcLoQ/b",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hyo250/gittest/blob/master/iris_k_fold_%26_best_model_%EC%B0%BE%EA%B8%B0(%EC%B5%9C%EC%A2%85).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E4EFlKAf_SAm",
        "outputId": "054b3ccb-889b-4d4c-a8b8-9820883684e0"
      },
      "source": [
        "cd /content/drive/MyDrive/인공지능실습/모두의 딥러닝/"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/인공지능실습/모두의 딥러닝\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F7WL2fB4AUoG"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import tensorflow as tf"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sg66_-91Ch6g"
      },
      "source": [
        "# iris 머신러닝\n",
        "- x: 꽃의 너비, 꽃잎높이, 꽃받침 너비, 꽃받침높이\n",
        "- y: 꽃의 종류\n",
        "\n",
        "- 인공지능= 예측 or 분류\n",
        "- 회귀분석(예측)에서 결과값은 1개, 무한대 범위에서.\n",
        "- 분류는 이항분류(둘중 하나) 또는 다항 분류\n",
        "- 결과값이 0~1사이일때 output을 1로 하면 출력결과는 0~1 사이 되도록 시그모이드를 사용하는 것을 권장\n",
        "- 시그모이드는 0~1사이로 출력. 중간값 0.5 기준으로 사용자가 직접 if 함수로 true / false 값으로 만들어야 함. \n",
        "- "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DVaY7paXDs75"
      },
      "source": [
        "- 다항분류의 예"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9XUqUkUDq2R"
      },
      "source": [
        "![image.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMc9EZL_DvVj"
      },
      "source": [
        "- 이항분류의 예"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PImk0TYYEC-R"
      },
      "source": [
        "![image.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418
        },
        "id": "kh-U5aNdAQvw",
        "outputId": "e9462464-5b85-4bba-d40e-362fbda4cce5"
      },
      "source": [
        "df=pd.read_csv('./dataset/iris.csv', header=None)\n",
        "df"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5.1</td>\n",
              "      <td>3.5</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4.9</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4.7</td>\n",
              "      <td>3.2</td>\n",
              "      <td>1.3</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4.6</td>\n",
              "      <td>3.1</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5.0</td>\n",
              "      <td>3.6</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>145</th>\n",
              "      <td>6.7</td>\n",
              "      <td>3.0</td>\n",
              "      <td>5.2</td>\n",
              "      <td>2.3</td>\n",
              "      <td>Iris-virginica</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>146</th>\n",
              "      <td>6.3</td>\n",
              "      <td>2.5</td>\n",
              "      <td>5.0</td>\n",
              "      <td>1.9</td>\n",
              "      <td>Iris-virginica</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>147</th>\n",
              "      <td>6.5</td>\n",
              "      <td>3.0</td>\n",
              "      <td>5.2</td>\n",
              "      <td>2.0</td>\n",
              "      <td>Iris-virginica</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>148</th>\n",
              "      <td>6.2</td>\n",
              "      <td>3.4</td>\n",
              "      <td>5.4</td>\n",
              "      <td>2.3</td>\n",
              "      <td>Iris-virginica</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>149</th>\n",
              "      <td>5.9</td>\n",
              "      <td>3.0</td>\n",
              "      <td>5.1</td>\n",
              "      <td>1.8</td>\n",
              "      <td>Iris-virginica</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>150 rows × 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       0    1    2    3               4\n",
              "0    5.1  3.5  1.4  0.2     Iris-setosa\n",
              "1    4.9  3.0  1.4  0.2     Iris-setosa\n",
              "2    4.7  3.2  1.3  0.2     Iris-setosa\n",
              "3    4.6  3.1  1.5  0.2     Iris-setosa\n",
              "4    5.0  3.6  1.4  0.2     Iris-setosa\n",
              "..   ...  ...  ...  ...             ...\n",
              "145  6.7  3.0  5.2  2.3  Iris-virginica\n",
              "146  6.3  2.5  5.0  1.9  Iris-virginica\n",
              "147  6.5  3.0  5.2  2.0  Iris-virginica\n",
              "148  6.2  3.4  5.4  2.3  Iris-virginica\n",
              "149  5.9  3.0  5.1  1.8  Iris-virginica\n",
              "\n",
              "[150 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L0FUgnbUAiP2",
        "outputId": "48acfce1-257d-4341-d70b-2fb4f59d9cf1"
      },
      "source": [
        "df.info()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 150 entries, 0 to 149\n",
            "Data columns (total 5 columns):\n",
            " #   Column  Non-Null Count  Dtype  \n",
            "---  ------  --------------  -----  \n",
            " 0   0       150 non-null    float64\n",
            " 1   1       150 non-null    float64\n",
            " 2   2       150 non-null    float64\n",
            " 3   3       150 non-null    float64\n",
            " 4   4       150 non-null    object \n",
            "dtypes: float64(4), object(1)\n",
            "memory usage: 6.0+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5hqAkb6_R-fF"
      },
      "source": [
        "# 데이터 살펴보기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ijZ049lSJzY",
        "outputId": "847846b1-310e-4650-f58f-e2f1de369b0b"
      },
      "source": [
        "#주의! iris 데이터는 꽃의 종류별로 0~50, 51~100, 101~150개가 정렬되어 있음.\n",
        "print(df[4].value_counts())\n",
        "print(df.head(10))\n",
        "print(df.iloc[50:60])\n",
        "print(df.tail(10))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iris-virginica     50\n",
            "Iris-versicolor    50\n",
            "Iris-setosa        50\n",
            "Name: 4, dtype: int64\n",
            "     0    1    2    3            4\n",
            "0  5.1  3.5  1.4  0.2  Iris-setosa\n",
            "1  4.9  3.0  1.4  0.2  Iris-setosa\n",
            "2  4.7  3.2  1.3  0.2  Iris-setosa\n",
            "3  4.6  3.1  1.5  0.2  Iris-setosa\n",
            "4  5.0  3.6  1.4  0.2  Iris-setosa\n",
            "5  5.4  3.9  1.7  0.4  Iris-setosa\n",
            "6  4.6  3.4  1.4  0.3  Iris-setosa\n",
            "7  5.0  3.4  1.5  0.2  Iris-setosa\n",
            "8  4.4  2.9  1.4  0.2  Iris-setosa\n",
            "9  4.9  3.1  1.5  0.1  Iris-setosa\n",
            "      0    1    2    3                4\n",
            "50  7.0  3.2  4.7  1.4  Iris-versicolor\n",
            "51  6.4  3.2  4.5  1.5  Iris-versicolor\n",
            "52  6.9  3.1  4.9  1.5  Iris-versicolor\n",
            "53  5.5  2.3  4.0  1.3  Iris-versicolor\n",
            "54  6.5  2.8  4.6  1.5  Iris-versicolor\n",
            "55  5.7  2.8  4.5  1.3  Iris-versicolor\n",
            "56  6.3  3.3  4.7  1.6  Iris-versicolor\n",
            "57  4.9  2.4  3.3  1.0  Iris-versicolor\n",
            "58  6.6  2.9  4.6  1.3  Iris-versicolor\n",
            "59  5.2  2.7  3.9  1.4  Iris-versicolor\n",
            "       0    1    2    3               4\n",
            "140  6.7  3.1  5.6  2.4  Iris-virginica\n",
            "141  6.9  3.1  5.1  2.3  Iris-virginica\n",
            "142  5.8  2.7  5.1  1.9  Iris-virginica\n",
            "143  6.8  3.2  5.9  2.3  Iris-virginica\n",
            "144  6.7  3.3  5.7  2.5  Iris-virginica\n",
            "145  6.7  3.0  5.2  2.3  Iris-virginica\n",
            "146  6.3  2.5  5.0  1.9  Iris-virginica\n",
            "147  6.5  3.0  5.2  2.0  Iris-virginica\n",
            "148  6.2  3.4  5.4  2.3  Iris-virginica\n",
            "149  5.9  3.0  5.1  1.8  Iris-virginica\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vUegviisIuXa",
        "outputId": "3728f2a4-58b7-4537-ee89-c5521e9a1e26"
      },
      "source": [
        "dataset=df.values\n",
        "X=dataset[:, 0:4]\n",
        "y_obj=dataset[:, 4]\n",
        "X.dtype, y_obj.dtype  #(dtype('O'), dtype('O'))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(dtype('O'), dtype('O'))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jHtftZjcJ95w"
      },
      "source": [
        "#형변환 & #문자열 인코딩\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "le=LabelEncoder()\n",
        "le.fit(y_obj)\n",
        "y_le=le.transform(y_obj)\n",
        "X=X.astype('float')\n",
        "y_le=y_le.astype('float')"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D1Ohgko9SaAt",
        "outputId": "4864502d-67db-4e66-ed23-5e9a7825bb95"
      },
      "source": [
        "y_le  #y_le는 0 , 1 ,2 로 라벨인코딩된 값들"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 2., 2.,\n",
              "       2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.,\n",
              "       2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.,\n",
              "       2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YSpiIzaQKZ2k",
        "outputId": "5d172bb5-df57-40e2-a113-086323401901"
      },
      "source": [
        "X.dtype, y_le.dtype "
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(dtype('float64'), dtype('float64'))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SueG7m9bL6FY",
        "outputId": "3b4ccdd3-6b11-44a8-85df-8f7a5f007fae"
      },
      "source": [
        "#from tensorflow.keras.utils import np_utils 하니 생기는 오류 해결하기 위해 아래를 확인.\n",
        "import tensorflow.keras.utils\n",
        "dir(tensorflow.keras.utils)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['CustomObjectScope',\n",
              " 'GeneratorEnqueuer',\n",
              " 'OrderedEnqueuer',\n",
              " 'Progbar',\n",
              " 'Sequence',\n",
              " 'SequenceEnqueuer',\n",
              " '__builtins__',\n",
              " '__cached__',\n",
              " '__doc__',\n",
              " '__file__',\n",
              " '__loader__',\n",
              " '__name__',\n",
              " '__package__',\n",
              " '__path__',\n",
              " '__spec__',\n",
              " '_sys',\n",
              " 'custom_object_scope',\n",
              " 'deserialize_keras_object',\n",
              " 'experimental',\n",
              " 'get_custom_objects',\n",
              " 'get_file',\n",
              " 'get_registered_name',\n",
              " 'get_registered_object',\n",
              " 'get_source_inputs',\n",
              " 'model_to_dot',\n",
              " 'normalize',\n",
              " 'pack_x_y_sample_weight',\n",
              " 'plot_model',\n",
              " 'register_keras_serializable',\n",
              " 'serialize_keras_object',\n",
              " 'to_categorical',\n",
              " 'unpack_x_y_sample_weight']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1UKIat2DK9nO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1264ecd-b486-4b13-9f1a-7874518ba9dc"
      },
      "source": [
        "from tensorflow.keras.utils import to_categorical #이걸 적용해줌.\n",
        "y_encoded=tf.keras.utils.to_categorical(y_le)\n",
        "y_encoded #y_le를 원핫인코딩한 결과. 0 은 [ 1., 0., 0.] 1은 [0., 1 ., 0.] 2는 [0., 0., 1.]"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1vGbJW8o1t2X",
        "outputId": "4f8a7e4b-287d-471f-9f3a-efb0dabdf871"
      },
      "source": [
        "y_encoded.shape"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(150, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yYYNkQctNs-7"
      },
      "source": [
        "#k-fold 안하고 훈련셋 테스트셋 골고루 섞이게 하고 싶을 때. \n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, \n",
        "                                                    shuffle=True,  #체계적추출\n",
        "                                                    stratify=y_encoded,  #층화추출\n",
        "                                                    random_state=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q8VyD1CeQWiS"
      },
      "source": [
        "# 모델의 실행\n",
        "- 데이터 수 너무 적으니==> k-fold 적용\n",
        "- y값이 순차적으로 있으니 그게 골고루 train test에 들어가도록 할 것.\n",
        "- 추가 설정: \n",
        "-- 학습 자동 중단 ==> model.fit에 callbacks=[early_stopping_callback. \n",
        "-- 모델 업데이트 및 저장==> checkpointer\n",
        " ---  = ModelCheckpoint(filepath= , monitor='val_loss', verbose=1, save_best_olny=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MQY-Y4k4TKCB"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dense\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "import os"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aX3OUITvkP86"
      },
      "source": [
        "# k-fold 없이 예측해보기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Frww2p3BkPrt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "958545fd-a7f3-445d-9b0d-69778f83ff3e"
      },
      "source": [
        "normal_model=Sequential()\n",
        "normal_model.add(Dense(16, input_dim=4, activation='relu'))\n",
        "normal_model.add(Dense(3, activation='softmax')) #3= y_encoded nunique. \n",
        "normal_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "normal_model.fit(X,y_encoded, epochs=50, batch_size=1)\n",
        "\n",
        "print('\\n 정확도: %.4f'%normal_model.evaluate(X, y_encoded)[1])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "150/150 [==============================] - 1s 1ms/step - loss: 1.3232 - accuracy: 0.3816\n",
            "Epoch 2/50\n",
            "150/150 [==============================] - 0s 1ms/step - loss: 0.8098 - accuracy: 0.8121\n",
            "Epoch 3/50\n",
            "150/150 [==============================] - 0s 1ms/step - loss: 0.6507 - accuracy: 0.9117\n",
            "Epoch 4/50\n",
            "150/150 [==============================] - 0s 1ms/step - loss: 0.5689 - accuracy: 0.8277\n",
            "Epoch 5/50\n",
            "150/150 [==============================] - 0s 1ms/step - loss: 0.4937 - accuracy: 0.8389\n",
            "Epoch 6/50\n",
            "150/150 [==============================] - 0s 1ms/step - loss: 0.4397 - accuracy: 0.8863\n",
            "Epoch 7/50\n",
            "150/150 [==============================] - 0s 1ms/step - loss: 0.4066 - accuracy: 0.9494\n",
            "Epoch 8/50\n",
            "150/150 [==============================] - 0s 1ms/step - loss: 0.3495 - accuracy: 0.9284\n",
            "Epoch 9/50\n",
            "150/150 [==============================] - 0s 1ms/step - loss: 0.3536 - accuracy: 0.9036\n",
            "Epoch 10/50\n",
            "150/150 [==============================] - 0s 1ms/step - loss: 0.3152 - accuracy: 0.9487\n",
            "Epoch 11/50\n",
            "150/150 [==============================] - 0s 1ms/step - loss: 0.2799 - accuracy: 0.9505\n",
            "Epoch 12/50\n",
            "150/150 [==============================] - 0s 995us/step - loss: 0.3162 - accuracy: 0.9564\n",
            "Epoch 13/50\n",
            "150/150 [==============================] - 0s 1ms/step - loss: 0.3096 - accuracy: 0.9569\n",
            "Epoch 14/50\n",
            "150/150 [==============================] - 0s 1ms/step - loss: 0.2905 - accuracy: 0.9351\n",
            "Epoch 15/50\n",
            "150/150 [==============================] - 0s 1ms/step - loss: 0.2876 - accuracy: 0.9615\n",
            "Epoch 16/50\n",
            "150/150 [==============================] - 0s 1ms/step - loss: 0.2497 - accuracy: 0.9618\n",
            "Epoch 17/50\n",
            "150/150 [==============================] - 0s 1ms/step - loss: 0.2532 - accuracy: 0.9438\n",
            "Epoch 18/50\n",
            "150/150 [==============================] - 0s 1ms/step - loss: 0.2766 - accuracy: 0.9280\n",
            "Epoch 19/50\n",
            "150/150 [==============================] - 0s 1ms/step - loss: 0.2515 - accuracy: 0.9237\n",
            "Epoch 20/50\n",
            "150/150 [==============================] - 0s 1ms/step - loss: 0.2143 - accuracy: 0.9715\n",
            "Epoch 21/50\n",
            "150/150 [==============================] - 0s 1ms/step - loss: 0.2223 - accuracy: 0.9511\n",
            "Epoch 22/50\n",
            "150/150 [==============================] - 0s 1ms/step - loss: 0.2270 - accuracy: 0.9426\n",
            "Epoch 23/50\n",
            "150/150 [==============================] - 0s 1ms/step - loss: 0.1843 - accuracy: 0.9627\n",
            "Epoch 24/50\n",
            "150/150 [==============================] - 0s 1ms/step - loss: 0.1815 - accuracy: 0.9641\n",
            "Epoch 25/50\n",
            "150/150 [==============================] - 0s 1ms/step - loss: 0.2221 - accuracy: 0.9194\n",
            "Epoch 26/50\n",
            "150/150 [==============================] - 0s 1ms/step - loss: 0.2072 - accuracy: 0.9692\n",
            "Epoch 27/50\n",
            "150/150 [==============================] - 0s 1ms/step - loss: 0.1756 - accuracy: 0.9701\n",
            "Epoch 28/50\n",
            "150/150 [==============================] - 0s 1ms/step - loss: 0.1753 - accuracy: 0.9825\n",
            "Epoch 29/50\n",
            "150/150 [==============================] - 0s 1ms/step - loss: 0.2159 - accuracy: 0.9296\n",
            "Epoch 30/50\n",
            "150/150 [==============================] - 0s 1ms/step - loss: 0.1385 - accuracy: 0.9855\n",
            "Epoch 31/50\n",
            "150/150 [==============================] - 0s 1ms/step - loss: 0.1963 - accuracy: 0.9046\n",
            "Epoch 32/50\n",
            "150/150 [==============================] - 0s 1ms/step - loss: 0.1647 - accuracy: 0.9525\n",
            "Epoch 33/50\n",
            "150/150 [==============================] - 0s 1ms/step - loss: 0.1405 - accuracy: 0.9657\n",
            "Epoch 34/50\n",
            "150/150 [==============================] - 0s 1ms/step - loss: 0.1477 - accuracy: 0.9816\n",
            "Epoch 35/50\n",
            "150/150 [==============================] - 0s 1ms/step - loss: 0.1312 - accuracy: 0.9801\n",
            "Epoch 36/50\n",
            "150/150 [==============================] - 0s 1ms/step - loss: 0.1635 - accuracy: 0.9625\n",
            "Epoch 37/50\n",
            "150/150 [==============================] - 0s 1ms/step - loss: 0.1509 - accuracy: 0.9644\n",
            "Epoch 38/50\n",
            "150/150 [==============================] - 0s 1ms/step - loss: 0.1152 - accuracy: 0.9807\n",
            "Epoch 39/50\n",
            "150/150 [==============================] - 0s 1ms/step - loss: 0.1138 - accuracy: 0.9722\n",
            "Epoch 40/50\n",
            "150/150 [==============================] - 0s 1ms/step - loss: 0.1265 - accuracy: 0.9759\n",
            "Epoch 41/50\n",
            "150/150 [==============================] - 0s 1ms/step - loss: 0.1617 - accuracy: 0.9699\n",
            "Epoch 42/50\n",
            "150/150 [==============================] - 0s 1ms/step - loss: 0.1101 - accuracy: 0.9857\n",
            "Epoch 43/50\n",
            "150/150 [==============================] - 0s 1ms/step - loss: 0.1082 - accuracy: 0.9910\n",
            "Epoch 44/50\n",
            "150/150 [==============================] - 0s 1ms/step - loss: 0.1271 - accuracy: 0.9785\n",
            "Epoch 45/50\n",
            "150/150 [==============================] - 0s 1ms/step - loss: 0.1038 - accuracy: 0.9736\n",
            "Epoch 46/50\n",
            "150/150 [==============================] - 0s 1ms/step - loss: 0.1541 - accuracy: 0.9547\n",
            "Epoch 47/50\n",
            "150/150 [==============================] - 0s 1ms/step - loss: 0.1124 - accuracy: 0.9772\n",
            "Epoch 48/50\n",
            "150/150 [==============================] - 0s 1ms/step - loss: 0.0970 - accuracy: 0.9891\n",
            "Epoch 49/50\n",
            "150/150 [==============================] - 0s 1ms/step - loss: 0.1098 - accuracy: 0.9840\n",
            "Epoch 50/50\n",
            "150/150 [==============================] - 0s 1ms/step - loss: 0.1348 - accuracy: 0.9349\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 0.1142 - accuracy: 0.9800\n",
            "\n",
            " 정확도: 0.9800\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9q1SsJ23nbwh"
      },
      "source": [
        "# k-fold 로 해보기..\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ev_sQXWHTqmH"
      },
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "80fsQ094npkt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69ae3a72-6736-4da3-be3c-361a51f6779e"
      },
      "source": [
        "# for 로 실행해보는 k-fold\n",
        "accuracy=[]\n",
        "n_fold=10\n",
        "for i in range(10):\n",
        "\n",
        "    X_train, X_test, Y_train, Y_test = train_test_split(X, y_encoded, test_size=0.2) \n",
        "    \n",
        "    model = Sequential()\n",
        "    model.add(Dense(24, input_dim=4, activation='relu'))\n",
        "    model.add(Dense(10, activation='relu'))\n",
        "    model.add(Dense(3, activation='softmax'))\n",
        "\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "    \n",
        "    model.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=100, batch_size=5, verbose=1)\n",
        "    \n",
        "    k_accuracy = '%.4f'%(model.evaluate(X_test, Y_test)[1])\n",
        "    accuracy.append(k_accuracy)\n",
        "            \n",
        "print('\\n %.f fold accuracy: '%n_fold, accuracy)\n",
        "# 10 fold accuracy:  ['1.0000', '1.0000', '1.0000', '0.9667', '1.0000', '0.9667', '1.0000', '1.0000', '0.9667', '0.9667'] <===split할때 shuffle=True, stratify=y_encoded 넣었을때값..."
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "24/24 [==============================] - 1s 15ms/step - loss: 1.1200 - accuracy: 0.4047 - val_loss: 1.1328 - val_accuracy: 0.2667\n",
            "Epoch 2/100\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 1.0693 - accuracy: 0.3258 - val_loss: 1.0886 - val_accuracy: 0.2667\n",
            "Epoch 3/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.9958 - accuracy: 0.3685 - val_loss: 1.0592 - val_accuracy: 0.2667\n",
            "Epoch 4/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.9723 - accuracy: 0.3652 - val_loss: 1.0276 - val_accuracy: 0.4000\n",
            "Epoch 5/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.9537 - accuracy: 0.3896 - val_loss: 1.0083 - val_accuracy: 0.2333\n",
            "Epoch 6/100\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.9186 - accuracy: 0.3732 - val_loss: 0.9799 - val_accuracy: 0.4000\n",
            "Epoch 7/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.8931 - accuracy: 0.4179 - val_loss: 0.9577 - val_accuracy: 0.4333\n",
            "Epoch 8/100\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.8572 - accuracy: 0.4816 - val_loss: 0.9367 - val_accuracy: 0.4333\n",
            "Epoch 9/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.8231 - accuracy: 0.4141 - val_loss: 0.9154 - val_accuracy: 0.3333\n",
            "Epoch 10/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.7923 - accuracy: 0.4476 - val_loss: 0.8905 - val_accuracy: 0.4333\n",
            "Epoch 11/100\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.7849 - accuracy: 0.5718 - val_loss: 0.8694 - val_accuracy: 0.4333\n",
            "Epoch 12/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.7976 - accuracy: 0.4295 - val_loss: 0.8475 - val_accuracy: 0.4333\n",
            "Epoch 13/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.8000 - accuracy: 0.3886 - val_loss: 0.8294 - val_accuracy: 0.4333\n",
            "Epoch 14/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.7376 - accuracy: 0.4990 - val_loss: 0.8078 - val_accuracy: 0.4333\n",
            "Epoch 15/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.7356 - accuracy: 0.5204 - val_loss: 0.7857 - val_accuracy: 0.4333\n",
            "Epoch 16/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.7118 - accuracy: 0.6459 - val_loss: 0.7614 - val_accuracy: 0.5333\n",
            "Epoch 17/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.6971 - accuracy: 0.7239 - val_loss: 0.7463 - val_accuracy: 0.8000\n",
            "Epoch 18/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.7135 - accuracy: 0.8619 - val_loss: 0.7266 - val_accuracy: 0.8333\n",
            "Epoch 19/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.6577 - accuracy: 0.8475 - val_loss: 0.7001 - val_accuracy: 0.9333\n",
            "Epoch 20/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.6149 - accuracy: 0.9213 - val_loss: 0.6817 - val_accuracy: 0.9333\n",
            "Epoch 21/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.6139 - accuracy: 0.9591 - val_loss: 0.6622 - val_accuracy: 0.9000\n",
            "Epoch 22/100\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.5936 - accuracy: 0.9177 - val_loss: 0.6451 - val_accuracy: 0.9333\n",
            "Epoch 23/100\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.5949 - accuracy: 0.9185 - val_loss: 0.6256 - val_accuracy: 0.9667\n",
            "Epoch 24/100\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.5360 - accuracy: 0.9890 - val_loss: 0.6073 - val_accuracy: 0.9667\n",
            "Epoch 25/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.5679 - accuracy: 0.9499 - val_loss: 0.5938 - val_accuracy: 0.9333\n",
            "Epoch 26/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.5242 - accuracy: 0.8911 - val_loss: 0.5774 - val_accuracy: 0.9667\n",
            "Epoch 27/100\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.5153 - accuracy: 0.9505 - val_loss: 0.5592 - val_accuracy: 0.9667\n",
            "Epoch 28/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.5029 - accuracy: 0.9587 - val_loss: 0.5466 - val_accuracy: 0.9667\n",
            "Epoch 29/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.4897 - accuracy: 0.9668 - val_loss: 0.5325 - val_accuracy: 0.9667\n",
            "Epoch 30/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.4368 - accuracy: 0.9407 - val_loss: 0.5165 - val_accuracy: 0.9667\n",
            "Epoch 31/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.4358 - accuracy: 0.9535 - val_loss: 0.5033 - val_accuracy: 0.9667\n",
            "Epoch 32/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.4169 - accuracy: 0.9874 - val_loss: 0.4903 - val_accuracy: 0.9667\n",
            "Epoch 33/100\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.4392 - accuracy: 0.9816 - val_loss: 0.4802 - val_accuracy: 0.9667\n",
            "Epoch 34/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.4147 - accuracy: 0.9853 - val_loss: 0.4778 - val_accuracy: 0.9333\n",
            "Epoch 35/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.4330 - accuracy: 0.9303 - val_loss: 0.4576 - val_accuracy: 0.9667\n",
            "Epoch 36/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.3890 - accuracy: 0.9651 - val_loss: 0.4464 - val_accuracy: 0.9667\n",
            "Epoch 37/100\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.3769 - accuracy: 0.9641 - val_loss: 0.4345 - val_accuracy: 0.9667\n",
            "Epoch 38/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.3576 - accuracy: 0.9678 - val_loss: 0.4296 - val_accuracy: 0.9667\n",
            "Epoch 39/100\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.3460 - accuracy: 0.9546 - val_loss: 0.4174 - val_accuracy: 0.9667\n",
            "Epoch 40/100\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.3570 - accuracy: 0.9766 - val_loss: 0.4073 - val_accuracy: 0.9667\n",
            "Epoch 41/100\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.3659 - accuracy: 0.9606 - val_loss: 0.4197 - val_accuracy: 0.9333\n",
            "Epoch 42/100\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.3351 - accuracy: 0.9693 - val_loss: 0.3894 - val_accuracy: 0.9667\n",
            "Epoch 43/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.3326 - accuracy: 0.9589 - val_loss: 0.3829 - val_accuracy: 0.9667\n",
            "Epoch 44/100\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.2916 - accuracy: 0.9925 - val_loss: 0.3744 - val_accuracy: 0.9667\n",
            "Epoch 45/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.3345 - accuracy: 0.9908 - val_loss: 0.3740 - val_accuracy: 0.9667\n",
            "Epoch 46/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.3329 - accuracy: 0.9634 - val_loss: 0.3612 - val_accuracy: 0.9667\n",
            "Epoch 47/100\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.2956 - accuracy: 0.9766 - val_loss: 0.3529 - val_accuracy: 0.9667\n",
            "Epoch 48/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.2938 - accuracy: 0.9876 - val_loss: 0.3467 - val_accuracy: 0.9667\n",
            "Epoch 49/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.2695 - accuracy: 0.9918 - val_loss: 0.3419 - val_accuracy: 0.9667\n",
            "Epoch 50/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.2776 - accuracy: 0.9480 - val_loss: 0.3376 - val_accuracy: 0.9667\n",
            "Epoch 51/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.2571 - accuracy: 0.9968 - val_loss: 0.3283 - val_accuracy: 0.9667\n",
            "Epoch 52/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.2747 - accuracy: 0.9838 - val_loss: 0.3366 - val_accuracy: 0.9667\n",
            "Epoch 53/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.2417 - accuracy: 0.9520 - val_loss: 0.3166 - val_accuracy: 0.9667\n",
            "Epoch 54/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.2562 - accuracy: 0.9521 - val_loss: 0.3303 - val_accuracy: 0.9667\n",
            "Epoch 55/100\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.2439 - accuracy: 0.9578 - val_loss: 0.3065 - val_accuracy: 0.9667\n",
            "Epoch 56/100\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.2287 - accuracy: 0.9780 - val_loss: 0.3028 - val_accuracy: 0.9667\n",
            "Epoch 57/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.2436 - accuracy: 0.9910 - val_loss: 0.3049 - val_accuracy: 0.9667\n",
            "Epoch 58/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.2392 - accuracy: 0.9695 - val_loss: 0.2917 - val_accuracy: 0.9667\n",
            "Epoch 59/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.2274 - accuracy: 0.9840 - val_loss: 0.2877 - val_accuracy: 0.9667\n",
            "Epoch 60/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.2384 - accuracy: 0.9699 - val_loss: 0.2979 - val_accuracy: 0.9667\n",
            "Epoch 61/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.2251 - accuracy: 0.9532 - val_loss: 0.2845 - val_accuracy: 0.9667\n",
            "Epoch 62/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.2228 - accuracy: 0.9796 - val_loss: 0.2747 - val_accuracy: 0.9667\n",
            "Epoch 63/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.2355 - accuracy: 0.9563 - val_loss: 0.2709 - val_accuracy: 0.9667\n",
            "Epoch 64/100\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.2146 - accuracy: 0.9871 - val_loss: 0.2675 - val_accuracy: 0.9667\n",
            "Epoch 65/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.2039 - accuracy: 0.9821 - val_loss: 0.2636 - val_accuracy: 0.9667\n",
            "Epoch 66/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.2456 - accuracy: 0.9674 - val_loss: 0.2654 - val_accuracy: 0.9667\n",
            "Epoch 67/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1922 - accuracy: 0.9840 - val_loss: 0.2579 - val_accuracy: 0.9667\n",
            "Epoch 68/100\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.2060 - accuracy: 0.9480 - val_loss: 0.2679 - val_accuracy: 0.9667\n",
            "Epoch 69/100\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.2214 - accuracy: 0.9572 - val_loss: 0.2574 - val_accuracy: 0.9667\n",
            "Epoch 70/100\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.1906 - accuracy: 0.9827 - val_loss: 0.2467 - val_accuracy: 0.9667\n",
            "Epoch 71/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.1859 - accuracy: 0.9895 - val_loss: 0.2452 - val_accuracy: 0.9667\n",
            "Epoch 72/100\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.1825 - accuracy: 0.9842 - val_loss: 0.2504 - val_accuracy: 0.9667\n",
            "Epoch 73/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1858 - accuracy: 0.9756 - val_loss: 0.2374 - val_accuracy: 0.9667\n",
            "Epoch 74/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1693 - accuracy: 0.9891 - val_loss: 0.2344 - val_accuracy: 0.9667\n",
            "Epoch 75/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1767 - accuracy: 0.9768 - val_loss: 0.2573 - val_accuracy: 0.9667\n",
            "Epoch 76/100\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.1764 - accuracy: 0.9735 - val_loss: 0.2429 - val_accuracy: 0.9667\n",
            "Epoch 77/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1998 - accuracy: 0.9654 - val_loss: 0.2263 - val_accuracy: 0.9667\n",
            "Epoch 78/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1570 - accuracy: 0.9982 - val_loss: 0.2298 - val_accuracy: 0.9667\n",
            "Epoch 79/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1770 - accuracy: 0.9879 - val_loss: 0.2399 - val_accuracy: 0.9667\n",
            "Epoch 80/100\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.1620 - accuracy: 0.9705 - val_loss: 0.2201 - val_accuracy: 0.9667\n",
            "Epoch 81/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1702 - accuracy: 0.9659 - val_loss: 0.2193 - val_accuracy: 0.9667\n",
            "Epoch 82/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1517 - accuracy: 0.9758 - val_loss: 0.2155 - val_accuracy: 0.9667\n",
            "Epoch 83/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1540 - accuracy: 0.9929 - val_loss: 0.2249 - val_accuracy: 0.9667\n",
            "Epoch 84/100\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.1590 - accuracy: 0.9628 - val_loss: 0.2247 - val_accuracy: 0.9667\n",
            "Epoch 85/100\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.1694 - accuracy: 0.9607 - val_loss: 0.2089 - val_accuracy: 0.9667\n",
            "Epoch 86/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1807 - accuracy: 0.9625 - val_loss: 0.2295 - val_accuracy: 0.9667\n",
            "Epoch 87/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1499 - accuracy: 0.9706 - val_loss: 0.2048 - val_accuracy: 0.9667\n",
            "Epoch 88/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1416 - accuracy: 0.9754 - val_loss: 0.2023 - val_accuracy: 0.9667\n",
            "Epoch 89/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.1602 - accuracy: 0.9670 - val_loss: 0.2016 - val_accuracy: 0.9667\n",
            "Epoch 90/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1582 - accuracy: 0.9656 - val_loss: 0.2104 - val_accuracy: 0.9667\n",
            "Epoch 91/100\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.1479 - accuracy: 0.9727 - val_loss: 0.1962 - val_accuracy: 0.9667\n",
            "Epoch 92/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1166 - accuracy: 0.9767 - val_loss: 0.2036 - val_accuracy: 0.9667\n",
            "Epoch 93/100\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.1257 - accuracy: 0.9972 - val_loss: 0.2031 - val_accuracy: 0.9667\n",
            "Epoch 94/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1240 - accuracy: 0.9936 - val_loss: 0.1924 - val_accuracy: 0.9667\n",
            "Epoch 95/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1255 - accuracy: 0.9923 - val_loss: 0.1965 - val_accuracy: 0.9667\n",
            "Epoch 96/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1171 - accuracy: 0.9832 - val_loss: 0.1884 - val_accuracy: 0.9667\n",
            "Epoch 97/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1237 - accuracy: 0.9745 - val_loss: 0.1922 - val_accuracy: 0.9667\n",
            "Epoch 98/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1290 - accuracy: 0.9575 - val_loss: 0.1861 - val_accuracy: 0.9667\n",
            "Epoch 99/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1286 - accuracy: 0.9801 - val_loss: 0.1872 - val_accuracy: 0.9667\n",
            "Epoch 100/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1042 - accuracy: 0.9976 - val_loss: 0.1872 - val_accuracy: 0.9667\n",
            "1/1 [==============================] - 0s 282ms/step - loss: 0.1872 - accuracy: 0.9667\n",
            "Epoch 1/100\n",
            "24/24 [==============================] - 1s 13ms/step - loss: 1.5831 - accuracy: 0.4235 - val_loss: 1.5295 - val_accuracy: 0.2000\n",
            "Epoch 2/100\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 1.2156 - accuracy: 0.3272 - val_loss: 1.2682 - val_accuracy: 0.2000\n",
            "Epoch 3/100\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.9714 - accuracy: 0.4334 - val_loss: 1.1660 - val_accuracy: 0.2000\n",
            "Epoch 4/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.9538 - accuracy: 0.4377 - val_loss: 1.0356 - val_accuracy: 0.3333\n",
            "Epoch 5/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.8983 - accuracy: 0.4262 - val_loss: 0.9138 - val_accuracy: 0.3333\n",
            "Epoch 6/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.8201 - accuracy: 0.5217 - val_loss: 0.8032 - val_accuracy: 0.7000\n",
            "Epoch 7/100\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.7330 - accuracy: 0.6541 - val_loss: 0.7072 - val_accuracy: 0.7000\n",
            "Epoch 8/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.6807 - accuracy: 0.7009 - val_loss: 0.6463 - val_accuracy: 0.8333\n",
            "Epoch 9/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.6112 - accuracy: 0.7959 - val_loss: 0.6062 - val_accuracy: 0.8333\n",
            "Epoch 10/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.5776 - accuracy: 0.7706 - val_loss: 0.5568 - val_accuracy: 0.8333\n",
            "Epoch 11/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.5837 - accuracy: 0.8169 - val_loss: 0.5177 - val_accuracy: 0.9000\n",
            "Epoch 12/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.5418 - accuracy: 0.7565 - val_loss: 0.4798 - val_accuracy: 0.9333\n",
            "Epoch 13/100\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.5168 - accuracy: 0.9059 - val_loss: 0.4592 - val_accuracy: 0.8667\n",
            "Epoch 14/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.4748 - accuracy: 0.7868 - val_loss: 0.4321 - val_accuracy: 0.9333\n",
            "Epoch 15/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.4750 - accuracy: 0.9006 - val_loss: 0.4023 - val_accuracy: 0.9333\n",
            "Epoch 16/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.4605 - accuracy: 0.8332 - val_loss: 0.4222 - val_accuracy: 0.8000\n",
            "Epoch 17/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.4463 - accuracy: 0.7892 - val_loss: 0.3623 - val_accuracy: 0.9667\n",
            "Epoch 18/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.4196 - accuracy: 0.9689 - val_loss: 0.3578 - val_accuracy: 0.9333\n",
            "Epoch 19/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.3955 - accuracy: 0.8653 - val_loss: 0.3376 - val_accuracy: 0.9333\n",
            "Epoch 20/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.3703 - accuracy: 0.9251 - val_loss: 0.3137 - val_accuracy: 0.9667\n",
            "Epoch 21/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.3599 - accuracy: 0.9241 - val_loss: 0.3083 - val_accuracy: 0.9667\n",
            "Epoch 22/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.3293 - accuracy: 0.9707 - val_loss: 0.2845 - val_accuracy: 1.0000\n",
            "Epoch 23/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.3782 - accuracy: 0.9749 - val_loss: 0.2875 - val_accuracy: 0.9667\n",
            "Epoch 24/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.3220 - accuracy: 0.9674 - val_loss: 0.2668 - val_accuracy: 0.9667\n",
            "Epoch 25/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.3212 - accuracy: 0.9612 - val_loss: 0.2556 - val_accuracy: 1.0000\n",
            "Epoch 26/100\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.2977 - accuracy: 0.9633 - val_loss: 0.2448 - val_accuracy: 1.0000\n",
            "Epoch 27/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.3019 - accuracy: 0.9685 - val_loss: 0.2337 - val_accuracy: 1.0000\n",
            "Epoch 28/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.3050 - accuracy: 0.9622 - val_loss: 0.2315 - val_accuracy: 0.9667\n",
            "Epoch 29/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.2871 - accuracy: 0.9365 - val_loss: 0.2086 - val_accuracy: 0.9667\n",
            "Epoch 30/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.2760 - accuracy: 0.9908 - val_loss: 0.2117 - val_accuracy: 1.0000\n",
            "Epoch 31/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.2538 - accuracy: 0.9399 - val_loss: 0.2088 - val_accuracy: 1.0000\n",
            "Epoch 32/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.2845 - accuracy: 0.9444 - val_loss: 0.2079 - val_accuracy: 0.9667\n",
            "Epoch 33/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.2553 - accuracy: 0.9308 - val_loss: 0.1883 - val_accuracy: 1.0000\n",
            "Epoch 34/100\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.2435 - accuracy: 0.9715 - val_loss: 0.1786 - val_accuracy: 1.0000\n",
            "Epoch 35/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.2480 - accuracy: 0.9530 - val_loss: 0.1701 - val_accuracy: 1.0000\n",
            "Epoch 36/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.2234 - accuracy: 0.9719 - val_loss: 0.1836 - val_accuracy: 0.9667\n",
            "Epoch 37/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.2149 - accuracy: 0.9714 - val_loss: 0.1567 - val_accuracy: 0.9667\n",
            "Epoch 38/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1933 - accuracy: 0.9672 - val_loss: 0.1684 - val_accuracy: 1.0000\n",
            "Epoch 39/100\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.1688 - accuracy: 0.9855 - val_loss: 0.1479 - val_accuracy: 1.0000\n",
            "Epoch 40/100\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.2073 - accuracy: 0.9543 - val_loss: 0.1421 - val_accuracy: 1.0000\n",
            "Epoch 41/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1980 - accuracy: 0.9640 - val_loss: 0.1384 - val_accuracy: 1.0000\n",
            "Epoch 42/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1923 - accuracy: 0.9608 - val_loss: 0.1335 - val_accuracy: 1.0000\n",
            "Epoch 43/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1738 - accuracy: 0.9486 - val_loss: 0.1338 - val_accuracy: 1.0000\n",
            "Epoch 44/100\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.1675 - accuracy: 0.9597 - val_loss: 0.1248 - val_accuracy: 0.9667\n",
            "Epoch 45/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1610 - accuracy: 0.9807 - val_loss: 0.1254 - val_accuracy: 1.0000\n",
            "Epoch 46/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1509 - accuracy: 0.9762 - val_loss: 0.1212 - val_accuracy: 0.9667\n",
            "Epoch 47/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1685 - accuracy: 0.9458 - val_loss: 0.1166 - val_accuracy: 1.0000\n",
            "Epoch 48/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1853 - accuracy: 0.9588 - val_loss: 0.1200 - val_accuracy: 1.0000\n",
            "Epoch 49/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1662 - accuracy: 0.9662 - val_loss: 0.1080 - val_accuracy: 1.0000\n",
            "Epoch 50/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1667 - accuracy: 0.9485 - val_loss: 0.1050 - val_accuracy: 1.0000\n",
            "Epoch 51/100\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.1416 - accuracy: 0.9836 - val_loss: 0.1013 - val_accuracy: 1.0000\n",
            "Epoch 52/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1354 - accuracy: 0.9808 - val_loss: 0.0988 - val_accuracy: 1.0000\n",
            "Epoch 53/100\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.1456 - accuracy: 0.9460 - val_loss: 0.0964 - val_accuracy: 1.0000\n",
            "Epoch 54/100\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.1157 - accuracy: 0.9743 - val_loss: 0.0968 - val_accuracy: 0.9667\n",
            "Epoch 55/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.1596 - accuracy: 0.9929 - val_loss: 0.0960 - val_accuracy: 1.0000\n",
            "Epoch 56/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1249 - accuracy: 0.9781 - val_loss: 0.0892 - val_accuracy: 1.0000\n",
            "Epoch 57/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1395 - accuracy: 0.9868 - val_loss: 0.0898 - val_accuracy: 1.0000\n",
            "Epoch 58/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1535 - accuracy: 0.9606 - val_loss: 0.0854 - val_accuracy: 1.0000\n",
            "Epoch 59/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.1247 - accuracy: 0.9757 - val_loss: 0.0845 - val_accuracy: 1.0000\n",
            "Epoch 60/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1278 - accuracy: 0.9446 - val_loss: 0.0820 - val_accuracy: 1.0000\n",
            "Epoch 61/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.1341 - accuracy: 0.9626 - val_loss: 0.0803 - val_accuracy: 1.0000\n",
            "Epoch 62/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1252 - accuracy: 0.9612 - val_loss: 0.0816 - val_accuracy: 1.0000\n",
            "Epoch 63/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1034 - accuracy: 0.9823 - val_loss: 0.0795 - val_accuracy: 1.0000\n",
            "Epoch 64/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1191 - accuracy: 0.9793 - val_loss: 0.0756 - val_accuracy: 1.0000\n",
            "Epoch 65/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1249 - accuracy: 0.9608 - val_loss: 0.0806 - val_accuracy: 0.9667\n",
            "Epoch 66/100\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.1285 - accuracy: 0.9692 - val_loss: 0.0715 - val_accuracy: 1.0000\n",
            "Epoch 67/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.1240 - accuracy: 0.9487 - val_loss: 0.0752 - val_accuracy: 0.9667\n",
            "Epoch 68/100\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.0975 - accuracy: 0.9690 - val_loss: 0.0701 - val_accuracy: 1.0000\n",
            "Epoch 69/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1082 - accuracy: 0.9692 - val_loss: 0.0686 - val_accuracy: 1.0000\n",
            "Epoch 70/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1433 - accuracy: 0.9323 - val_loss: 0.0717 - val_accuracy: 0.9667\n",
            "Epoch 71/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1085 - accuracy: 0.9829 - val_loss: 0.0671 - val_accuracy: 1.0000\n",
            "Epoch 72/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1232 - accuracy: 0.9722 - val_loss: 0.0638 - val_accuracy: 1.0000\n",
            "Epoch 73/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.0889 - accuracy: 0.9716 - val_loss: 0.0643 - val_accuracy: 1.0000\n",
            "Epoch 74/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1045 - accuracy: 0.9412 - val_loss: 0.0625 - val_accuracy: 1.0000\n",
            "Epoch 75/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.0860 - accuracy: 0.9763 - val_loss: 0.0658 - val_accuracy: 1.0000\n",
            "Epoch 76/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1081 - accuracy: 0.9590 - val_loss: 0.0618 - val_accuracy: 1.0000\n",
            "Epoch 77/100\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.1221 - accuracy: 0.9507 - val_loss: 0.0720 - val_accuracy: 0.9667\n",
            "Epoch 78/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1123 - accuracy: 0.9779 - val_loss: 0.0619 - val_accuracy: 1.0000\n",
            "Epoch 79/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.0762 - accuracy: 0.9852 - val_loss: 0.0641 - val_accuracy: 0.9667\n",
            "Epoch 80/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1100 - accuracy: 0.9461 - val_loss: 0.0557 - val_accuracy: 1.0000\n",
            "Epoch 81/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.0761 - accuracy: 0.9776 - val_loss: 0.0551 - val_accuracy: 1.0000\n",
            "Epoch 82/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.0808 - accuracy: 0.9730 - val_loss: 0.0637 - val_accuracy: 0.9667\n",
            "Epoch 83/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.0894 - accuracy: 0.9941 - val_loss: 0.0541 - val_accuracy: 1.0000\n",
            "Epoch 84/100\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.0959 - accuracy: 0.9797 - val_loss: 0.0753 - val_accuracy: 0.9667\n",
            "Epoch 85/100\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.1103 - accuracy: 0.9574 - val_loss: 0.0530 - val_accuracy: 1.0000\n",
            "Epoch 86/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1434 - accuracy: 0.9304 - val_loss: 0.0514 - val_accuracy: 1.0000\n",
            "Epoch 87/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.0989 - accuracy: 0.9462 - val_loss: 0.0504 - val_accuracy: 1.0000\n",
            "Epoch 88/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.0926 - accuracy: 0.9600 - val_loss: 0.0528 - val_accuracy: 1.0000\n",
            "Epoch 89/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.0781 - accuracy: 0.9915 - val_loss: 0.0489 - val_accuracy: 1.0000\n",
            "Epoch 90/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.0827 - accuracy: 0.9749 - val_loss: 0.0673 - val_accuracy: 0.9667\n",
            "Epoch 91/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1044 - accuracy: 0.9629 - val_loss: 0.0498 - val_accuracy: 1.0000\n",
            "Epoch 92/100\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.0586 - accuracy: 0.9918 - val_loss: 0.0573 - val_accuracy: 0.9667\n",
            "Epoch 93/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.0713 - accuracy: 0.9867 - val_loss: 0.0513 - val_accuracy: 1.0000\n",
            "Epoch 94/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.0691 - accuracy: 0.9941 - val_loss: 0.0480 - val_accuracy: 1.0000\n",
            "Epoch 95/100\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.1215 - accuracy: 0.9614 - val_loss: 0.0501 - val_accuracy: 1.0000\n",
            "Epoch 96/100\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.1119 - accuracy: 0.9328 - val_loss: 0.0461 - val_accuracy: 1.0000\n",
            "Epoch 97/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.0797 - accuracy: 0.9760 - val_loss: 0.0444 - val_accuracy: 1.0000\n",
            "Epoch 98/100\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.0769 - accuracy: 0.9698 - val_loss: 0.0570 - val_accuracy: 0.9667\n",
            "Epoch 99/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1001 - accuracy: 0.9876 - val_loss: 0.0441 - val_accuracy: 1.0000\n",
            "Epoch 100/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.0880 - accuracy: 0.9650 - val_loss: 0.0478 - val_accuracy: 1.0000\n",
            "1/1 [==============================] - 0s 261ms/step - loss: 0.0478 - accuracy: 1.0000\n",
            "Epoch 1/100\n",
            "24/24 [==============================] - 1s 15ms/step - loss: 1.7002 - accuracy: 0.5948 - val_loss: 0.9050 - val_accuracy: 0.7667\n",
            "Epoch 2/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.9569 - accuracy: 0.5789 - val_loss: 0.8368 - val_accuracy: 0.6000\n",
            "Epoch 3/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.8402 - accuracy: 0.4202 - val_loss: 0.7268 - val_accuracy: 0.6000\n",
            "Epoch 4/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.7694 - accuracy: 0.4471 - val_loss: 0.6543 - val_accuracy: 0.6667\n",
            "Epoch 5/100\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.7128 - accuracy: 0.6350 - val_loss: 0.5767 - val_accuracy: 0.7000\n",
            "Epoch 6/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.6492 - accuracy: 0.6762 - val_loss: 0.5252 - val_accuracy: 0.8333\n",
            "Epoch 7/100\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.6245 - accuracy: 0.7050 - val_loss: 0.4702 - val_accuracy: 0.9333\n",
            "Epoch 8/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.5935 - accuracy: 0.7676 - val_loss: 0.4412 - val_accuracy: 0.8333\n",
            "Epoch 9/100\n",
            "24/24 [==============================] - 0s 5ms/step - loss: 0.5707 - accuracy: 0.7755 - val_loss: 0.4054 - val_accuracy: 1.0000\n",
            "Epoch 10/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.5099 - accuracy: 0.8034 - val_loss: 0.3726 - val_accuracy: 1.0000\n",
            "Epoch 11/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.4952 - accuracy: 0.9020 - val_loss: 0.3526 - val_accuracy: 1.0000\n",
            "Epoch 12/100\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.4581 - accuracy: 0.8901 - val_loss: 0.3353 - val_accuracy: 0.9000\n",
            "Epoch 13/100\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.4706 - accuracy: 0.8697 - val_loss: 0.3297 - val_accuracy: 0.9333\n",
            "Epoch 14/100\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.4682 - accuracy: 0.7300 - val_loss: 0.2883 - val_accuracy: 1.0000\n",
            "Epoch 15/100\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.4370 - accuracy: 0.8886 - val_loss: 0.2849 - val_accuracy: 0.9667\n",
            "Epoch 16/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.4029 - accuracy: 0.8489 - val_loss: 0.2657 - val_accuracy: 0.9667\n",
            "Epoch 17/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.3704 - accuracy: 0.9357 - val_loss: 0.2511 - val_accuracy: 1.0000\n",
            "Epoch 18/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.3771 - accuracy: 0.8805 - val_loss: 0.2376 - val_accuracy: 1.0000\n",
            "Epoch 19/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.3478 - accuracy: 0.9506 - val_loss: 0.2253 - val_accuracy: 1.0000\n",
            "Epoch 20/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.2711 - accuracy: 0.9912 - val_loss: 0.2129 - val_accuracy: 1.0000\n",
            "Epoch 21/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.2857 - accuracy: 0.9386 - val_loss: 0.2036 - val_accuracy: 1.0000\n",
            "Epoch 22/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.2960 - accuracy: 0.9778 - val_loss: 0.1900 - val_accuracy: 1.0000\n",
            "Epoch 23/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.2918 - accuracy: 0.9733 - val_loss: 0.1847 - val_accuracy: 1.0000\n",
            "Epoch 24/100\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.2503 - accuracy: 0.9488 - val_loss: 0.1701 - val_accuracy: 1.0000\n",
            "Epoch 25/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.2773 - accuracy: 0.9238 - val_loss: 0.1612 - val_accuracy: 1.0000\n",
            "Epoch 26/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.2346 - accuracy: 0.9748 - val_loss: 0.1583 - val_accuracy: 1.0000\n",
            "Epoch 27/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.2536 - accuracy: 0.9608 - val_loss: 0.1451 - val_accuracy: 1.0000\n",
            "Epoch 28/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.2238 - accuracy: 0.9581 - val_loss: 0.1383 - val_accuracy: 1.0000\n",
            "Epoch 29/100\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.2204 - accuracy: 0.9448 - val_loss: 0.1351 - val_accuracy: 1.0000\n",
            "Epoch 30/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.2372 - accuracy: 0.9138 - val_loss: 0.1328 - val_accuracy: 1.0000\n",
            "Epoch 31/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.2069 - accuracy: 0.9581 - val_loss: 0.1207 - val_accuracy: 1.0000\n",
            "Epoch 32/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.2092 - accuracy: 0.9727 - val_loss: 0.1274 - val_accuracy: 1.0000\n",
            "Epoch 33/100\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.1908 - accuracy: 0.9822 - val_loss: 0.1221 - val_accuracy: 1.0000\n",
            "Epoch 34/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1933 - accuracy: 0.9685 - val_loss: 0.1115 - val_accuracy: 1.0000\n",
            "Epoch 35/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1891 - accuracy: 0.9712 - val_loss: 0.1016 - val_accuracy: 1.0000\n",
            "Epoch 36/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1658 - accuracy: 0.9889 - val_loss: 0.1098 - val_accuracy: 1.0000\n",
            "Epoch 37/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1879 - accuracy: 0.9676 - val_loss: 0.0959 - val_accuracy: 1.0000\n",
            "Epoch 38/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1716 - accuracy: 0.9496 - val_loss: 0.1162 - val_accuracy: 0.9667\n",
            "Epoch 39/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1392 - accuracy: 0.9864 - val_loss: 0.0895 - val_accuracy: 1.0000\n",
            "Epoch 40/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1710 - accuracy: 0.9665 - val_loss: 0.0937 - val_accuracy: 1.0000\n",
            "Epoch 41/100\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.1794 - accuracy: 0.9249 - val_loss: 0.1183 - val_accuracy: 0.9667\n",
            "Epoch 42/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1555 - accuracy: 0.9346 - val_loss: 0.0811 - val_accuracy: 1.0000\n",
            "Epoch 43/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.1453 - accuracy: 0.9612 - val_loss: 0.0757 - val_accuracy: 1.0000\n",
            "Epoch 44/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1508 - accuracy: 0.9639 - val_loss: 0.0791 - val_accuracy: 1.0000\n",
            "Epoch 45/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1477 - accuracy: 0.9684 - val_loss: 0.0719 - val_accuracy: 1.0000\n",
            "Epoch 46/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.1689 - accuracy: 0.9648 - val_loss: 0.0686 - val_accuracy: 1.0000\n",
            "Epoch 47/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.1508 - accuracy: 0.9643 - val_loss: 0.0842 - val_accuracy: 1.0000\n",
            "Epoch 48/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1514 - accuracy: 0.9564 - val_loss: 0.0938 - val_accuracy: 0.9667\n",
            "Epoch 49/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.1508 - accuracy: 0.9539 - val_loss: 0.0679 - val_accuracy: 1.0000\n",
            "Epoch 50/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1377 - accuracy: 0.9613 - val_loss: 0.0638 - val_accuracy: 1.0000\n",
            "Epoch 51/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1249 - accuracy: 0.9844 - val_loss: 0.0715 - val_accuracy: 1.0000\n",
            "Epoch 52/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1478 - accuracy: 0.9644 - val_loss: 0.0618 - val_accuracy: 1.0000\n",
            "Epoch 53/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1039 - accuracy: 0.9816 - val_loss: 0.0601 - val_accuracy: 1.0000\n",
            "Epoch 54/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.1399 - accuracy: 0.9611 - val_loss: 0.0632 - val_accuracy: 1.0000\n",
            "Epoch 55/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1283 - accuracy: 0.9614 - val_loss: 0.0555 - val_accuracy: 1.0000\n",
            "Epoch 56/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1141 - accuracy: 0.9629 - val_loss: 0.0542 - val_accuracy: 1.0000\n",
            "Epoch 57/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1089 - accuracy: 0.9711 - val_loss: 0.0736 - val_accuracy: 0.9667\n",
            "Epoch 58/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.1478 - accuracy: 0.9609 - val_loss: 0.0576 - val_accuracy: 1.0000\n",
            "Epoch 59/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.1043 - accuracy: 0.9754 - val_loss: 0.0516 - val_accuracy: 1.0000\n",
            "Epoch 60/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1235 - accuracy: 0.9509 - val_loss: 0.0527 - val_accuracy: 1.0000\n",
            "Epoch 61/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1200 - accuracy: 0.9816 - val_loss: 0.0498 - val_accuracy: 1.0000\n",
            "Epoch 62/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.0867 - accuracy: 0.9911 - val_loss: 0.0581 - val_accuracy: 1.0000\n",
            "Epoch 63/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1127 - accuracy: 0.9581 - val_loss: 0.0480 - val_accuracy: 1.0000\n",
            "Epoch 64/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.0887 - accuracy: 0.9907 - val_loss: 0.0627 - val_accuracy: 1.0000\n",
            "Epoch 65/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1321 - accuracy: 0.9403 - val_loss: 0.0466 - val_accuracy: 1.0000\n",
            "Epoch 66/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.0859 - accuracy: 0.9851 - val_loss: 0.0525 - val_accuracy: 1.0000\n",
            "Epoch 67/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1262 - accuracy: 0.9692 - val_loss: 0.0597 - val_accuracy: 1.0000\n",
            "Epoch 68/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.0975 - accuracy: 0.9770 - val_loss: 0.0515 - val_accuracy: 1.0000\n",
            "Epoch 69/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.0883 - accuracy: 0.9586 - val_loss: 0.0448 - val_accuracy: 1.0000\n",
            "Epoch 70/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.0763 - accuracy: 0.9873 - val_loss: 0.0573 - val_accuracy: 1.0000\n",
            "Epoch 71/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.0988 - accuracy: 0.9722 - val_loss: 0.0430 - val_accuracy: 1.0000\n",
            "Epoch 72/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1050 - accuracy: 0.9767 - val_loss: 0.0428 - val_accuracy: 1.0000\n",
            "Epoch 73/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1301 - accuracy: 0.9497 - val_loss: 0.0489 - val_accuracy: 1.0000\n",
            "Epoch 74/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.1098 - accuracy: 0.9436 - val_loss: 0.0414 - val_accuracy: 1.0000\n",
            "Epoch 75/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1377 - accuracy: 0.9413 - val_loss: 0.0458 - val_accuracy: 1.0000\n",
            "Epoch 76/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1068 - accuracy: 0.9587 - val_loss: 0.0663 - val_accuracy: 0.9667\n",
            "Epoch 77/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1018 - accuracy: 0.9633 - val_loss: 0.0400 - val_accuracy: 1.0000\n",
            "Epoch 78/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1133 - accuracy: 0.9708 - val_loss: 0.0396 - val_accuracy: 1.0000\n",
            "Epoch 79/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.0969 - accuracy: 0.9666 - val_loss: 0.0429 - val_accuracy: 1.0000\n",
            "Epoch 80/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.0586 - accuracy: 0.9916 - val_loss: 0.0407 - val_accuracy: 1.0000\n",
            "Epoch 81/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.0641 - accuracy: 0.9790 - val_loss: 0.0420 - val_accuracy: 1.0000\n",
            "Epoch 82/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.1007 - accuracy: 0.9551 - val_loss: 0.0383 - val_accuracy: 1.0000\n",
            "Epoch 83/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.0716 - accuracy: 0.9837 - val_loss: 0.0391 - val_accuracy: 1.0000\n",
            "Epoch 84/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.0735 - accuracy: 0.9659 - val_loss: 0.0414 - val_accuracy: 0.9667\n",
            "Epoch 85/100\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.1142 - accuracy: 0.9550 - val_loss: 0.0376 - val_accuracy: 1.0000\n",
            "Epoch 86/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1123 - accuracy: 0.9510 - val_loss: 0.0406 - val_accuracy: 1.0000\n",
            "Epoch 87/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.0851 - accuracy: 0.9616 - val_loss: 0.0372 - val_accuracy: 1.0000\n",
            "Epoch 88/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1027 - accuracy: 0.9681 - val_loss: 0.0369 - val_accuracy: 1.0000\n",
            "Epoch 89/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1030 - accuracy: 0.9421 - val_loss: 0.0374 - val_accuracy: 1.0000\n",
            "Epoch 90/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1085 - accuracy: 0.9486 - val_loss: 0.0378 - val_accuracy: 1.0000\n",
            "Epoch 91/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.0624 - accuracy: 0.9781 - val_loss: 0.0423 - val_accuracy: 1.0000\n",
            "Epoch 92/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1163 - accuracy: 0.9558 - val_loss: 0.0388 - val_accuracy: 1.0000\n",
            "Epoch 93/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.0712 - accuracy: 0.9772 - val_loss: 0.0400 - val_accuracy: 1.0000\n",
            "Epoch 94/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.0954 - accuracy: 0.9587 - val_loss: 0.0380 - val_accuracy: 1.0000\n",
            "Epoch 95/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.0629 - accuracy: 0.9844 - val_loss: 0.0455 - val_accuracy: 1.0000\n",
            "Epoch 96/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.0980 - accuracy: 0.9510 - val_loss: 0.0364 - val_accuracy: 1.0000\n",
            "Epoch 97/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.0612 - accuracy: 0.9861 - val_loss: 0.0408 - val_accuracy: 1.0000\n",
            "Epoch 98/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.0582 - accuracy: 0.9947 - val_loss: 0.0351 - val_accuracy: 1.0000\n",
            "Epoch 99/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.0800 - accuracy: 0.9751 - val_loss: 0.0362 - val_accuracy: 1.0000\n",
            "Epoch 100/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.0948 - accuracy: 0.9625 - val_loss: 0.0448 - val_accuracy: 1.0000\n",
            "1/1 [==============================] - 0s 292ms/step - loss: 0.0448 - accuracy: 1.0000\n",
            "Epoch 1/100\n",
            "24/24 [==============================] - 1s 14ms/step - loss: 3.9012 - accuracy: 0.3701 - val_loss: 2.5031 - val_accuracy: 0.2667\n",
            "Epoch 2/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 1.9592 - accuracy: 0.3025 - val_loss: 0.9284 - val_accuracy: 0.3000\n",
            "Epoch 3/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.8009 - accuracy: 0.5837 - val_loss: 0.7763 - val_accuracy: 0.5333\n",
            "Epoch 4/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.7457 - accuracy: 0.5590 - val_loss: 0.7235 - val_accuracy: 0.7000\n",
            "Epoch 5/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.6762 - accuracy: 0.7168 - val_loss: 0.6926 - val_accuracy: 0.7000\n",
            "Epoch 6/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.6633 - accuracy: 0.7544 - val_loss: 0.6542 - val_accuracy: 0.7333\n",
            "Epoch 7/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.6354 - accuracy: 0.7171 - val_loss: 0.6273 - val_accuracy: 0.8333\n",
            "Epoch 8/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.6348 - accuracy: 0.7985 - val_loss: 0.6165 - val_accuracy: 0.6333\n",
            "Epoch 9/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.5517 - accuracy: 0.8615 - val_loss: 0.5738 - val_accuracy: 0.7000\n",
            "Epoch 10/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.5698 - accuracy: 0.6874 - val_loss: 0.5649 - val_accuracy: 0.9667\n",
            "Epoch 11/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.4665 - accuracy: 0.9250 - val_loss: 0.5469 - val_accuracy: 0.9667\n",
            "Epoch 12/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.5299 - accuracy: 0.9412 - val_loss: 0.5293 - val_accuracy: 0.9333\n",
            "Epoch 13/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.4741 - accuracy: 0.8576 - val_loss: 0.5177 - val_accuracy: 0.9333\n",
            "Epoch 14/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.5142 - accuracy: 0.9396 - val_loss: 0.5077 - val_accuracy: 0.9667\n",
            "Epoch 15/100\n",
            "24/24 [==============================] - 0s 5ms/step - loss: 0.4842 - accuracy: 0.9558 - val_loss: 0.4924 - val_accuracy: 0.9333\n",
            "Epoch 16/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.4809 - accuracy: 0.8905 - val_loss: 0.4813 - val_accuracy: 0.9333\n",
            "Epoch 17/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.4614 - accuracy: 0.9588 - val_loss: 0.4878 - val_accuracy: 0.9000\n",
            "Epoch 18/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.4003 - accuracy: 0.9829 - val_loss: 0.4693 - val_accuracy: 0.9667\n",
            "Epoch 19/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.4194 - accuracy: 0.8716 - val_loss: 0.4490 - val_accuracy: 0.8333\n",
            "Epoch 20/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.4097 - accuracy: 0.8903 - val_loss: 0.4541 - val_accuracy: 0.9667\n",
            "Epoch 21/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.4368 - accuracy: 0.9675 - val_loss: 0.4376 - val_accuracy: 0.9333\n",
            "Epoch 22/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.4476 - accuracy: 0.8856 - val_loss: 0.4319 - val_accuracy: 0.9333\n",
            "Epoch 23/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.3671 - accuracy: 0.9513 - val_loss: 0.4310 - val_accuracy: 0.9667\n",
            "Epoch 24/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.3349 - accuracy: 0.9335 - val_loss: 0.4180 - val_accuracy: 0.9333\n",
            "Epoch 25/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.3784 - accuracy: 0.9550 - val_loss: 0.4127 - val_accuracy: 0.9667\n",
            "Epoch 26/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.3623 - accuracy: 0.9423 - val_loss: 0.4126 - val_accuracy: 0.9667\n",
            "Epoch 27/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.3826 - accuracy: 0.9512 - val_loss: 0.3993 - val_accuracy: 0.9667\n",
            "Epoch 28/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.3745 - accuracy: 0.9855 - val_loss: 0.3814 - val_accuracy: 0.9333\n",
            "Epoch 29/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.3706 - accuracy: 0.9527 - val_loss: 0.3777 - val_accuracy: 0.9333\n",
            "Epoch 30/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.3283 - accuracy: 0.9666 - val_loss: 0.3747 - val_accuracy: 0.9667\n",
            "Epoch 31/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.3676 - accuracy: 0.9656 - val_loss: 0.3604 - val_accuracy: 0.9333\n",
            "Epoch 32/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.3014 - accuracy: 0.9834 - val_loss: 0.3709 - val_accuracy: 0.9000\n",
            "Epoch 33/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.3454 - accuracy: 0.9884 - val_loss: 0.3485 - val_accuracy: 0.9667\n",
            "Epoch 34/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.3005 - accuracy: 0.9843 - val_loss: 0.3592 - val_accuracy: 0.9000\n",
            "Epoch 35/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.3464 - accuracy: 0.9538 - val_loss: 0.3357 - val_accuracy: 0.9667\n",
            "Epoch 36/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.2940 - accuracy: 0.9625 - val_loss: 0.3255 - val_accuracy: 0.9667\n",
            "Epoch 37/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.2861 - accuracy: 0.9545 - val_loss: 0.3200 - val_accuracy: 0.9667\n",
            "Epoch 38/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.2778 - accuracy: 0.9701 - val_loss: 0.3193 - val_accuracy: 0.9667\n",
            "Epoch 39/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.2710 - accuracy: 0.9825 - val_loss: 0.3091 - val_accuracy: 0.9667\n",
            "Epoch 40/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.2730 - accuracy: 0.9656 - val_loss: 0.2971 - val_accuracy: 0.9667\n",
            "Epoch 41/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.2594 - accuracy: 0.9790 - val_loss: 0.2858 - val_accuracy: 0.9333\n",
            "Epoch 42/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.2294 - accuracy: 0.9781 - val_loss: 0.2847 - val_accuracy: 0.9667\n",
            "Epoch 43/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.2391 - accuracy: 0.9869 - val_loss: 0.2776 - val_accuracy: 0.9667\n",
            "Epoch 44/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.2373 - accuracy: 0.9880 - val_loss: 0.2734 - val_accuracy: 0.9667\n",
            "Epoch 45/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.2360 - accuracy: 0.9730 - val_loss: 0.2591 - val_accuracy: 0.9667\n",
            "Epoch 46/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.2381 - accuracy: 0.9607 - val_loss: 0.2532 - val_accuracy: 0.9667\n",
            "Epoch 47/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.2205 - accuracy: 0.9943 - val_loss: 0.2452 - val_accuracy: 0.9667\n",
            "Epoch 48/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.2184 - accuracy: 0.9878 - val_loss: 0.2611 - val_accuracy: 0.9333\n",
            "Epoch 49/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1893 - accuracy: 0.9619 - val_loss: 0.2331 - val_accuracy: 0.9667\n",
            "Epoch 50/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.1959 - accuracy: 0.9930 - val_loss: 0.2335 - val_accuracy: 0.9667\n",
            "Epoch 51/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.2002 - accuracy: 0.9743 - val_loss: 0.2255 - val_accuracy: 0.9667\n",
            "Epoch 52/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1607 - accuracy: 0.9872 - val_loss: 0.2181 - val_accuracy: 0.9667\n",
            "Epoch 53/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1869 - accuracy: 0.9908 - val_loss: 0.2148 - val_accuracy: 0.9667\n",
            "Epoch 54/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.1632 - accuracy: 0.9901 - val_loss: 0.2069 - val_accuracy: 0.9667\n",
            "Epoch 55/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1645 - accuracy: 0.9811 - val_loss: 0.2002 - val_accuracy: 0.9667\n",
            "Epoch 56/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.1677 - accuracy: 0.9835 - val_loss: 0.2007 - val_accuracy: 0.9667\n",
            "Epoch 57/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.1946 - accuracy: 0.9423 - val_loss: 0.1898 - val_accuracy: 0.9667\n",
            "Epoch 58/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.1467 - accuracy: 0.9855 - val_loss: 0.2112 - val_accuracy: 0.9333\n",
            "Epoch 59/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1892 - accuracy: 0.9362 - val_loss: 0.1802 - val_accuracy: 0.9667\n",
            "Epoch 60/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1572 - accuracy: 0.9743 - val_loss: 0.1762 - val_accuracy: 0.9667\n",
            "Epoch 61/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1758 - accuracy: 0.9426 - val_loss: 0.1721 - val_accuracy: 0.9667\n",
            "Epoch 62/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1430 - accuracy: 0.9681 - val_loss: 0.1705 - val_accuracy: 0.9667\n",
            "Epoch 63/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1580 - accuracy: 0.9921 - val_loss: 0.1752 - val_accuracy: 0.9667\n",
            "Epoch 64/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1532 - accuracy: 0.9811 - val_loss: 0.1609 - val_accuracy: 0.9667\n",
            "Epoch 65/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.1142 - accuracy: 0.9850 - val_loss: 0.1785 - val_accuracy: 0.9333\n",
            "Epoch 66/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1371 - accuracy: 0.9361 - val_loss: 0.1547 - val_accuracy: 0.9667\n",
            "Epoch 67/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1038 - accuracy: 0.9938 - val_loss: 0.1515 - val_accuracy: 0.9667\n",
            "Epoch 68/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1181 - accuracy: 0.9909 - val_loss: 0.1484 - val_accuracy: 0.9667\n",
            "Epoch 69/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.1105 - accuracy: 0.9759 - val_loss: 0.1475 - val_accuracy: 0.9667\n",
            "Epoch 70/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1451 - accuracy: 0.9796 - val_loss: 0.1486 - val_accuracy: 0.9667\n",
            "Epoch 71/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.1353 - accuracy: 0.9629 - val_loss: 0.1405 - val_accuracy: 0.9667\n",
            "Epoch 72/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.0926 - accuracy: 0.9946 - val_loss: 0.1390 - val_accuracy: 0.9667\n",
            "Epoch 73/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.1090 - accuracy: 0.9921 - val_loss: 0.1362 - val_accuracy: 0.9667\n",
            "Epoch 74/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1105 - accuracy: 0.9657 - val_loss: 0.1436 - val_accuracy: 0.9667\n",
            "Epoch 75/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.0980 - accuracy: 0.9779 - val_loss: 0.1318 - val_accuracy: 0.9667\n",
            "Epoch 76/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.1413 - accuracy: 0.9408 - val_loss: 0.1290 - val_accuracy: 0.9667\n",
            "Epoch 77/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.1430 - accuracy: 0.9597 - val_loss: 0.1360 - val_accuracy: 0.9333\n",
            "Epoch 78/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1336 - accuracy: 0.9835 - val_loss: 0.1266 - val_accuracy: 0.9667\n",
            "Epoch 79/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.0746 - accuracy: 0.9791 - val_loss: 0.1252 - val_accuracy: 0.9667\n",
            "Epoch 80/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1384 - accuracy: 0.9498 - val_loss: 0.1214 - val_accuracy: 0.9667\n",
            "Epoch 81/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1128 - accuracy: 0.9613 - val_loss: 0.1196 - val_accuracy: 0.9667\n",
            "Epoch 82/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.0791 - accuracy: 0.9684 - val_loss: 0.1183 - val_accuracy: 0.9667\n",
            "Epoch 83/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1205 - accuracy: 0.9692 - val_loss: 0.1253 - val_accuracy: 1.0000\n",
            "Epoch 84/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.1168 - accuracy: 0.9239 - val_loss: 0.1156 - val_accuracy: 0.9667\n",
            "Epoch 85/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.1163 - accuracy: 0.9559 - val_loss: 0.1193 - val_accuracy: 0.9667\n",
            "Epoch 86/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1072 - accuracy: 0.9790 - val_loss: 0.1177 - val_accuracy: 0.9667\n",
            "Epoch 87/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.0931 - accuracy: 0.9648 - val_loss: 0.1140 - val_accuracy: 0.9667\n",
            "Epoch 88/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.0872 - accuracy: 0.9816 - val_loss: 0.1099 - val_accuracy: 0.9667\n",
            "Epoch 89/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.0766 - accuracy: 0.9890 - val_loss: 0.1251 - val_accuracy: 0.9333\n",
            "Epoch 90/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.0964 - accuracy: 0.9471 - val_loss: 0.1075 - val_accuracy: 0.9667\n",
            "Epoch 91/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1088 - accuracy: 0.9416 - val_loss: 0.1064 - val_accuracy: 0.9667\n",
            "Epoch 92/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.1079 - accuracy: 0.9692 - val_loss: 0.1048 - val_accuracy: 0.9667\n",
            "Epoch 93/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.0685 - accuracy: 0.9898 - val_loss: 0.1054 - val_accuracy: 0.9667\n",
            "Epoch 94/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.0773 - accuracy: 0.9704 - val_loss: 0.1043 - val_accuracy: 0.9667\n",
            "Epoch 95/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.1326 - accuracy: 0.9628 - val_loss: 0.1114 - val_accuracy: 0.9667\n",
            "Epoch 96/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.0722 - accuracy: 0.9948 - val_loss: 0.1019 - val_accuracy: 0.9667\n",
            "Epoch 97/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.0729 - accuracy: 0.9894 - val_loss: 0.1016 - val_accuracy: 0.9667\n",
            "Epoch 98/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.0605 - accuracy: 0.9953 - val_loss: 0.1068 - val_accuracy: 1.0000\n",
            "Epoch 99/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1136 - accuracy: 0.9379 - val_loss: 0.0998 - val_accuracy: 0.9667\n",
            "Epoch 100/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.0892 - accuracy: 0.9436 - val_loss: 0.0994 - val_accuracy: 0.9667\n",
            "1/1 [==============================] - 0s 294ms/step - loss: 0.0994 - accuracy: 0.9667\n",
            "Epoch 1/100\n",
            "24/24 [==============================] - 1s 14ms/step - loss: 1.2889 - accuracy: 0.3969 - val_loss: 1.0186 - val_accuracy: 0.4667\n",
            "Epoch 2/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.9748 - accuracy: 0.4704 - val_loss: 0.8859 - val_accuracy: 0.6667\n",
            "Epoch 3/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.8619 - accuracy: 0.6701 - val_loss: 0.7886 - val_accuracy: 0.6667\n",
            "Epoch 4/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.7772 - accuracy: 0.7020 - val_loss: 0.7115 - val_accuracy: 0.7333\n",
            "Epoch 5/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.6986 - accuracy: 0.7178 - val_loss: 0.6503 - val_accuracy: 0.7667\n",
            "Epoch 6/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.6519 - accuracy: 0.7426 - val_loss: 0.5959 - val_accuracy: 0.7667\n",
            "Epoch 7/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.6176 - accuracy: 0.7419 - val_loss: 0.5490 - val_accuracy: 0.8000\n",
            "Epoch 8/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.5991 - accuracy: 0.7262 - val_loss: 0.5136 - val_accuracy: 0.8000\n",
            "Epoch 9/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.5331 - accuracy: 0.7765 - val_loss: 0.4823 - val_accuracy: 0.9667\n",
            "Epoch 10/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.4834 - accuracy: 0.8521 - val_loss: 0.4566 - val_accuracy: 0.9000\n",
            "Epoch 11/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.4577 - accuracy: 0.8559 - val_loss: 0.4336 - val_accuracy: 0.9333\n",
            "Epoch 12/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.4582 - accuracy: 0.9218 - val_loss: 0.4130 - val_accuracy: 0.9667\n",
            "Epoch 13/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.4629 - accuracy: 0.8833 - val_loss: 0.3970 - val_accuracy: 0.9333\n",
            "Epoch 14/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.4501 - accuracy: 0.8854 - val_loss: 0.3748 - val_accuracy: 0.9667\n",
            "Epoch 15/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.4105 - accuracy: 0.9207 - val_loss: 0.3576 - val_accuracy: 0.9667\n",
            "Epoch 16/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.3950 - accuracy: 0.9258 - val_loss: 0.3423 - val_accuracy: 0.9667\n",
            "Epoch 17/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.3807 - accuracy: 0.9145 - val_loss: 0.3250 - val_accuracy: 0.9667\n",
            "Epoch 18/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.3557 - accuracy: 0.9159 - val_loss: 0.3093 - val_accuracy: 1.0000\n",
            "Epoch 19/100\n",
            "24/24 [==============================] - 0s 5ms/step - loss: 0.3664 - accuracy: 0.9097 - val_loss: 0.2979 - val_accuracy: 0.9667\n",
            "Epoch 20/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.3389 - accuracy: 0.9501 - val_loss: 0.2820 - val_accuracy: 0.9667\n",
            "Epoch 21/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.3251 - accuracy: 0.9370 - val_loss: 0.2746 - val_accuracy: 0.9667\n",
            "Epoch 22/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.3532 - accuracy: 0.9059 - val_loss: 0.2514 - val_accuracy: 1.0000\n",
            "Epoch 23/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.2983 - accuracy: 0.9428 - val_loss: 0.2429 - val_accuracy: 0.9667\n",
            "Epoch 24/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.2930 - accuracy: 0.9318 - val_loss: 0.2317 - val_accuracy: 0.9667\n",
            "Epoch 25/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.3061 - accuracy: 0.8729 - val_loss: 0.2263 - val_accuracy: 0.9667\n",
            "Epoch 26/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.2632 - accuracy: 0.9360 - val_loss: 0.2059 - val_accuracy: 1.0000\n",
            "Epoch 27/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.2762 - accuracy: 0.9594 - val_loss: 0.1973 - val_accuracy: 1.0000\n",
            "Epoch 28/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.2728 - accuracy: 0.9429 - val_loss: 0.1805 - val_accuracy: 1.0000\n",
            "Epoch 29/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.2137 - accuracy: 0.9779 - val_loss: 0.1709 - val_accuracy: 1.0000\n",
            "Epoch 30/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.2182 - accuracy: 0.9579 - val_loss: 0.1699 - val_accuracy: 1.0000\n",
            "Epoch 31/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.2120 - accuracy: 0.9555 - val_loss: 0.1557 - val_accuracy: 1.0000\n",
            "Epoch 32/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.1831 - accuracy: 0.9699 - val_loss: 0.1537 - val_accuracy: 1.0000\n",
            "Epoch 33/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.1964 - accuracy: 0.9567 - val_loss: 0.1371 - val_accuracy: 1.0000\n",
            "Epoch 34/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.2073 - accuracy: 0.9668 - val_loss: 0.1300 - val_accuracy: 1.0000\n",
            "Epoch 35/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1687 - accuracy: 0.9757 - val_loss: 0.1433 - val_accuracy: 1.0000\n",
            "Epoch 36/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.2017 - accuracy: 0.9685 - val_loss: 0.1265 - val_accuracy: 1.0000\n",
            "Epoch 37/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1932 - accuracy: 0.9515 - val_loss: 0.1184 - val_accuracy: 1.0000\n",
            "Epoch 38/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1845 - accuracy: 0.9519 - val_loss: 0.1094 - val_accuracy: 1.0000\n",
            "Epoch 39/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1577 - accuracy: 0.9649 - val_loss: 0.1010 - val_accuracy: 1.0000\n",
            "Epoch 40/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1809 - accuracy: 0.9879 - val_loss: 0.0964 - val_accuracy: 1.0000\n",
            "Epoch 41/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1563 - accuracy: 0.9429 - val_loss: 0.0983 - val_accuracy: 1.0000\n",
            "Epoch 42/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.1737 - accuracy: 0.9649 - val_loss: 0.0915 - val_accuracy: 1.0000\n",
            "Epoch 43/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1568 - accuracy: 0.9638 - val_loss: 0.0851 - val_accuracy: 1.0000\n",
            "Epoch 44/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1510 - accuracy: 0.9835 - val_loss: 0.0874 - val_accuracy: 1.0000\n",
            "Epoch 45/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1251 - accuracy: 0.9801 - val_loss: 0.0772 - val_accuracy: 1.0000\n",
            "Epoch 46/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.1518 - accuracy: 0.9796 - val_loss: 0.0751 - val_accuracy: 1.0000\n",
            "Epoch 47/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1231 - accuracy: 0.9692 - val_loss: 0.0721 - val_accuracy: 1.0000\n",
            "Epoch 48/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1391 - accuracy: 0.9886 - val_loss: 0.0684 - val_accuracy: 1.0000\n",
            "Epoch 49/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.1106 - accuracy: 0.9931 - val_loss: 0.0723 - val_accuracy: 1.0000\n",
            "Epoch 50/100\n",
            "24/24 [==============================] - 0s 5ms/step - loss: 0.1432 - accuracy: 0.9706 - val_loss: 0.0651 - val_accuracy: 1.0000\n",
            "Epoch 51/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1166 - accuracy: 0.9809 - val_loss: 0.0633 - val_accuracy: 1.0000\n",
            "Epoch 52/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.1465 - accuracy: 0.9683 - val_loss: 0.0674 - val_accuracy: 1.0000\n",
            "Epoch 53/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1604 - accuracy: 0.9458 - val_loss: 0.0586 - val_accuracy: 1.0000\n",
            "Epoch 54/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.1180 - accuracy: 0.9808 - val_loss: 0.0560 - val_accuracy: 1.0000\n",
            "Epoch 55/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1236 - accuracy: 0.9787 - val_loss: 0.0587 - val_accuracy: 1.0000\n",
            "Epoch 56/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.1109 - accuracy: 0.9829 - val_loss: 0.0509 - val_accuracy: 1.0000\n",
            "Epoch 57/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1505 - accuracy: 0.9794 - val_loss: 0.0500 - val_accuracy: 1.0000\n",
            "Epoch 58/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.0872 - accuracy: 0.9935 - val_loss: 0.0495 - val_accuracy: 1.0000\n",
            "Epoch 59/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1057 - accuracy: 0.9815 - val_loss: 0.0463 - val_accuracy: 1.0000\n",
            "Epoch 60/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1217 - accuracy: 0.9650 - val_loss: 0.0465 - val_accuracy: 1.0000\n",
            "Epoch 61/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.1134 - accuracy: 0.9580 - val_loss: 0.0445 - val_accuracy: 1.0000\n",
            "Epoch 62/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1199 - accuracy: 0.9750 - val_loss: 0.0457 - val_accuracy: 1.0000\n",
            "Epoch 63/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1010 - accuracy: 0.9570 - val_loss: 0.0439 - val_accuracy: 1.0000\n",
            "Epoch 64/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.0960 - accuracy: 0.9827 - val_loss: 0.0397 - val_accuracy: 1.0000\n",
            "Epoch 65/100\n",
            "24/24 [==============================] - 0s 5ms/step - loss: 0.1014 - accuracy: 0.9778 - val_loss: 0.0415 - val_accuracy: 1.0000\n",
            "Epoch 66/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1370 - accuracy: 0.9751 - val_loss: 0.0405 - val_accuracy: 1.0000\n",
            "Epoch 67/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1216 - accuracy: 0.9378 - val_loss: 0.0379 - val_accuracy: 1.0000\n",
            "Epoch 68/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.1040 - accuracy: 0.9596 - val_loss: 0.0403 - val_accuracy: 1.0000\n",
            "Epoch 69/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1000 - accuracy: 0.9716 - val_loss: 0.0342 - val_accuracy: 1.0000\n",
            "Epoch 70/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1096 - accuracy: 0.9686 - val_loss: 0.0353 - val_accuracy: 1.0000\n",
            "Epoch 71/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.1124 - accuracy: 0.9601 - val_loss: 0.0386 - val_accuracy: 1.0000\n",
            "Epoch 72/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.0954 - accuracy: 0.9846 - val_loss: 0.0385 - val_accuracy: 1.0000\n",
            "Epoch 73/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.0793 - accuracy: 0.9694 - val_loss: 0.0324 - val_accuracy: 1.0000\n",
            "Epoch 74/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.0852 - accuracy: 0.9737 - val_loss: 0.0338 - val_accuracy: 1.0000\n",
            "Epoch 75/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.0879 - accuracy: 0.9574 - val_loss: 0.0299 - val_accuracy: 1.0000\n",
            "Epoch 76/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.1135 - accuracy: 0.9398 - val_loss: 0.0412 - val_accuracy: 1.0000\n",
            "Epoch 77/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.0839 - accuracy: 0.9554 - val_loss: 0.0294 - val_accuracy: 1.0000\n",
            "Epoch 78/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1016 - accuracy: 0.9831 - val_loss: 0.0342 - val_accuracy: 1.0000\n",
            "Epoch 79/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.0900 - accuracy: 0.9739 - val_loss: 0.0271 - val_accuracy: 1.0000\n",
            "Epoch 80/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.0983 - accuracy: 0.9845 - val_loss: 0.0270 - val_accuracy: 1.0000\n",
            "Epoch 81/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1114 - accuracy: 0.9759 - val_loss: 0.0262 - val_accuracy: 1.0000\n",
            "Epoch 82/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1174 - accuracy: 0.9303 - val_loss: 0.0250 - val_accuracy: 1.0000\n",
            "Epoch 83/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.0839 - accuracy: 0.9650 - val_loss: 0.0317 - val_accuracy: 1.0000\n",
            "Epoch 84/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.0889 - accuracy: 0.9608 - val_loss: 0.0262 - val_accuracy: 1.0000\n",
            "Epoch 85/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.0800 - accuracy: 0.9659 - val_loss: 0.0253 - val_accuracy: 1.0000\n",
            "Epoch 86/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.0826 - accuracy: 0.9764 - val_loss: 0.0256 - val_accuracy: 1.0000\n",
            "Epoch 87/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.0901 - accuracy: 0.9817 - val_loss: 0.0305 - val_accuracy: 1.0000\n",
            "Epoch 88/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.0721 - accuracy: 0.9783 - val_loss: 0.0219 - val_accuracy: 1.0000\n",
            "Epoch 89/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.0944 - accuracy: 0.9783 - val_loss: 0.0227 - val_accuracy: 1.0000\n",
            "Epoch 90/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.0536 - accuracy: 0.9865 - val_loss: 0.0211 - val_accuracy: 1.0000\n",
            "Epoch 91/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.0950 - accuracy: 0.9491 - val_loss: 0.0289 - val_accuracy: 1.0000\n",
            "Epoch 92/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.0830 - accuracy: 0.9840 - val_loss: 0.0207 - val_accuracy: 1.0000\n",
            "Epoch 93/100\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.1064 - accuracy: 0.9326 - val_loss: 0.0244 - val_accuracy: 1.0000\n",
            "Epoch 94/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.0834 - accuracy: 0.9760 - val_loss: 0.0218 - val_accuracy: 1.0000\n",
            "Epoch 95/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.0726 - accuracy: 0.9824 - val_loss: 0.0192 - val_accuracy: 1.0000\n",
            "Epoch 96/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.0781 - accuracy: 0.9769 - val_loss: 0.0195 - val_accuracy: 1.0000\n",
            "Epoch 97/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.0869 - accuracy: 0.9669 - val_loss: 0.0281 - val_accuracy: 1.0000\n",
            "Epoch 98/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.0528 - accuracy: 0.9817 - val_loss: 0.0180 - val_accuracy: 1.0000\n",
            "Epoch 99/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.0922 - accuracy: 0.9694 - val_loss: 0.0181 - val_accuracy: 1.0000\n",
            "Epoch 100/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.0672 - accuracy: 0.9942 - val_loss: 0.0174 - val_accuracy: 1.0000\n",
            "1/1 [==============================] - 0s 322ms/step - loss: 0.0174 - accuracy: 1.0000\n",
            "Epoch 1/100\n",
            "24/24 [==============================] - 1s 14ms/step - loss: 1.4153 - accuracy: 0.3240 - val_loss: 1.3357 - val_accuracy: 0.1667\n",
            "Epoch 2/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 1.1663 - accuracy: 0.3035 - val_loss: 1.1670 - val_accuracy: 0.1667\n",
            "Epoch 3/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 1.0398 - accuracy: 0.4356 - val_loss: 1.0282 - val_accuracy: 0.1667\n",
            "Epoch 4/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.9366 - accuracy: 0.3970 - val_loss: 0.9034 - val_accuracy: 0.5667\n",
            "Epoch 5/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.8489 - accuracy: 0.7763 - val_loss: 0.8087 - val_accuracy: 0.7000\n",
            "Epoch 6/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.7920 - accuracy: 0.8080 - val_loss: 0.7227 - val_accuracy: 0.9000\n",
            "Epoch 7/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.7222 - accuracy: 0.7974 - val_loss: 0.6471 - val_accuracy: 0.9000\n",
            "Epoch 8/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.6917 - accuracy: 0.8004 - val_loss: 0.5688 - val_accuracy: 1.0000\n",
            "Epoch 9/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.6212 - accuracy: 0.9296 - val_loss: 0.5171 - val_accuracy: 1.0000\n",
            "Epoch 10/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.5740 - accuracy: 0.9881 - val_loss: 0.4720 - val_accuracy: 1.0000\n",
            "Epoch 11/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.5370 - accuracy: 0.9362 - val_loss: 0.4375 - val_accuracy: 1.0000\n",
            "Epoch 12/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.5267 - accuracy: 0.9296 - val_loss: 0.3949 - val_accuracy: 1.0000\n",
            "Epoch 13/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.4919 - accuracy: 0.9354 - val_loss: 0.3796 - val_accuracy: 1.0000\n",
            "Epoch 14/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.4765 - accuracy: 0.9668 - val_loss: 0.3618 - val_accuracy: 1.0000\n",
            "Epoch 15/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.4063 - accuracy: 0.9793 - val_loss: 0.3265 - val_accuracy: 1.0000\n",
            "Epoch 16/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.3871 - accuracy: 0.9529 - val_loss: 0.3337 - val_accuracy: 1.0000\n",
            "Epoch 17/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.4094 - accuracy: 0.9631 - val_loss: 0.3014 - val_accuracy: 1.0000\n",
            "Epoch 18/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.3720 - accuracy: 0.9504 - val_loss: 0.2787 - val_accuracy: 1.0000\n",
            "Epoch 19/100\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.3580 - accuracy: 0.9622 - val_loss: 0.2770 - val_accuracy: 1.0000\n",
            "Epoch 20/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.3780 - accuracy: 0.9023 - val_loss: 0.2458 - val_accuracy: 1.0000\n",
            "Epoch 21/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.3734 - accuracy: 0.9395 - val_loss: 0.2472 - val_accuracy: 1.0000\n",
            "Epoch 22/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.3598 - accuracy: 0.9259 - val_loss: 0.2348 - val_accuracy: 1.0000\n",
            "Epoch 23/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.3117 - accuracy: 0.9124 - val_loss: 0.2142 - val_accuracy: 1.0000\n",
            "Epoch 24/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.3426 - accuracy: 0.9220 - val_loss: 0.2079 - val_accuracy: 1.0000\n",
            "Epoch 25/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.2854 - accuracy: 0.9575 - val_loss: 0.1995 - val_accuracy: 1.0000\n",
            "Epoch 26/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.2602 - accuracy: 0.9798 - val_loss: 0.1864 - val_accuracy: 1.0000\n",
            "Epoch 27/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.2444 - accuracy: 0.9789 - val_loss: 0.1852 - val_accuracy: 1.0000\n",
            "Epoch 28/100\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.2194 - accuracy: 0.9887 - val_loss: 0.1734 - val_accuracy: 1.0000\n",
            "Epoch 29/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.2450 - accuracy: 0.9747 - val_loss: 0.1801 - val_accuracy: 1.0000\n",
            "Epoch 30/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.2278 - accuracy: 0.9310 - val_loss: 0.1535 - val_accuracy: 1.0000\n",
            "Epoch 31/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1975 - accuracy: 0.9697 - val_loss: 0.1673 - val_accuracy: 1.0000\n",
            "Epoch 32/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.1931 - accuracy: 0.9800 - val_loss: 0.1385 - val_accuracy: 1.0000\n",
            "Epoch 33/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1997 - accuracy: 0.9521 - val_loss: 0.1322 - val_accuracy: 1.0000\n",
            "Epoch 34/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.1815 - accuracy: 0.9887 - val_loss: 0.1362 - val_accuracy: 1.0000\n",
            "Epoch 35/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1690 - accuracy: 0.9702 - val_loss: 0.1335 - val_accuracy: 1.0000\n",
            "Epoch 36/100\n",
            "24/24 [==============================] - 0s 5ms/step - loss: 0.1920 - accuracy: 0.9257 - val_loss: 0.1138 - val_accuracy: 1.0000\n",
            "Epoch 37/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1602 - accuracy: 0.9679 - val_loss: 0.1232 - val_accuracy: 1.0000\n",
            "Epoch 38/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1593 - accuracy: 0.9919 - val_loss: 0.0913 - val_accuracy: 1.0000\n",
            "Epoch 39/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.1446 - accuracy: 0.9781 - val_loss: 0.1085 - val_accuracy: 1.0000\n",
            "Epoch 40/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1349 - accuracy: 0.9778 - val_loss: 0.1081 - val_accuracy: 1.0000\n",
            "Epoch 41/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.1315 - accuracy: 0.9553 - val_loss: 0.0803 - val_accuracy: 1.0000\n",
            "Epoch 42/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.1806 - accuracy: 0.9618 - val_loss: 0.1062 - val_accuracy: 1.0000\n",
            "Epoch 43/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.1612 - accuracy: 0.9769 - val_loss: 0.0960 - val_accuracy: 1.0000\n",
            "Epoch 44/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1302 - accuracy: 0.9774 - val_loss: 0.0744 - val_accuracy: 1.0000\n",
            "Epoch 45/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1291 - accuracy: 0.9595 - val_loss: 0.0724 - val_accuracy: 1.0000\n",
            "Epoch 46/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.1229 - accuracy: 0.9863 - val_loss: 0.0857 - val_accuracy: 1.0000\n",
            "Epoch 47/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.1059 - accuracy: 0.9911 - val_loss: 0.0781 - val_accuracy: 1.0000\n",
            "Epoch 48/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1567 - accuracy: 0.9717 - val_loss: 0.0718 - val_accuracy: 1.0000\n",
            "Epoch 49/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1358 - accuracy: 0.9688 - val_loss: 0.0888 - val_accuracy: 1.0000\n",
            "Epoch 50/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1175 - accuracy: 0.9728 - val_loss: 0.0508 - val_accuracy: 1.0000\n",
            "Epoch 51/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1025 - accuracy: 0.9765 - val_loss: 0.1140 - val_accuracy: 0.9667\n",
            "Epoch 52/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1687 - accuracy: 0.9140 - val_loss: 0.0528 - val_accuracy: 1.0000\n",
            "Epoch 53/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1273 - accuracy: 0.9591 - val_loss: 0.0719 - val_accuracy: 1.0000\n",
            "Epoch 54/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.1260 - accuracy: 0.9800 - val_loss: 0.0622 - val_accuracy: 1.0000\n",
            "Epoch 55/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.0949 - accuracy: 0.9890 - val_loss: 0.0714 - val_accuracy: 1.0000\n",
            "Epoch 56/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.1003 - accuracy: 0.9522 - val_loss: 0.0495 - val_accuracy: 1.0000\n",
            "Epoch 57/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1065 - accuracy: 0.9798 - val_loss: 0.0888 - val_accuracy: 1.0000\n",
            "Epoch 58/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.1610 - accuracy: 0.8940 - val_loss: 0.0418 - val_accuracy: 1.0000\n",
            "Epoch 59/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1173 - accuracy: 0.9489 - val_loss: 0.0479 - val_accuracy: 1.0000\n",
            "Epoch 60/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1502 - accuracy: 0.9478 - val_loss: 0.0391 - val_accuracy: 1.0000\n",
            "Epoch 61/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.1049 - accuracy: 0.9682 - val_loss: 0.0451 - val_accuracy: 1.0000\n",
            "Epoch 62/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.0852 - accuracy: 0.9912 - val_loss: 0.0564 - val_accuracy: 1.0000\n",
            "Epoch 63/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.1108 - accuracy: 0.9787 - val_loss: 0.0424 - val_accuracy: 1.0000\n",
            "Epoch 64/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.0962 - accuracy: 0.9759 - val_loss: 0.1010 - val_accuracy: 0.9667\n",
            "Epoch 65/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1468 - accuracy: 0.9166 - val_loss: 0.0444 - val_accuracy: 1.0000\n",
            "Epoch 66/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.0915 - accuracy: 0.9625 - val_loss: 0.0427 - val_accuracy: 1.0000\n",
            "Epoch 67/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.1181 - accuracy: 0.9689 - val_loss: 0.0317 - val_accuracy: 1.0000\n",
            "Epoch 68/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.0716 - accuracy: 0.9796 - val_loss: 0.0591 - val_accuracy: 1.0000\n",
            "Epoch 69/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.0806 - accuracy: 0.9526 - val_loss: 0.0321 - val_accuracy: 1.0000\n",
            "Epoch 70/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1271 - accuracy: 0.9382 - val_loss: 0.0298 - val_accuracy: 1.0000\n",
            "Epoch 71/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.0962 - accuracy: 0.9708 - val_loss: 0.0554 - val_accuracy: 1.0000\n",
            "Epoch 72/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.0863 - accuracy: 0.9809 - val_loss: 0.0335 - val_accuracy: 1.0000\n",
            "Epoch 73/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.0900 - accuracy: 0.9592 - val_loss: 0.0413 - val_accuracy: 1.0000\n",
            "Epoch 74/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.0742 - accuracy: 0.9663 - val_loss: 0.0344 - val_accuracy: 1.0000\n",
            "Epoch 75/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1010 - accuracy: 0.9782 - val_loss: 0.0648 - val_accuracy: 1.0000\n",
            "Epoch 76/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.0801 - accuracy: 0.9528 - val_loss: 0.0544 - val_accuracy: 1.0000\n",
            "Epoch 77/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.0885 - accuracy: 0.9865 - val_loss: 0.0293 - val_accuracy: 1.0000\n",
            "Epoch 78/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.0867 - accuracy: 0.9638 - val_loss: 0.0398 - val_accuracy: 1.0000\n",
            "Epoch 79/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.1039 - accuracy: 0.9839 - val_loss: 0.0459 - val_accuracy: 1.0000\n",
            "Epoch 80/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.0603 - accuracy: 0.9772 - val_loss: 0.0207 - val_accuracy: 1.0000\n",
            "Epoch 81/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1423 - accuracy: 0.9292 - val_loss: 0.0449 - val_accuracy: 1.0000\n",
            "Epoch 82/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.0645 - accuracy: 0.9888 - val_loss: 0.0370 - val_accuracy: 1.0000\n",
            "Epoch 83/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.0719 - accuracy: 0.9746 - val_loss: 0.0452 - val_accuracy: 1.0000\n",
            "Epoch 84/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.0596 - accuracy: 0.9827 - val_loss: 0.0173 - val_accuracy: 1.0000\n",
            "Epoch 85/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1618 - accuracy: 0.9627 - val_loss: 0.0276 - val_accuracy: 1.0000\n",
            "Epoch 86/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.0696 - accuracy: 0.9860 - val_loss: 0.0316 - val_accuracy: 1.0000\n",
            "Epoch 87/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.0990 - accuracy: 0.9716 - val_loss: 0.0354 - val_accuracy: 1.0000\n",
            "Epoch 88/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.0969 - accuracy: 0.9955 - val_loss: 0.0221 - val_accuracy: 1.0000\n",
            "Epoch 89/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.0868 - accuracy: 0.9729 - val_loss: 0.0362 - val_accuracy: 1.0000\n",
            "Epoch 90/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.0596 - accuracy: 0.9957 - val_loss: 0.0333 - val_accuracy: 1.0000\n",
            "Epoch 91/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.0892 - accuracy: 0.9614 - val_loss: 0.0416 - val_accuracy: 1.0000\n",
            "Epoch 92/100\n",
            "24/24 [==============================] - 0s 5ms/step - loss: 0.0658 - accuracy: 0.9626 - val_loss: 0.0293 - val_accuracy: 1.0000\n",
            "Epoch 93/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.0449 - accuracy: 0.9931 - val_loss: 0.0386 - val_accuracy: 1.0000\n",
            "Epoch 94/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.0645 - accuracy: 0.9612 - val_loss: 0.0226 - val_accuracy: 1.0000\n",
            "Epoch 95/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.0621 - accuracy: 0.9760 - val_loss: 0.0333 - val_accuracy: 1.0000\n",
            "Epoch 96/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.0798 - accuracy: 0.9803 - val_loss: 0.0220 - val_accuracy: 1.0000\n",
            "Epoch 97/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.0795 - accuracy: 0.9820 - val_loss: 0.0335 - val_accuracy: 1.0000\n",
            "Epoch 98/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.0576 - accuracy: 0.9933 - val_loss: 0.0281 - val_accuracy: 1.0000\n",
            "Epoch 99/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1458 - accuracy: 0.9325 - val_loss: 0.0159 - val_accuracy: 1.0000\n",
            "Epoch 100/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.0715 - accuracy: 0.9813 - val_loss: 0.0477 - val_accuracy: 1.0000\n",
            "1/1 [==============================] - 1s 794ms/step - loss: 0.0477 - accuracy: 1.0000\n",
            "Epoch 1/100\n",
            "24/24 [==============================] - 1s 16ms/step - loss: 1.1523 - accuracy: 0.3541 - val_loss: 1.0800 - val_accuracy: 0.3667\n",
            "Epoch 2/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.9192 - accuracy: 0.5446 - val_loss: 1.0222 - val_accuracy: 0.2667\n",
            "Epoch 3/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.9358 - accuracy: 0.4240 - val_loss: 0.9411 - val_accuracy: 0.2667\n",
            "Epoch 4/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.8053 - accuracy: 0.5139 - val_loss: 0.8770 - val_accuracy: 0.3000\n",
            "Epoch 5/100\n",
            "24/24 [==============================] - 0s 5ms/step - loss: 0.7891 - accuracy: 0.5321 - val_loss: 0.8215 - val_accuracy: 0.3000\n",
            "Epoch 6/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.7381 - accuracy: 0.5723 - val_loss: 0.7514 - val_accuracy: 0.8333\n",
            "Epoch 7/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.6871 - accuracy: 0.8684 - val_loss: 0.6983 - val_accuracy: 0.7333\n",
            "Epoch 8/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.6287 - accuracy: 0.8151 - val_loss: 0.6186 - val_accuracy: 0.9000\n",
            "Epoch 9/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.5354 - accuracy: 0.9140 - val_loss: 0.5623 - val_accuracy: 0.9000\n",
            "Epoch 10/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.5706 - accuracy: 0.8338 - val_loss: 0.5350 - val_accuracy: 0.8333\n",
            "Epoch 11/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.5067 - accuracy: 0.8007 - val_loss: 0.4694 - val_accuracy: 0.9333\n",
            "Epoch 12/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.4138 - accuracy: 0.9228 - val_loss: 0.4129 - val_accuracy: 0.9667\n",
            "Epoch 13/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.3939 - accuracy: 0.9376 - val_loss: 0.3821 - val_accuracy: 0.9667\n",
            "Epoch 14/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.3458 - accuracy: 0.9495 - val_loss: 0.3445 - val_accuracy: 0.9667\n",
            "Epoch 15/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.3301 - accuracy: 0.9741 - val_loss: 0.3208 - val_accuracy: 0.9667\n",
            "Epoch 16/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.3160 - accuracy: 0.9412 - val_loss: 0.3027 - val_accuracy: 0.9667\n",
            "Epoch 17/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.2720 - accuracy: 0.9721 - val_loss: 0.2736 - val_accuracy: 0.9667\n",
            "Epoch 18/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.2945 - accuracy: 0.9578 - val_loss: 0.2907 - val_accuracy: 0.9333\n",
            "Epoch 19/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.2887 - accuracy: 0.9343 - val_loss: 0.2374 - val_accuracy: 0.9667\n",
            "Epoch 20/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.2438 - accuracy: 0.9739 - val_loss: 0.2350 - val_accuracy: 0.9667\n",
            "Epoch 21/100\n",
            "24/24 [==============================] - 0s 5ms/step - loss: 0.2863 - accuracy: 0.9049 - val_loss: 0.2590 - val_accuracy: 0.9333\n",
            "Epoch 22/100\n",
            "24/24 [==============================] - 0s 5ms/step - loss: 0.2513 - accuracy: 0.9480 - val_loss: 0.2126 - val_accuracy: 0.9667\n",
            "Epoch 23/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.2233 - accuracy: 0.9649 - val_loss: 0.2042 - val_accuracy: 0.9667\n",
            "Epoch 24/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.1920 - accuracy: 0.9645 - val_loss: 0.1724 - val_accuracy: 1.0000\n",
            "Epoch 25/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.2130 - accuracy: 0.9846 - val_loss: 0.2192 - val_accuracy: 0.9667\n",
            "Epoch 26/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1872 - accuracy: 0.9729 - val_loss: 0.1696 - val_accuracy: 0.9667\n",
            "Epoch 27/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1807 - accuracy: 0.9835 - val_loss: 0.1648 - val_accuracy: 0.9667\n",
            "Epoch 28/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1847 - accuracy: 0.9451 - val_loss: 0.1450 - val_accuracy: 0.9667\n",
            "Epoch 29/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.1745 - accuracy: 0.9504 - val_loss: 0.1506 - val_accuracy: 0.9667\n",
            "Epoch 30/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.1463 - accuracy: 0.9829 - val_loss: 0.1343 - val_accuracy: 0.9667\n",
            "Epoch 31/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.1744 - accuracy: 0.9848 - val_loss: 0.1348 - val_accuracy: 0.9667\n",
            "Epoch 32/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.1522 - accuracy: 0.9790 - val_loss: 0.1481 - val_accuracy: 0.9667\n",
            "Epoch 33/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.1840 - accuracy: 0.9644 - val_loss: 0.1315 - val_accuracy: 0.9667\n",
            "Epoch 34/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.1811 - accuracy: 0.9729 - val_loss: 0.1525 - val_accuracy: 0.9667\n",
            "Epoch 35/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1471 - accuracy: 0.9693 - val_loss: 0.1128 - val_accuracy: 0.9667\n",
            "Epoch 36/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1336 - accuracy: 0.9709 - val_loss: 0.1068 - val_accuracy: 1.0000\n",
            "Epoch 37/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.1318 - accuracy: 0.9897 - val_loss: 0.1177 - val_accuracy: 0.9667\n",
            "Epoch 38/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1500 - accuracy: 0.9835 - val_loss: 0.1082 - val_accuracy: 0.9667\n",
            "Epoch 39/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.1202 - accuracy: 0.9857 - val_loss: 0.1045 - val_accuracy: 0.9667\n",
            "Epoch 40/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.1412 - accuracy: 0.9752 - val_loss: 0.0936 - val_accuracy: 1.0000\n",
            "Epoch 41/100\n",
            "24/24 [==============================] - 0s 5ms/step - loss: 0.1199 - accuracy: 0.9845 - val_loss: 0.1052 - val_accuracy: 0.9667\n",
            "Epoch 42/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.1470 - accuracy: 0.9640 - val_loss: 0.0949 - val_accuracy: 0.9667\n",
            "Epoch 43/100\n",
            "24/24 [==============================] - 0s 5ms/step - loss: 0.1071 - accuracy: 0.9939 - val_loss: 0.0890 - val_accuracy: 0.9667\n",
            "Epoch 44/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1377 - accuracy: 0.9779 - val_loss: 0.1009 - val_accuracy: 0.9667\n",
            "Epoch 45/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.1362 - accuracy: 0.9768 - val_loss: 0.0951 - val_accuracy: 0.9667\n",
            "Epoch 46/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.1247 - accuracy: 0.9821 - val_loss: 0.0832 - val_accuracy: 0.9667\n",
            "Epoch 47/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.1164 - accuracy: 0.9733 - val_loss: 0.0914 - val_accuracy: 0.9667\n",
            "Epoch 48/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.1212 - accuracy: 0.9792 - val_loss: 0.0752 - val_accuracy: 1.0000\n",
            "Epoch 49/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.1366 - accuracy: 0.9343 - val_loss: 0.1083 - val_accuracy: 0.9667\n",
            "Epoch 50/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.1282 - accuracy: 0.9736 - val_loss: 0.0888 - val_accuracy: 0.9667\n",
            "Epoch 51/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.1270 - accuracy: 0.9621 - val_loss: 0.0769 - val_accuracy: 0.9667\n",
            "Epoch 52/100\n",
            "24/24 [==============================] - 0s 6ms/step - loss: 0.1114 - accuracy: 0.9546 - val_loss: 0.1120 - val_accuracy: 0.9667\n",
            "Epoch 53/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.1138 - accuracy: 0.9789 - val_loss: 0.0821 - val_accuracy: 0.9667\n",
            "Epoch 54/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.0849 - accuracy: 0.9943 - val_loss: 0.0785 - val_accuracy: 0.9667\n",
            "Epoch 55/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.1024 - accuracy: 0.9835 - val_loss: 0.0692 - val_accuracy: 0.9667\n",
            "Epoch 56/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.1334 - accuracy: 0.9633 - val_loss: 0.1111 - val_accuracy: 0.9667\n",
            "Epoch 57/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.0854 - accuracy: 0.9800 - val_loss: 0.0666 - val_accuracy: 0.9667\n",
            "Epoch 58/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.1076 - accuracy: 0.9873 - val_loss: 0.0704 - val_accuracy: 0.9667\n",
            "Epoch 59/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.0971 - accuracy: 0.9880 - val_loss: 0.0748 - val_accuracy: 0.9667\n",
            "Epoch 60/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.1191 - accuracy: 0.9711 - val_loss: 0.0787 - val_accuracy: 0.9667\n",
            "Epoch 61/100\n",
            "24/24 [==============================] - 0s 5ms/step - loss: 0.1112 - accuracy: 0.9609 - val_loss: 0.0721 - val_accuracy: 0.9667\n",
            "Epoch 62/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.1163 - accuracy: 0.9692 - val_loss: 0.0712 - val_accuracy: 0.9667\n",
            "Epoch 63/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.0847 - accuracy: 0.9877 - val_loss: 0.0664 - val_accuracy: 0.9667\n",
            "Epoch 64/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.1010 - accuracy: 0.9643 - val_loss: 0.0750 - val_accuracy: 0.9667\n",
            "Epoch 65/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.0797 - accuracy: 0.9914 - val_loss: 0.0609 - val_accuracy: 0.9667\n",
            "Epoch 66/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1023 - accuracy: 0.9775 - val_loss: 0.0769 - val_accuracy: 0.9667\n",
            "Epoch 67/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1209 - accuracy: 0.9519 - val_loss: 0.0666 - val_accuracy: 0.9667\n",
            "Epoch 68/100\n",
            "24/24 [==============================] - 0s 5ms/step - loss: 0.0598 - accuracy: 0.9905 - val_loss: 0.0564 - val_accuracy: 0.9667\n",
            "Epoch 69/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.0834 - accuracy: 0.9886 - val_loss: 0.0576 - val_accuracy: 0.9667\n",
            "Epoch 70/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.0662 - accuracy: 0.9869 - val_loss: 0.0579 - val_accuracy: 0.9667\n",
            "Epoch 71/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.0867 - accuracy: 0.9927 - val_loss: 0.0598 - val_accuracy: 0.9667\n",
            "Epoch 72/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.0860 - accuracy: 0.9773 - val_loss: 0.0501 - val_accuracy: 1.0000\n",
            "Epoch 73/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.0754 - accuracy: 0.9779 - val_loss: 0.0573 - val_accuracy: 0.9667\n",
            "Epoch 74/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.0879 - accuracy: 0.9698 - val_loss: 0.0545 - val_accuracy: 0.9667\n",
            "Epoch 75/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.0980 - accuracy: 0.9746 - val_loss: 0.0786 - val_accuracy: 0.9667\n",
            "Epoch 76/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.0883 - accuracy: 0.9616 - val_loss: 0.0555 - val_accuracy: 0.9667\n",
            "Epoch 77/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1112 - accuracy: 0.9851 - val_loss: 0.0611 - val_accuracy: 0.9667\n",
            "Epoch 78/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.0593 - accuracy: 0.9939 - val_loss: 0.0508 - val_accuracy: 0.9667\n",
            "Epoch 79/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.1599 - accuracy: 0.9225 - val_loss: 0.0544 - val_accuracy: 0.9667\n",
            "Epoch 80/100\n",
            "24/24 [==============================] - 0s 5ms/step - loss: 0.0671 - accuracy: 0.9797 - val_loss: 0.0516 - val_accuracy: 0.9667\n",
            "Epoch 81/100\n",
            "24/24 [==============================] - 0s 5ms/step - loss: 0.0538 - accuracy: 0.9932 - val_loss: 0.0538 - val_accuracy: 0.9667\n",
            "Epoch 82/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.0733 - accuracy: 0.9865 - val_loss: 0.0470 - val_accuracy: 1.0000\n",
            "Epoch 83/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.0700 - accuracy: 0.9848 - val_loss: 0.0548 - val_accuracy: 0.9667\n",
            "Epoch 84/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.0759 - accuracy: 0.9828 - val_loss: 0.0474 - val_accuracy: 0.9667\n",
            "Epoch 85/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.0880 - accuracy: 0.9691 - val_loss: 0.0518 - val_accuracy: 0.9667\n",
            "Epoch 86/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.0725 - accuracy: 0.9830 - val_loss: 0.0508 - val_accuracy: 0.9667\n",
            "Epoch 87/100\n",
            "24/24 [==============================] - 0s 5ms/step - loss: 0.1164 - accuracy: 0.9438 - val_loss: 0.0489 - val_accuracy: 0.9667\n",
            "Epoch 88/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.0616 - accuracy: 0.9886 - val_loss: 0.0545 - val_accuracy: 0.9667\n",
            "Epoch 89/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.1164 - accuracy: 0.9665 - val_loss: 0.0612 - val_accuracy: 0.9667\n",
            "Epoch 90/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1036 - accuracy: 0.9655 - val_loss: 0.0740 - val_accuracy: 0.9667\n",
            "Epoch 91/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.0846 - accuracy: 0.9654 - val_loss: 0.0487 - val_accuracy: 0.9667\n",
            "Epoch 92/100\n",
            "24/24 [==============================] - 0s 5ms/step - loss: 0.0954 - accuracy: 0.9703 - val_loss: 0.0476 - val_accuracy: 0.9667\n",
            "Epoch 93/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.0716 - accuracy: 0.9802 - val_loss: 0.0571 - val_accuracy: 0.9667\n",
            "Epoch 94/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.0933 - accuracy: 0.9756 - val_loss: 0.0551 - val_accuracy: 0.9667\n",
            "Epoch 95/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.0899 - accuracy: 0.9840 - val_loss: 0.0471 - val_accuracy: 0.9667\n",
            "Epoch 96/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.0811 - accuracy: 0.9781 - val_loss: 0.0554 - val_accuracy: 0.9667\n",
            "Epoch 97/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.0656 - accuracy: 0.9782 - val_loss: 0.0449 - val_accuracy: 0.9667\n",
            "Epoch 98/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.0807 - accuracy: 0.9737 - val_loss: 0.0778 - val_accuracy: 0.9667\n",
            "Epoch 99/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.0708 - accuracy: 0.9600 - val_loss: 0.0453 - val_accuracy: 0.9667\n",
            "Epoch 100/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.0759 - accuracy: 0.9822 - val_loss: 0.0395 - val_accuracy: 1.0000\n",
            "1/1 [==============================] - 0s 306ms/step - loss: 0.0395 - accuracy: 1.0000\n",
            "Epoch 1/100\n",
            "24/24 [==============================] - 1s 18ms/step - loss: 1.0956 - accuracy: 0.2816 - val_loss: 0.8816 - val_accuracy: 0.4000\n",
            "Epoch 2/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.7739 - accuracy: 0.5648 - val_loss: 0.6232 - val_accuracy: 0.8333\n",
            "Epoch 3/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.6917 - accuracy: 0.8151 - val_loss: 0.5246 - val_accuracy: 0.9333\n",
            "Epoch 4/100\n",
            "24/24 [==============================] - 0s 5ms/step - loss: 0.5716 - accuracy: 0.8397 - val_loss: 0.4742 - val_accuracy: 0.9333\n",
            "Epoch 5/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.5310 - accuracy: 0.9042 - val_loss: 0.4186 - val_accuracy: 0.9333\n",
            "Epoch 6/100\n",
            "24/24 [==============================] - 0s 5ms/step - loss: 0.4510 - accuracy: 0.9089 - val_loss: 0.3824 - val_accuracy: 0.9667\n",
            "Epoch 7/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.4411 - accuracy: 0.9622 - val_loss: 0.3425 - val_accuracy: 0.9333\n",
            "Epoch 8/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.4054 - accuracy: 0.9175 - val_loss: 0.3250 - val_accuracy: 0.9667\n",
            "Epoch 9/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.3871 - accuracy: 0.9779 - val_loss: 0.2893 - val_accuracy: 0.9667\n",
            "Epoch 10/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.3346 - accuracy: 0.9784 - val_loss: 0.2685 - val_accuracy: 0.9667\n",
            "Epoch 11/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.3462 - accuracy: 0.9248 - val_loss: 0.2442 - val_accuracy: 0.9667\n",
            "Epoch 12/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.3594 - accuracy: 0.8876 - val_loss: 0.2296 - val_accuracy: 0.9667\n",
            "Epoch 13/100\n",
            "24/24 [==============================] - 0s 5ms/step - loss: 0.3469 - accuracy: 0.9233 - val_loss: 0.2135 - val_accuracy: 0.9667\n",
            "Epoch 14/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.2914 - accuracy: 0.9454 - val_loss: 0.2064 - val_accuracy: 0.9667\n",
            "Epoch 15/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.2899 - accuracy: 0.9687 - val_loss: 0.1890 - val_accuracy: 0.9667\n",
            "Epoch 16/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.2701 - accuracy: 0.9684 - val_loss: 0.1781 - val_accuracy: 0.9667\n",
            "Epoch 17/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.2594 - accuracy: 0.9546 - val_loss: 0.1712 - val_accuracy: 0.9667\n",
            "Epoch 18/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.2392 - accuracy: 0.9721 - val_loss: 0.1669 - val_accuracy: 0.9667\n",
            "Epoch 19/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.3002 - accuracy: 0.9008 - val_loss: 0.1596 - val_accuracy: 0.9667\n",
            "Epoch 20/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.2565 - accuracy: 0.9063 - val_loss: 0.1465 - val_accuracy: 0.9667\n",
            "Epoch 21/100\n",
            "24/24 [==============================] - 0s 5ms/step - loss: 0.2631 - accuracy: 0.9132 - val_loss: 0.1418 - val_accuracy: 0.9667\n",
            "Epoch 22/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.1904 - accuracy: 0.9662 - val_loss: 0.1398 - val_accuracy: 0.9667\n",
            "Epoch 23/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1998 - accuracy: 0.9829 - val_loss: 0.1281 - val_accuracy: 0.9667\n",
            "Epoch 24/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1753 - accuracy: 0.9663 - val_loss: 0.1268 - val_accuracy: 0.9667\n",
            "Epoch 25/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1807 - accuracy: 0.9542 - val_loss: 0.1217 - val_accuracy: 0.9667\n",
            "Epoch 26/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.1626 - accuracy: 0.9607 - val_loss: 0.1139 - val_accuracy: 0.9667\n",
            "Epoch 27/100\n",
            "24/24 [==============================] - 0s 5ms/step - loss: 0.2211 - accuracy: 0.9313 - val_loss: 0.1114 - val_accuracy: 0.9667\n",
            "Epoch 28/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.1674 - accuracy: 0.9742 - val_loss: 0.1067 - val_accuracy: 0.9667\n",
            "Epoch 29/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1736 - accuracy: 0.9636 - val_loss: 0.1063 - val_accuracy: 0.9667\n",
            "Epoch 30/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1714 - accuracy: 0.9359 - val_loss: 0.1121 - val_accuracy: 0.9667\n",
            "Epoch 31/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1242 - accuracy: 0.9857 - val_loss: 0.0990 - val_accuracy: 0.9667\n",
            "Epoch 32/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1409 - accuracy: 0.9572 - val_loss: 0.0987 - val_accuracy: 0.9667\n",
            "Epoch 33/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1126 - accuracy: 0.9923 - val_loss: 0.0911 - val_accuracy: 0.9667\n",
            "Epoch 34/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1287 - accuracy: 0.9954 - val_loss: 0.0916 - val_accuracy: 0.9667\n",
            "Epoch 35/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.1420 - accuracy: 0.9707 - val_loss: 0.0881 - val_accuracy: 0.9667\n",
            "Epoch 36/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1369 - accuracy: 0.9659 - val_loss: 0.0858 - val_accuracy: 0.9667\n",
            "Epoch 37/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.1258 - accuracy: 0.9878 - val_loss: 0.0869 - val_accuracy: 0.9667\n",
            "Epoch 38/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1453 - accuracy: 0.9618 - val_loss: 0.0803 - val_accuracy: 0.9667\n",
            "Epoch 39/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.1149 - accuracy: 0.9837 - val_loss: 0.0834 - val_accuracy: 0.9667\n",
            "Epoch 40/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1462 - accuracy: 0.9494 - val_loss: 0.1054 - val_accuracy: 0.9667\n",
            "Epoch 41/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.0928 - accuracy: 0.9745 - val_loss: 0.0765 - val_accuracy: 0.9667\n",
            "Epoch 42/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.0927 - accuracy: 0.9838 - val_loss: 0.0761 - val_accuracy: 0.9667\n",
            "Epoch 43/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.1151 - accuracy: 0.9763 - val_loss: 0.0780 - val_accuracy: 0.9667\n",
            "Epoch 44/100\n",
            "24/24 [==============================] - 0s 5ms/step - loss: 0.1264 - accuracy: 0.9466 - val_loss: 0.0880 - val_accuracy: 0.9667\n",
            "Epoch 45/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.1149 - accuracy: 0.9805 - val_loss: 0.0714 - val_accuracy: 0.9667\n",
            "Epoch 46/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.0913 - accuracy: 0.9904 - val_loss: 0.0738 - val_accuracy: 0.9667\n",
            "Epoch 47/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1161 - accuracy: 0.9580 - val_loss: 0.0705 - val_accuracy: 0.9667\n",
            "Epoch 48/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.1167 - accuracy: 0.9701 - val_loss: 0.0794 - val_accuracy: 0.9667\n",
            "Epoch 49/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.0820 - accuracy: 0.9818 - val_loss: 0.0648 - val_accuracy: 0.9667\n",
            "Epoch 50/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1118 - accuracy: 0.9855 - val_loss: 0.0825 - val_accuracy: 0.9667\n",
            "Epoch 51/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.1038 - accuracy: 0.9791 - val_loss: 0.0688 - val_accuracy: 0.9667\n",
            "Epoch 52/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.1023 - accuracy: 0.9754 - val_loss: 0.0689 - val_accuracy: 0.9667\n",
            "Epoch 53/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.0705 - accuracy: 0.9964 - val_loss: 0.0701 - val_accuracy: 0.9667\n",
            "Epoch 54/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.1176 - accuracy: 0.9463 - val_loss: 0.0653 - val_accuracy: 0.9667\n",
            "Epoch 55/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.1274 - accuracy: 0.9343 - val_loss: 0.0583 - val_accuracy: 0.9667\n",
            "Epoch 56/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.1206 - accuracy: 0.9478 - val_loss: 0.0630 - val_accuracy: 0.9667\n",
            "Epoch 57/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1341 - accuracy: 0.9431 - val_loss: 0.0711 - val_accuracy: 0.9667\n",
            "Epoch 58/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.0806 - accuracy: 0.9639 - val_loss: 0.0577 - val_accuracy: 0.9667\n",
            "Epoch 59/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.0901 - accuracy: 0.9810 - val_loss: 0.0613 - val_accuracy: 0.9667\n",
            "Epoch 60/100\n",
            "24/24 [==============================] - 0s 5ms/step - loss: 0.1103 - accuracy: 0.9437 - val_loss: 0.0552 - val_accuracy: 0.9667\n",
            "Epoch 61/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.0896 - accuracy: 0.9751 - val_loss: 0.0617 - val_accuracy: 0.9667\n",
            "Epoch 62/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.0978 - accuracy: 0.9779 - val_loss: 0.0788 - val_accuracy: 0.9667\n",
            "Epoch 63/100\n",
            "24/24 [==============================] - 0s 5ms/step - loss: 0.1106 - accuracy: 0.9563 - val_loss: 0.0526 - val_accuracy: 0.9667\n",
            "Epoch 64/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.0999 - accuracy: 0.9786 - val_loss: 0.0653 - val_accuracy: 0.9667\n",
            "Epoch 65/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1041 - accuracy: 0.9689 - val_loss: 0.0704 - val_accuracy: 0.9667\n",
            "Epoch 66/100\n",
            "24/24 [==============================] - 0s 5ms/step - loss: 0.0800 - accuracy: 0.9803 - val_loss: 0.0589 - val_accuracy: 0.9667\n",
            "Epoch 67/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.0673 - accuracy: 0.9870 - val_loss: 0.0615 - val_accuracy: 0.9667\n",
            "Epoch 68/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.0848 - accuracy: 0.9826 - val_loss: 0.0500 - val_accuracy: 0.9667\n",
            "Epoch 69/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1034 - accuracy: 0.9538 - val_loss: 0.0776 - val_accuracy: 0.9667\n",
            "Epoch 70/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.1221 - accuracy: 0.9780 - val_loss: 0.0572 - val_accuracy: 0.9667\n",
            "Epoch 71/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.0776 - accuracy: 0.9797 - val_loss: 0.0654 - val_accuracy: 0.9667\n",
            "Epoch 72/100\n",
            "24/24 [==============================] - 0s 5ms/step - loss: 0.0741 - accuracy: 0.9662 - val_loss: 0.0679 - val_accuracy: 0.9667\n",
            "Epoch 73/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.0764 - accuracy: 0.9692 - val_loss: 0.0689 - val_accuracy: 0.9667\n",
            "Epoch 74/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.0856 - accuracy: 0.9777 - val_loss: 0.0506 - val_accuracy: 0.9667\n",
            "Epoch 75/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1069 - accuracy: 0.9681 - val_loss: 0.0911 - val_accuracy: 0.9667\n",
            "Epoch 76/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.0934 - accuracy: 0.9672 - val_loss: 0.0615 - val_accuracy: 0.9667\n",
            "Epoch 77/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.0875 - accuracy: 0.9633 - val_loss: 0.0786 - val_accuracy: 0.9667\n",
            "Epoch 78/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.0896 - accuracy: 0.9802 - val_loss: 0.0637 - val_accuracy: 0.9667\n",
            "Epoch 79/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.0812 - accuracy: 0.9633 - val_loss: 0.0462 - val_accuracy: 0.9667\n",
            "Epoch 80/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.1046 - accuracy: 0.9717 - val_loss: 0.0676 - val_accuracy: 0.9667\n",
            "Epoch 81/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.0638 - accuracy: 0.9851 - val_loss: 0.0595 - val_accuracy: 0.9667\n",
            "Epoch 82/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.0701 - accuracy: 0.9826 - val_loss: 0.0519 - val_accuracy: 0.9667\n",
            "Epoch 83/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.0752 - accuracy: 0.9798 - val_loss: 0.0682 - val_accuracy: 0.9667\n",
            "Epoch 84/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.0904 - accuracy: 0.9861 - val_loss: 0.0481 - val_accuracy: 0.9667\n",
            "Epoch 85/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.0945 - accuracy: 0.9657 - val_loss: 0.0627 - val_accuracy: 0.9667\n",
            "Epoch 86/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.0705 - accuracy: 0.9838 - val_loss: 0.0621 - val_accuracy: 0.9667\n",
            "Epoch 87/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.0552 - accuracy: 0.9921 - val_loss: 0.0508 - val_accuracy: 0.9667\n",
            "Epoch 88/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.0842 - accuracy: 0.9763 - val_loss: 0.0436 - val_accuracy: 0.9667\n",
            "Epoch 89/100\n",
            "24/24 [==============================] - 0s 5ms/step - loss: 0.0889 - accuracy: 0.9804 - val_loss: 0.0549 - val_accuracy: 0.9667\n",
            "Epoch 90/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.0890 - accuracy: 0.9639 - val_loss: 0.0515 - val_accuracy: 0.9667\n",
            "Epoch 91/100\n",
            "24/24 [==============================] - 0s 5ms/step - loss: 0.0674 - accuracy: 0.9821 - val_loss: 0.0498 - val_accuracy: 0.9667\n",
            "Epoch 92/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.0658 - accuracy: 0.9834 - val_loss: 0.0625 - val_accuracy: 0.9667\n",
            "Epoch 93/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.0691 - accuracy: 0.9803 - val_loss: 0.0423 - val_accuracy: 0.9667\n",
            "Epoch 94/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.0659 - accuracy: 0.9841 - val_loss: 0.0524 - val_accuracy: 0.9667\n",
            "Epoch 95/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.0746 - accuracy: 0.9609 - val_loss: 0.0511 - val_accuracy: 0.9667\n",
            "Epoch 96/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1029 - accuracy: 0.9676 - val_loss: 0.0570 - val_accuracy: 0.9667\n",
            "Epoch 97/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.0813 - accuracy: 0.9439 - val_loss: 0.0390 - val_accuracy: 1.0000\n",
            "Epoch 98/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.0791 - accuracy: 0.9806 - val_loss: 0.0711 - val_accuracy: 0.9667\n",
            "Epoch 99/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.0739 - accuracy: 0.9554 - val_loss: 0.0392 - val_accuracy: 0.9667\n",
            "Epoch 100/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.0583 - accuracy: 0.9831 - val_loss: 0.0613 - val_accuracy: 0.9667\n",
            "1/1 [==============================] - 0s 311ms/step - loss: 0.0613 - accuracy: 0.9667\n",
            "Epoch 1/100\n",
            "24/24 [==============================] - 1s 17ms/step - loss: 1.0988 - accuracy: 0.3242 - val_loss: 0.9781 - val_accuracy: 0.4000\n",
            "Epoch 2/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.9393 - accuracy: 0.5513 - val_loss: 0.8826 - val_accuracy: 0.8667\n",
            "Epoch 3/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.8587 - accuracy: 0.7985 - val_loss: 0.8155 - val_accuracy: 0.6333\n",
            "Epoch 4/100\n",
            "24/24 [==============================] - 0s 5ms/step - loss: 0.7746 - accuracy: 0.7237 - val_loss: 0.7408 - val_accuracy: 0.9000\n",
            "Epoch 5/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.6994 - accuracy: 0.8219 - val_loss: 0.6809 - val_accuracy: 0.7000\n",
            "Epoch 6/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.6444 - accuracy: 0.8030 - val_loss: 0.6181 - val_accuracy: 0.8333\n",
            "Epoch 7/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.5910 - accuracy: 0.9186 - val_loss: 0.5681 - val_accuracy: 0.8333\n",
            "Epoch 8/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.5219 - accuracy: 0.7831 - val_loss: 0.5306 - val_accuracy: 0.8333\n",
            "Epoch 9/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.4970 - accuracy: 0.9193 - val_loss: 0.4978 - val_accuracy: 0.9000\n",
            "Epoch 10/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.4657 - accuracy: 0.9404 - val_loss: 0.4719 - val_accuracy: 0.8333\n",
            "Epoch 11/100\n",
            "24/24 [==============================] - 0s 5ms/step - loss: 0.4656 - accuracy: 0.8489 - val_loss: 0.4467 - val_accuracy: 0.9000\n",
            "Epoch 12/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.4005 - accuracy: 0.9668 - val_loss: 0.4299 - val_accuracy: 0.8333\n",
            "Epoch 13/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.4189 - accuracy: 0.9481 - val_loss: 0.4085 - val_accuracy: 0.9000\n",
            "Epoch 14/100\n",
            "24/24 [==============================] - 0s 5ms/step - loss: 0.4104 - accuracy: 0.9098 - val_loss: 0.3829 - val_accuracy: 1.0000\n",
            "Epoch 15/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.3782 - accuracy: 0.9769 - val_loss: 0.3562 - val_accuracy: 1.0000\n",
            "Epoch 16/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.3652 - accuracy: 0.9443 - val_loss: 0.3632 - val_accuracy: 0.8333\n",
            "Epoch 17/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.3586 - accuracy: 0.9087 - val_loss: 0.3191 - val_accuracy: 0.9667\n",
            "Epoch 18/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.2840 - accuracy: 0.9599 - val_loss: 0.3025 - val_accuracy: 0.9667\n",
            "Epoch 19/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.3012 - accuracy: 0.9715 - val_loss: 0.2845 - val_accuracy: 1.0000\n",
            "Epoch 20/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.2878 - accuracy: 0.9716 - val_loss: 0.2780 - val_accuracy: 0.9333\n",
            "Epoch 21/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.2961 - accuracy: 0.8879 - val_loss: 0.2549 - val_accuracy: 0.9333\n",
            "Epoch 22/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.2573 - accuracy: 0.9637 - val_loss: 0.2413 - val_accuracy: 0.9667\n",
            "Epoch 23/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.2145 - accuracy: 0.9902 - val_loss: 0.2274 - val_accuracy: 1.0000\n",
            "Epoch 24/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.2007 - accuracy: 0.9930 - val_loss: 0.2178 - val_accuracy: 1.0000\n",
            "Epoch 25/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.2204 - accuracy: 0.9853 - val_loss: 0.2160 - val_accuracy: 0.9333\n",
            "Epoch 26/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.2136 - accuracy: 0.9739 - val_loss: 0.1969 - val_accuracy: 1.0000\n",
            "Epoch 27/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1994 - accuracy: 0.9843 - val_loss: 0.1858 - val_accuracy: 1.0000\n",
            "Epoch 28/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.2199 - accuracy: 0.9646 - val_loss: 0.1808 - val_accuracy: 0.9667\n",
            "Epoch 29/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.1756 - accuracy: 0.9614 - val_loss: 0.1709 - val_accuracy: 0.9667\n",
            "Epoch 30/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.1332 - accuracy: 0.9919 - val_loss: 0.1544 - val_accuracy: 1.0000\n",
            "Epoch 31/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.1533 - accuracy: 0.9804 - val_loss: 0.1658 - val_accuracy: 0.9667\n",
            "Epoch 32/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.1559 - accuracy: 0.9611 - val_loss: 0.1402 - val_accuracy: 1.0000\n",
            "Epoch 33/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1370 - accuracy: 0.9912 - val_loss: 0.1454 - val_accuracy: 0.9667\n",
            "Epoch 34/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.1262 - accuracy: 0.9831 - val_loss: 0.1359 - val_accuracy: 0.9333\n",
            "Epoch 35/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1625 - accuracy: 0.9254 - val_loss: 0.1234 - val_accuracy: 1.0000\n",
            "Epoch 36/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1412 - accuracy: 0.9581 - val_loss: 0.1205 - val_accuracy: 0.9667\n",
            "Epoch 37/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.1302 - accuracy: 0.9815 - val_loss: 0.1270 - val_accuracy: 0.9667\n",
            "Epoch 38/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.1091 - accuracy: 0.9923 - val_loss: 0.1114 - val_accuracy: 1.0000\n",
            "Epoch 39/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.1247 - accuracy: 0.9712 - val_loss: 0.1072 - val_accuracy: 1.0000\n",
            "Epoch 40/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.1223 - accuracy: 0.9576 - val_loss: 0.1134 - val_accuracy: 0.9667\n",
            "Epoch 41/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.1122 - accuracy: 0.9700 - val_loss: 0.1013 - val_accuracy: 1.0000\n",
            "Epoch 42/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1167 - accuracy: 0.9794 - val_loss: 0.0984 - val_accuracy: 1.0000\n",
            "Epoch 43/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.1027 - accuracy: 0.9844 - val_loss: 0.0954 - val_accuracy: 1.0000\n",
            "Epoch 44/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.0946 - accuracy: 0.9947 - val_loss: 0.0932 - val_accuracy: 1.0000\n",
            "Epoch 45/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.1553 - accuracy: 0.9049 - val_loss: 0.0913 - val_accuracy: 1.0000\n",
            "Epoch 46/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.1138 - accuracy: 0.9613 - val_loss: 0.0891 - val_accuracy: 1.0000\n",
            "Epoch 47/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.1250 - accuracy: 0.9779 - val_loss: 0.0877 - val_accuracy: 1.0000\n",
            "Epoch 48/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.0777 - accuracy: 0.9771 - val_loss: 0.0850 - val_accuracy: 1.0000\n",
            "Epoch 49/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1065 - accuracy: 0.9726 - val_loss: 0.0854 - val_accuracy: 0.9667\n",
            "Epoch 50/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.0576 - accuracy: 0.9883 - val_loss: 0.0816 - val_accuracy: 1.0000\n",
            "Epoch 51/100\n",
            "24/24 [==============================] - 0s 5ms/step - loss: 0.0885 - accuracy: 0.9798 - val_loss: 0.0857 - val_accuracy: 0.9667\n",
            "Epoch 52/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.1109 - accuracy: 0.9658 - val_loss: 0.0870 - val_accuracy: 0.9667\n",
            "Epoch 53/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.0607 - accuracy: 0.9901 - val_loss: 0.1177 - val_accuracy: 0.9333\n",
            "Epoch 54/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1252 - accuracy: 0.9267 - val_loss: 0.0782 - val_accuracy: 0.9667\n",
            "Epoch 55/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1411 - accuracy: 0.9669 - val_loss: 0.0773 - val_accuracy: 1.0000\n",
            "Epoch 56/100\n",
            "24/24 [==============================] - 0s 5ms/step - loss: 0.0793 - accuracy: 0.9687 - val_loss: 0.0731 - val_accuracy: 1.0000\n",
            "Epoch 57/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1273 - accuracy: 0.9586 - val_loss: 0.0754 - val_accuracy: 0.9667\n",
            "Epoch 58/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.0828 - accuracy: 0.9785 - val_loss: 0.0735 - val_accuracy: 1.0000\n",
            "Epoch 59/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1006 - accuracy: 0.9596 - val_loss: 0.0703 - val_accuracy: 1.0000\n",
            "Epoch 60/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.0741 - accuracy: 0.9778 - val_loss: 0.0774 - val_accuracy: 0.9667\n",
            "Epoch 61/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1104 - accuracy: 0.9794 - val_loss: 0.0680 - val_accuracy: 1.0000\n",
            "Epoch 62/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.0711 - accuracy: 0.9778 - val_loss: 0.0753 - val_accuracy: 0.9667\n",
            "Epoch 63/100\n",
            "24/24 [==============================] - 0s 5ms/step - loss: 0.1077 - accuracy: 0.9709 - val_loss: 0.0662 - val_accuracy: 1.0000\n",
            "Epoch 64/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.0763 - accuracy: 0.9705 - val_loss: 0.0654 - val_accuracy: 1.0000\n",
            "Epoch 65/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1314 - accuracy: 0.9557 - val_loss: 0.0646 - val_accuracy: 1.0000\n",
            "Epoch 66/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.0752 - accuracy: 0.9615 - val_loss: 0.0730 - val_accuracy: 0.9667\n",
            "Epoch 67/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.0721 - accuracy: 0.9957 - val_loss: 0.0663 - val_accuracy: 0.9667\n",
            "Epoch 68/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.0982 - accuracy: 0.9705 - val_loss: 0.0625 - val_accuracy: 1.0000\n",
            "Epoch 69/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.0824 - accuracy: 0.9737 - val_loss: 0.0617 - val_accuracy: 1.0000\n",
            "Epoch 70/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.0918 - accuracy: 0.9557 - val_loss: 0.0612 - val_accuracy: 1.0000\n",
            "Epoch 71/100\n",
            "24/24 [==============================] - 0s 5ms/step - loss: 0.0746 - accuracy: 0.9840 - val_loss: 0.1018 - val_accuracy: 0.9333\n",
            "Epoch 72/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.0924 - accuracy: 0.9451 - val_loss: 0.0608 - val_accuracy: 1.0000\n",
            "Epoch 73/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.0738 - accuracy: 0.9761 - val_loss: 0.0923 - val_accuracy: 0.9333\n",
            "Epoch 74/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.0931 - accuracy: 0.9627 - val_loss: 0.0591 - val_accuracy: 1.0000\n",
            "Epoch 75/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.0517 - accuracy: 0.9902 - val_loss: 0.0597 - val_accuracy: 1.0000\n",
            "Epoch 76/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.0846 - accuracy: 0.9773 - val_loss: 0.0603 - val_accuracy: 1.0000\n",
            "Epoch 77/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.0629 - accuracy: 0.9802 - val_loss: 0.0595 - val_accuracy: 1.0000\n",
            "Epoch 78/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.0903 - accuracy: 0.9763 - val_loss: 0.0694 - val_accuracy: 0.9667\n",
            "Epoch 79/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.0666 - accuracy: 0.9870 - val_loss: 0.0636 - val_accuracy: 0.9667\n",
            "Epoch 80/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.0806 - accuracy: 0.9831 - val_loss: 0.0579 - val_accuracy: 1.0000\n",
            "Epoch 81/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.0943 - accuracy: 0.9676 - val_loss: 0.0605 - val_accuracy: 0.9667\n",
            "Epoch 82/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.0554 - accuracy: 0.9854 - val_loss: 0.0645 - val_accuracy: 0.9667\n",
            "Epoch 83/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.0594 - accuracy: 0.9935 - val_loss: 0.0584 - val_accuracy: 0.9667\n",
            "Epoch 84/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.0577 - accuracy: 0.9970 - val_loss: 0.0625 - val_accuracy: 0.9667\n",
            "Epoch 85/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.0937 - accuracy: 0.9772 - val_loss: 0.0637 - val_accuracy: 0.9667\n",
            "Epoch 86/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.0518 - accuracy: 0.9758 - val_loss: 0.0537 - val_accuracy: 1.0000\n",
            "Epoch 87/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.0475 - accuracy: 0.9974 - val_loss: 0.0690 - val_accuracy: 0.9667\n",
            "Epoch 88/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.0535 - accuracy: 0.9956 - val_loss: 0.0550 - val_accuracy: 1.0000\n",
            "Epoch 89/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.0655 - accuracy: 0.9851 - val_loss: 0.0632 - val_accuracy: 0.9667\n",
            "Epoch 90/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.0562 - accuracy: 0.9886 - val_loss: 0.0655 - val_accuracy: 0.9667\n",
            "Epoch 91/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.0942 - accuracy: 0.9528 - val_loss: 0.0534 - val_accuracy: 1.0000\n",
            "Epoch 92/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.0740 - accuracy: 0.9730 - val_loss: 0.0669 - val_accuracy: 0.9667\n",
            "Epoch 93/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.0765 - accuracy: 0.9699 - val_loss: 0.0557 - val_accuracy: 0.9667\n",
            "Epoch 94/100\n",
            "24/24 [==============================] - 0s 6ms/step - loss: 0.0729 - accuracy: 0.9739 - val_loss: 0.0595 - val_accuracy: 0.9667\n",
            "Epoch 95/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.0722 - accuracy: 0.9872 - val_loss: 0.0619 - val_accuracy: 0.9667\n",
            "Epoch 96/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.0645 - accuracy: 0.9748 - val_loss: 0.0597 - val_accuracy: 0.9667\n",
            "Epoch 97/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.0610 - accuracy: 0.9817 - val_loss: 0.0605 - val_accuracy: 0.9667\n",
            "Epoch 98/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.0958 - accuracy: 0.9725 - val_loss: 0.0756 - val_accuracy: 0.9667\n",
            "Epoch 99/100\n",
            "24/24 [==============================] - 0s 5ms/step - loss: 0.0931 - accuracy: 0.9518 - val_loss: 0.0508 - val_accuracy: 1.0000\n",
            "Epoch 100/100\n",
            "24/24 [==============================] - 0s 5ms/step - loss: 0.1004 - accuracy: 0.9537 - val_loss: 0.0499 - val_accuracy: 1.0000\n",
            "1/1 [==============================] - 0s 289ms/step - loss: 0.0499 - accuracy: 1.0000\n",
            "Epoch 1/100\n",
            "24/24 [==============================] - 1s 16ms/step - loss: 1.0977 - accuracy: 0.4013 - val_loss: 0.9547 - val_accuracy: 0.5333\n",
            "Epoch 2/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.9170 - accuracy: 0.7081 - val_loss: 0.8431 - val_accuracy: 0.6333\n",
            "Epoch 3/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.7885 - accuracy: 0.7215 - val_loss: 0.7464 - val_accuracy: 0.8000\n",
            "Epoch 4/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.7214 - accuracy: 0.7684 - val_loss: 0.6782 - val_accuracy: 0.7333\n",
            "Epoch 5/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.6294 - accuracy: 0.7737 - val_loss: 0.6203 - val_accuracy: 0.9000\n",
            "Epoch 6/100\n",
            "24/24 [==============================] - 0s 5ms/step - loss: 0.5834 - accuracy: 0.9672 - val_loss: 0.5747 - val_accuracy: 0.9000\n",
            "Epoch 7/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.5337 - accuracy: 0.8781 - val_loss: 0.5365 - val_accuracy: 0.8667\n",
            "Epoch 8/100\n",
            "24/24 [==============================] - 0s 5ms/step - loss: 0.4647 - accuracy: 0.8907 - val_loss: 0.5005 - val_accuracy: 0.9333\n",
            "Epoch 9/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.4360 - accuracy: 0.9354 - val_loss: 0.4703 - val_accuracy: 0.9333\n",
            "Epoch 10/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.4481 - accuracy: 0.9391 - val_loss: 0.4430 - val_accuracy: 0.9333\n",
            "Epoch 11/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.4086 - accuracy: 0.9568 - val_loss: 0.4194 - val_accuracy: 0.9333\n",
            "Epoch 12/100\n",
            "24/24 [==============================] - 0s 5ms/step - loss: 0.3851 - accuracy: 0.9338 - val_loss: 0.3926 - val_accuracy: 0.9333\n",
            "Epoch 13/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.3618 - accuracy: 0.8721 - val_loss: 0.3665 - val_accuracy: 0.9333\n",
            "Epoch 14/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.3350 - accuracy: 0.9655 - val_loss: 0.3498 - val_accuracy: 0.9333\n",
            "Epoch 15/100\n",
            "24/24 [==============================] - 0s 5ms/step - loss: 0.3288 - accuracy: 0.9555 - val_loss: 0.3312 - val_accuracy: 0.9333\n",
            "Epoch 16/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.2862 - accuracy: 0.9162 - val_loss: 0.3030 - val_accuracy: 0.9667\n",
            "Epoch 17/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.2838 - accuracy: 0.9708 - val_loss: 0.2969 - val_accuracy: 0.9333\n",
            "Epoch 18/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.2724 - accuracy: 0.9656 - val_loss: 0.2728 - val_accuracy: 0.9333\n",
            "Epoch 19/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.2421 - accuracy: 0.9679 - val_loss: 0.2536 - val_accuracy: 0.9667\n",
            "Epoch 20/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.2372 - accuracy: 0.9609 - val_loss: 0.2551 - val_accuracy: 0.9333\n",
            "Epoch 21/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.2024 - accuracy: 0.9947 - val_loss: 0.2258 - val_accuracy: 0.9667\n",
            "Epoch 22/100\n",
            "24/24 [==============================] - 0s 5ms/step - loss: 0.2215 - accuracy: 0.9564 - val_loss: 0.2387 - val_accuracy: 0.9333\n",
            "Epoch 23/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.2242 - accuracy: 0.9655 - val_loss: 0.2071 - val_accuracy: 0.9333\n",
            "Epoch 24/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.2032 - accuracy: 0.9532 - val_loss: 0.2084 - val_accuracy: 0.9333\n",
            "Epoch 25/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.1648 - accuracy: 0.9795 - val_loss: 0.1871 - val_accuracy: 0.9333\n",
            "Epoch 26/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.1576 - accuracy: 0.9920 - val_loss: 0.1739 - val_accuracy: 1.0000\n",
            "Epoch 27/100\n",
            "24/24 [==============================] - 0s 5ms/step - loss: 0.1930 - accuracy: 0.9448 - val_loss: 0.1667 - val_accuracy: 0.9667\n",
            "Epoch 28/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.1870 - accuracy: 0.9794 - val_loss: 0.1600 - val_accuracy: 0.9667\n",
            "Epoch 29/100\n",
            "24/24 [==============================] - 0s 5ms/step - loss: 0.1713 - accuracy: 0.9806 - val_loss: 0.1618 - val_accuracy: 0.9333\n",
            "Epoch 30/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.1627 - accuracy: 0.9797 - val_loss: 0.1456 - val_accuracy: 1.0000\n",
            "Epoch 31/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1619 - accuracy: 0.9462 - val_loss: 0.1616 - val_accuracy: 0.9333\n",
            "Epoch 32/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.1281 - accuracy: 0.9869 - val_loss: 0.1435 - val_accuracy: 0.9333\n",
            "Epoch 33/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.1153 - accuracy: 0.9943 - val_loss: 0.1262 - val_accuracy: 1.0000\n",
            "Epoch 34/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.1295 - accuracy: 0.9746 - val_loss: 0.1487 - val_accuracy: 0.9333\n",
            "Epoch 35/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.1207 - accuracy: 0.9780 - val_loss: 0.1163 - val_accuracy: 1.0000\n",
            "Epoch 36/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.1294 - accuracy: 0.9653 - val_loss: 0.1357 - val_accuracy: 0.9333\n",
            "Epoch 37/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.1219 - accuracy: 0.9915 - val_loss: 0.1164 - val_accuracy: 0.9667\n",
            "Epoch 38/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1387 - accuracy: 0.9717 - val_loss: 0.1128 - val_accuracy: 0.9667\n",
            "Epoch 39/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.1134 - accuracy: 0.9938 - val_loss: 0.0967 - val_accuracy: 1.0000\n",
            "Epoch 40/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.1056 - accuracy: 0.9514 - val_loss: 0.1218 - val_accuracy: 0.9333\n",
            "Epoch 41/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.1250 - accuracy: 0.9647 - val_loss: 0.0869 - val_accuracy: 1.0000\n",
            "Epoch 42/100\n",
            "24/24 [==============================] - 0s 5ms/step - loss: 0.1291 - accuracy: 0.9472 - val_loss: 0.1196 - val_accuracy: 0.9333\n",
            "Epoch 43/100\n",
            "24/24 [==============================] - 0s 5ms/step - loss: 0.1203 - accuracy: 0.9703 - val_loss: 0.1017 - val_accuracy: 0.9333\n",
            "Epoch 44/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.0781 - accuracy: 0.9922 - val_loss: 0.0912 - val_accuracy: 0.9333\n",
            "Epoch 45/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.1023 - accuracy: 0.9638 - val_loss: 0.0960 - val_accuracy: 0.9333\n",
            "Epoch 46/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1258 - accuracy: 0.9677 - val_loss: 0.0811 - val_accuracy: 1.0000\n",
            "Epoch 47/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.0876 - accuracy: 0.9754 - val_loss: 0.1055 - val_accuracy: 0.9333\n",
            "Epoch 48/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1169 - accuracy: 0.9609 - val_loss: 0.0899 - val_accuracy: 0.9333\n",
            "Epoch 49/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.0643 - accuracy: 0.9955 - val_loss: 0.0806 - val_accuracy: 0.9667\n",
            "Epoch 50/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.0665 - accuracy: 0.9969 - val_loss: 0.0825 - val_accuracy: 0.9667\n",
            "Epoch 51/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.0922 - accuracy: 0.9613 - val_loss: 0.0854 - val_accuracy: 0.9333\n",
            "Epoch 52/100\n",
            "24/24 [==============================] - 0s 5ms/step - loss: 0.1356 - accuracy: 0.9553 - val_loss: 0.1062 - val_accuracy: 0.9333\n",
            "Epoch 53/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.0572 - accuracy: 0.9934 - val_loss: 0.0655 - val_accuracy: 1.0000\n",
            "Epoch 54/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.0660 - accuracy: 0.9870 - val_loss: 0.0824 - val_accuracy: 0.9333\n",
            "Epoch 55/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.1118 - accuracy: 0.9713 - val_loss: 0.0878 - val_accuracy: 0.9333\n",
            "Epoch 56/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.0901 - accuracy: 0.9857 - val_loss: 0.0581 - val_accuracy: 1.0000\n",
            "Epoch 57/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.0905 - accuracy: 0.9872 - val_loss: 0.0641 - val_accuracy: 1.0000\n",
            "Epoch 58/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.0552 - accuracy: 0.9944 - val_loss: 0.0602 - val_accuracy: 1.0000\n",
            "Epoch 59/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.0962 - accuracy: 0.9702 - val_loss: 0.0683 - val_accuracy: 0.9667\n",
            "Epoch 60/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.0849 - accuracy: 0.9813 - val_loss: 0.0594 - val_accuracy: 1.0000\n",
            "Epoch 61/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.0885 - accuracy: 0.9711 - val_loss: 0.0776 - val_accuracy: 0.9333\n",
            "Epoch 62/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.0722 - accuracy: 0.9804 - val_loss: 0.0845 - val_accuracy: 0.9333\n",
            "Epoch 63/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.0568 - accuracy: 0.9874 - val_loss: 0.0593 - val_accuracy: 1.0000\n",
            "Epoch 64/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.0715 - accuracy: 0.9908 - val_loss: 0.0520 - val_accuracy: 1.0000\n",
            "Epoch 65/100\n",
            "24/24 [==============================] - 0s 5ms/step - loss: 0.1177 - accuracy: 0.9405 - val_loss: 0.0830 - val_accuracy: 0.9333\n",
            "Epoch 66/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.0559 - accuracy: 0.9938 - val_loss: 0.0638 - val_accuracy: 0.9667\n",
            "Epoch 67/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.1044 - accuracy: 0.9840 - val_loss: 0.0614 - val_accuracy: 0.9667\n",
            "Epoch 68/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.0766 - accuracy: 0.9756 - val_loss: 0.0776 - val_accuracy: 0.9333\n",
            "Epoch 69/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.0501 - accuracy: 0.9926 - val_loss: 0.0498 - val_accuracy: 1.0000\n",
            "Epoch 70/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.1422 - accuracy: 0.9412 - val_loss: 0.0816 - val_accuracy: 0.9333\n",
            "Epoch 71/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.0571 - accuracy: 0.9915 - val_loss: 0.0491 - val_accuracy: 1.0000\n",
            "Epoch 72/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.0832 - accuracy: 0.9771 - val_loss: 0.0492 - val_accuracy: 1.0000\n",
            "Epoch 73/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.0808 - accuracy: 0.9896 - val_loss: 0.0528 - val_accuracy: 1.0000\n",
            "Epoch 74/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1180 - accuracy: 0.9361 - val_loss: 0.0925 - val_accuracy: 0.9333\n",
            "Epoch 75/100\n",
            "24/24 [==============================] - 0s 5ms/step - loss: 0.0896 - accuracy: 0.9783 - val_loss: 0.0649 - val_accuracy: 0.9667\n",
            "Epoch 76/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.0478 - accuracy: 0.9980 - val_loss: 0.0689 - val_accuracy: 0.9667\n",
            "Epoch 77/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.0606 - accuracy: 0.9765 - val_loss: 0.0676 - val_accuracy: 0.9667\n",
            "Epoch 78/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.0708 - accuracy: 0.9801 - val_loss: 0.0577 - val_accuracy: 0.9667\n",
            "Epoch 79/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.0667 - accuracy: 0.9821 - val_loss: 0.0637 - val_accuracy: 0.9667\n",
            "Epoch 80/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.0665 - accuracy: 0.9803 - val_loss: 0.0670 - val_accuracy: 0.9667\n",
            "Epoch 81/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.0719 - accuracy: 0.9890 - val_loss: 0.0431 - val_accuracy: 1.0000\n",
            "Epoch 82/100\n",
            "24/24 [==============================] - 0s 5ms/step - loss: 0.0622 - accuracy: 0.9857 - val_loss: 0.0549 - val_accuracy: 0.9667\n",
            "Epoch 83/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.0474 - accuracy: 0.9953 - val_loss: 0.0670 - val_accuracy: 0.9667\n",
            "Epoch 84/100\n",
            "24/24 [==============================] - 0s 5ms/step - loss: 0.0454 - accuracy: 0.9936 - val_loss: 0.0523 - val_accuracy: 0.9667\n",
            "Epoch 85/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.0727 - accuracy: 0.9833 - val_loss: 0.0763 - val_accuracy: 0.9333\n",
            "Epoch 86/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.0529 - accuracy: 0.9821 - val_loss: 0.0416 - val_accuracy: 1.0000\n",
            "Epoch 87/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.0503 - accuracy: 0.9899 - val_loss: 0.0654 - val_accuracy: 0.9667\n",
            "Epoch 88/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1128 - accuracy: 0.9697 - val_loss: 0.0962 - val_accuracy: 0.9333\n",
            "Epoch 89/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.0607 - accuracy: 0.9878 - val_loss: 0.0351 - val_accuracy: 1.0000\n",
            "Epoch 90/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.0887 - accuracy: 0.9511 - val_loss: 0.0780 - val_accuracy: 0.9333\n",
            "Epoch 91/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.0483 - accuracy: 0.9939 - val_loss: 0.0374 - val_accuracy: 1.0000\n",
            "Epoch 92/100\n",
            "24/24 [==============================] - 0s 5ms/step - loss: 0.0549 - accuracy: 0.9928 - val_loss: 0.0613 - val_accuracy: 0.9667\n",
            "Epoch 93/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.0462 - accuracy: 0.9826 - val_loss: 0.0501 - val_accuracy: 0.9667\n",
            "Epoch 94/100\n",
            "24/24 [==============================] - 0s 5ms/step - loss: 0.0579 - accuracy: 0.9874 - val_loss: 0.0566 - val_accuracy: 0.9667\n",
            "Epoch 95/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.0767 - accuracy: 0.9757 - val_loss: 0.0632 - val_accuracy: 0.9667\n",
            "Epoch 96/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.1216 - accuracy: 0.9665 - val_loss: 0.0483 - val_accuracy: 0.9667\n",
            "Epoch 97/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.0917 - accuracy: 0.9625 - val_loss: 0.0568 - val_accuracy: 0.9667\n",
            "Epoch 98/100\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 0.0463 - accuracy: 0.9924 - val_loss: 0.0331 - val_accuracy: 1.0000\n",
            "Epoch 99/100\n",
            "24/24 [==============================] - 0s 5ms/step - loss: 0.0636 - accuracy: 0.9824 - val_loss: 0.0811 - val_accuracy: 0.9333\n",
            "Epoch 100/100\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.0779 - accuracy: 0.9734 - val_loss: 0.0398 - val_accuracy: 1.0000\n",
            "1/1 [==============================] - 0s 315ms/step - loss: 0.0398 - accuracy: 1.0000\n",
            "\n",
            " 10 fold accuracy:  ['0.9667', '1.0000', '1.0000', '0.9667', '1.0000', '1.0000', '1.0000', '0.9667', '1.0000', '1.0000']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tmt8heTbZ9ZE"
      },
      "source": [
        "Y=dataset[:, 4]\n",
        "Y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FtjdIvVDPbaW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sj1vvW4KcFym",
        "outputId": "5432180e-2ba6-444c-e1d4-0b0f7c67dbd8"
      },
      "source": [
        "#데이터셋이 너무 적으니 k-fold\n",
        "from sklearn.model_selection import StratifiedKFold, KFold #StratifiedKFold는 k-fold가 label을 데이터와 학습에 올바르게 분배하지 못하는 경우를 해결(데이터가 순서대로 있을때...꽃종류별 50개씩 있잖음지금...)\n",
        "n_fold=10 #쪼갤 파일 수 \n",
        "kf=KFold(n_splits=n_fold, shuffle=True, random_state=100) #비율 고려 안하고 무작위로 섞어서 kfold..\n",
        "\n",
        "accuracy=[]\n",
        "  \n",
        "#모델 저장\n",
        "Model_dir='./model/'\n",
        "if not os.path.exists(Model_dir):\n",
        "  os.mkdir(Model_dir)\n",
        "\n",
        "modelpath='./model/{epoch:02d}-{val_loss:.4f}.hdf5'\n",
        "\n",
        "#모델 업데이트 및 저장\n",
        "checkpointer = ModelCheckpoint(filepath=modelpath, monitor='val_loss', verbose=1, save_best_only=True)\n",
        "\n",
        "for train, test in kf.split(X,y_encoded): #y_encoded는 라벨인코딩 후 원핫인코딩한 값잉ㅁ.. \n",
        "  model=Sequential()\n",
        "  model.add(Dense(24, input_dim=4, activation='relu'))\n",
        "  model.add(Dense(10, activation='relu'))\n",
        "  model.add(Dense(3, activation='softmax')) #3= y_encoded nunique. \n",
        "  model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "  model.fit(X[train],y_encoded[train], validation_data=(X[test], y_encoded[test]), epochs=100, batch_size=5, verbose=1, callbacks=[checkpointer])\n",
        "\n",
        "  k_accuracy = '%.4f'%(model.evaluate(X[test], y_encoded[test])[1])\n",
        "  accuracy.append(k_accuracy)\n",
        "\n",
        "\n",
        "  print('\\n %.f fold accuracy: '%n_fold, accuracy)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "27/27 [==============================] - 1s 14ms/step - loss: 1.1395 - accuracy: 0.3359 - val_loss: 0.9580 - val_accuracy: 0.5333\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.95799, saving model to ./model/01-0.9580.hdf5\n",
            "Epoch 2/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 1.0557 - accuracy: 0.3064 - val_loss: 0.9228 - val_accuracy: 0.5333\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.95799 to 0.92280, saving model to ./model/02-0.9228.hdf5\n",
            "Epoch 3/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 1.0003 - accuracy: 0.4682 - val_loss: 0.8343 - val_accuracy: 0.5333\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.92280 to 0.83427, saving model to ./model/03-0.8343.hdf5\n",
            "Epoch 4/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.8947 - accuracy: 0.6451 - val_loss: 0.7193 - val_accuracy: 0.5333\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.83427 to 0.71933, saving model to ./model/04-0.7193.hdf5\n",
            "Epoch 5/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.7746 - accuracy: 0.7181 - val_loss: 0.5925 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.71933 to 0.59245, saving model to ./model/05-0.5925.hdf5\n",
            "Epoch 6/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.6843 - accuracy: 0.8127 - val_loss: 0.5091 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.59245 to 0.50910, saving model to ./model/06-0.5091.hdf5\n",
            "Epoch 7/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.6183 - accuracy: 0.7719 - val_loss: 0.4497 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.50910 to 0.44966, saving model to ./model/07-0.4497.hdf5\n",
            "Epoch 8/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.6122 - accuracy: 0.7451 - val_loss: 0.4084 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.44966 to 0.40843, saving model to ./model/08-0.4084.hdf5\n",
            "Epoch 9/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.5885 - accuracy: 0.8302 - val_loss: 0.3671 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00009: val_loss improved from 0.40843 to 0.36706, saving model to ./model/09-0.3671.hdf5\n",
            "Epoch 10/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.4912 - accuracy: 0.8626 - val_loss: 0.3364 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00010: val_loss improved from 0.36706 to 0.33636, saving model to ./model/10-0.3364.hdf5\n",
            "Epoch 11/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.5206 - accuracy: 0.8347 - val_loss: 0.3199 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00011: val_loss improved from 0.33636 to 0.31994, saving model to ./model/11-0.3199.hdf5\n",
            "Epoch 12/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.5409 - accuracy: 0.9299 - val_loss: 0.3048 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00012: val_loss improved from 0.31994 to 0.30481, saving model to ./model/12-0.3048.hdf5\n",
            "Epoch 13/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.4418 - accuracy: 0.9209 - val_loss: 0.2817 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00013: val_loss improved from 0.30481 to 0.28173, saving model to ./model/13-0.2817.hdf5\n",
            "Epoch 14/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.4706 - accuracy: 0.9540 - val_loss: 0.2746 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00014: val_loss improved from 0.28173 to 0.27459, saving model to ./model/14-0.2746.hdf5\n",
            "Epoch 15/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.4318 - accuracy: 0.9620 - val_loss: 0.2605 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00015: val_loss improved from 0.27459 to 0.26050, saving model to ./model/15-0.2605.hdf5\n",
            "Epoch 16/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.3824 - accuracy: 0.9541 - val_loss: 0.2477 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00016: val_loss improved from 0.26050 to 0.24774, saving model to ./model/16-0.2477.hdf5\n",
            "Epoch 17/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.3898 - accuracy: 0.9661 - val_loss: 0.2430 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00017: val_loss improved from 0.24774 to 0.24303, saving model to ./model/17-0.2430.hdf5\n",
            "Epoch 18/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.3264 - accuracy: 0.9843 - val_loss: 0.2330 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00018: val_loss improved from 0.24303 to 0.23298, saving model to ./model/18-0.2330.hdf5\n",
            "Epoch 19/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.3375 - accuracy: 0.9809 - val_loss: 0.2205 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00019: val_loss improved from 0.23298 to 0.22052, saving model to ./model/19-0.2205.hdf5\n",
            "Epoch 20/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.3323 - accuracy: 0.9753 - val_loss: 0.2103 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00020: val_loss improved from 0.22052 to 0.21026, saving model to ./model/20-0.2103.hdf5\n",
            "Epoch 21/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.3333 - accuracy: 0.9750 - val_loss: 0.2052 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00021: val_loss improved from 0.21026 to 0.20518, saving model to ./model/21-0.2052.hdf5\n",
            "Epoch 22/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.2999 - accuracy: 0.9701 - val_loss: 0.1598 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00022: val_loss improved from 0.20518 to 0.15979, saving model to ./model/22-0.1598.hdf5\n",
            "Epoch 23/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.2775 - accuracy: 0.9634 - val_loss: 0.1850 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.15979\n",
            "Epoch 24/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.2985 - accuracy: 0.9618 - val_loss: 0.1529 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00024: val_loss improved from 0.15979 to 0.15286, saving model to ./model/24-0.1529.hdf5\n",
            "Epoch 25/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.2819 - accuracy: 0.9631 - val_loss: 0.1382 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00025: val_loss improved from 0.15286 to 0.13817, saving model to ./model/25-0.1382.hdf5\n",
            "Epoch 26/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.2369 - accuracy: 0.9488 - val_loss: 0.1501 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.13817\n",
            "Epoch 27/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.2119 - accuracy: 0.9747 - val_loss: 0.1445 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.13817\n",
            "Epoch 28/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.2199 - accuracy: 0.9908 - val_loss: 0.1412 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.13817\n",
            "Epoch 29/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.2066 - accuracy: 0.9758 - val_loss: 0.1191 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00029: val_loss improved from 0.13817 to 0.11909, saving model to ./model/29-0.1191.hdf5\n",
            "Epoch 30/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.2172 - accuracy: 0.9472 - val_loss: 0.0920 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00030: val_loss improved from 0.11909 to 0.09204, saving model to ./model/30-0.0920.hdf5\n",
            "Epoch 31/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1946 - accuracy: 0.9499 - val_loss: 0.1011 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.09204\n",
            "Epoch 32/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1581 - accuracy: 0.9822 - val_loss: 0.0899 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00032: val_loss improved from 0.09204 to 0.08988, saving model to ./model/32-0.0899.hdf5\n",
            "Epoch 33/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1665 - accuracy: 0.9948 - val_loss: 0.0934 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.08988\n",
            "Epoch 34/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1796 - accuracy: 0.9857 - val_loss: 0.0931 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.08988\n",
            "Epoch 35/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1433 - accuracy: 0.9840 - val_loss: 0.0810 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00035: val_loss improved from 0.08988 to 0.08095, saving model to ./model/35-0.0810.hdf5\n",
            "Epoch 36/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1518 - accuracy: 0.9876 - val_loss: 0.1055 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.08095\n",
            "Epoch 37/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1755 - accuracy: 0.9670 - val_loss: 0.0913 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.08095\n",
            "Epoch 38/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1491 - accuracy: 0.9688 - val_loss: 0.0645 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00038: val_loss improved from 0.08095 to 0.06447, saving model to ./model/38-0.0645.hdf5\n",
            "Epoch 39/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1531 - accuracy: 0.9547 - val_loss: 0.0775 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.06447\n",
            "Epoch 40/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1293 - accuracy: 0.9823 - val_loss: 0.0756 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.06447\n",
            "Epoch 41/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1361 - accuracy: 0.9813 - val_loss: 0.0605 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00041: val_loss improved from 0.06447 to 0.06049, saving model to ./model/41-0.0605.hdf5\n",
            "Epoch 42/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1370 - accuracy: 0.9823 - val_loss: 0.0607 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.06049\n",
            "Epoch 43/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0994 - accuracy: 0.9958 - val_loss: 0.0548 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00043: val_loss improved from 0.06049 to 0.05484, saving model to ./model/43-0.0548.hdf5\n",
            "Epoch 44/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1165 - accuracy: 0.9736 - val_loss: 0.0636 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.05484\n",
            "Epoch 45/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1267 - accuracy: 0.9885 - val_loss: 0.0649 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.05484\n",
            "Epoch 46/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1250 - accuracy: 0.9478 - val_loss: 0.0682 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.05484\n",
            "Epoch 47/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1069 - accuracy: 0.9657 - val_loss: 0.0518 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00047: val_loss improved from 0.05484 to 0.05179, saving model to ./model/47-0.0518.hdf5\n",
            "Epoch 48/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1060 - accuracy: 0.9650 - val_loss: 0.0464 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00048: val_loss improved from 0.05179 to 0.04636, saving model to ./model/48-0.0464.hdf5\n",
            "Epoch 49/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.0914 - accuracy: 0.9835 - val_loss: 0.0728 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.04636\n",
            "Epoch 50/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1218 - accuracy: 0.9525 - val_loss: 0.0350 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00050: val_loss improved from 0.04636 to 0.03501, saving model to ./model/50-0.0350.hdf5\n",
            "Epoch 51/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1281 - accuracy: 0.9642 - val_loss: 0.0508 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.03501\n",
            "Epoch 52/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1040 - accuracy: 0.9815 - val_loss: 0.0530 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.03501\n",
            "Epoch 53/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1116 - accuracy: 0.9615 - val_loss: 0.0530 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.03501\n",
            "Epoch 54/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1158 - accuracy: 0.9701 - val_loss: 0.0480 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.03501\n",
            "Epoch 55/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1052 - accuracy: 0.9816 - val_loss: 0.0443 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.03501\n",
            "Epoch 56/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.0902 - accuracy: 0.9877 - val_loss: 0.0546 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.03501\n",
            "Epoch 57/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0875 - accuracy: 0.9644 - val_loss: 0.0394 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.03501\n",
            "Epoch 58/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0964 - accuracy: 0.9796 - val_loss: 0.0450 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.03501\n",
            "Epoch 59/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0989 - accuracy: 0.9770 - val_loss: 0.0324 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00059: val_loss improved from 0.03501 to 0.03242, saving model to ./model/59-0.0324.hdf5\n",
            "Epoch 60/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0932 - accuracy: 0.9857 - val_loss: 0.0426 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.03242\n",
            "Epoch 61/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1270 - accuracy: 0.9399 - val_loss: 0.0418 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.03242\n",
            "Epoch 62/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0924 - accuracy: 0.9793 - val_loss: 0.0482 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.03242\n",
            "Epoch 63/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0950 - accuracy: 0.9707 - val_loss: 0.0331 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.03242\n",
            "Epoch 64/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1021 - accuracy: 0.9836 - val_loss: 0.0307 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00064: val_loss improved from 0.03242 to 0.03069, saving model to ./model/64-0.0307.hdf5\n",
            "Epoch 65/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1017 - accuracy: 0.9701 - val_loss: 0.0404 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.03069\n",
            "Epoch 66/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1211 - accuracy: 0.9723 - val_loss: 0.0437 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.03069\n",
            "Epoch 67/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0867 - accuracy: 0.9650 - val_loss: 0.0372 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.03069\n",
            "Epoch 68/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1084 - accuracy: 0.9569 - val_loss: 0.0334 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.03069\n",
            "Epoch 69/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1222 - accuracy: 0.9406 - val_loss: 0.0207 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00069: val_loss improved from 0.03069 to 0.02068, saving model to ./model/69-0.0207.hdf5\n",
            "Epoch 70/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.0738 - accuracy: 0.9805 - val_loss: 0.0437 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.02068\n",
            "Epoch 71/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1011 - accuracy: 0.9517 - val_loss: 0.0353 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.02068\n",
            "Epoch 72/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0837 - accuracy: 0.9673 - val_loss: 0.0362 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.02068\n",
            "Epoch 73/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0767 - accuracy: 0.9734 - val_loss: 0.0361 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.02068\n",
            "Epoch 74/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.0614 - accuracy: 0.9910 - val_loss: 0.0349 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.02068\n",
            "Epoch 75/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.0991 - accuracy: 0.9601 - val_loss: 0.0199 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00075: val_loss improved from 0.02068 to 0.01991, saving model to ./model/75-0.0199.hdf5\n",
            "Epoch 76/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.0791 - accuracy: 0.9736 - val_loss: 0.0341 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.01991\n",
            "Epoch 77/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1236 - accuracy: 0.9384 - val_loss: 0.0270 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.01991\n",
            "Epoch 78/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.0814 - accuracy: 0.9887 - val_loss: 0.0441 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.01991\n",
            "Epoch 79/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.0740 - accuracy: 0.9804 - val_loss: 0.0248 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.01991\n",
            "Epoch 80/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0658 - accuracy: 0.9906 - val_loss: 0.0344 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.01991\n",
            "Epoch 81/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0937 - accuracy: 0.9425 - val_loss: 0.0182 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00081: val_loss improved from 0.01991 to 0.01819, saving model to ./model/81-0.0182.hdf5\n",
            "Epoch 82/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.0751 - accuracy: 0.9708 - val_loss: 0.0407 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.01819\n",
            "Epoch 83/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0926 - accuracy: 0.9671 - val_loss: 0.0183 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.01819\n",
            "Epoch 84/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.0792 - accuracy: 0.9700 - val_loss: 0.0274 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.01819\n",
            "Epoch 85/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.0907 - accuracy: 0.9802 - val_loss: 0.0371 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.01819\n",
            "Epoch 86/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0724 - accuracy: 0.9778 - val_loss: 0.0303 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.01819\n",
            "Epoch 87/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0867 - accuracy: 0.9571 - val_loss: 0.0300 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.01819\n",
            "Epoch 88/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0548 - accuracy: 0.9782 - val_loss: 0.0226 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.01819\n",
            "Epoch 89/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0519 - accuracy: 0.9888 - val_loss: 0.0321 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.01819\n",
            "Epoch 90/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0814 - accuracy: 0.9727 - val_loss: 0.0228 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.01819\n",
            "Epoch 91/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.0994 - accuracy: 0.9650 - val_loss: 0.0194 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.01819\n",
            "Epoch 92/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0867 - accuracy: 0.9843 - val_loss: 0.0261 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.01819\n",
            "Epoch 93/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0604 - accuracy: 0.9849 - val_loss: 0.0248 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.01819\n",
            "Epoch 94/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.0605 - accuracy: 0.9882 - val_loss: 0.0340 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.01819\n",
            "Epoch 95/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1062 - accuracy: 0.9510 - val_loss: 0.0352 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.01819\n",
            "Epoch 96/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.0939 - accuracy: 0.9592 - val_loss: 0.0370 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.01819\n",
            "Epoch 97/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0741 - accuracy: 0.9716 - val_loss: 0.0250 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.01819\n",
            "Epoch 98/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0685 - accuracy: 0.9812 - val_loss: 0.0212 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.01819\n",
            "Epoch 99/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0779 - accuracy: 0.9609 - val_loss: 0.0292 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.01819\n",
            "Epoch 100/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1187 - accuracy: 0.9518 - val_loss: 0.0141 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00100: val_loss improved from 0.01819 to 0.01414, saving model to ./model/100-0.0141.hdf5\n",
            "1/1 [==============================] - 0s 265ms/step - loss: 0.0141 - accuracy: 1.0000\n",
            "\n",
            " 10 fold accuracy:  ['1.0000']\n",
            "Epoch 1/100\n",
            "27/27 [==============================] - 1s 13ms/step - loss: 1.4019 - accuracy: 0.0093 - val_loss: 1.2077 - val_accuracy: 0.4000\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 0.01414\n",
            "Epoch 2/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 1.1903 - accuracy: 0.3079 - val_loss: 1.0797 - val_accuracy: 0.4000\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.01414\n",
            "Epoch 3/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 1.0382 - accuracy: 0.4900 - val_loss: 0.9907 - val_accuracy: 0.6000\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.01414\n",
            "Epoch 4/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.9286 - accuracy: 0.6745 - val_loss: 0.9020 - val_accuracy: 0.6000\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.01414\n",
            "Epoch 5/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.8459 - accuracy: 0.6567 - val_loss: 0.8453 - val_accuracy: 0.6000\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.01414\n",
            "Epoch 6/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.7422 - accuracy: 0.6771 - val_loss: 0.7968 - val_accuracy: 0.6000\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.01414\n",
            "Epoch 7/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.6967 - accuracy: 0.6874 - val_loss: 0.7480 - val_accuracy: 0.6000\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.01414\n",
            "Epoch 8/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.6100 - accuracy: 0.7165 - val_loss: 0.7033 - val_accuracy: 0.6000\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.01414\n",
            "Epoch 9/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.6289 - accuracy: 0.6589 - val_loss: 0.6510 - val_accuracy: 0.6000\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.01414\n",
            "Epoch 10/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.5581 - accuracy: 0.6698 - val_loss: 0.6125 - val_accuracy: 0.6000\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.01414\n",
            "Epoch 11/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.5418 - accuracy: 0.6791 - val_loss: 0.5796 - val_accuracy: 0.8667\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.01414\n",
            "Epoch 12/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.5039 - accuracy: 0.7531 - val_loss: 0.5179 - val_accuracy: 0.8667\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.01414\n",
            "Epoch 13/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.4599 - accuracy: 0.7607 - val_loss: 0.4814 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.01414\n",
            "Epoch 14/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.4003 - accuracy: 0.8679 - val_loss: 0.4419 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.01414\n",
            "Epoch 15/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.3830 - accuracy: 0.9112 - val_loss: 0.4087 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.01414\n",
            "Epoch 16/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.3471 - accuracy: 0.9080 - val_loss: 0.3838 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.01414\n",
            "Epoch 17/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.3823 - accuracy: 0.9432 - val_loss: 0.3678 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.01414\n",
            "Epoch 18/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.2955 - accuracy: 0.8686 - val_loss: 0.3220 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.01414\n",
            "Epoch 19/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.2873 - accuracy: 0.9550 - val_loss: 0.2940 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.01414\n",
            "Epoch 20/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.2615 - accuracy: 0.9492 - val_loss: 0.2720 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.01414\n",
            "Epoch 21/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.2630 - accuracy: 0.9480 - val_loss: 0.2509 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.01414\n",
            "Epoch 22/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.2087 - accuracy: 0.9871 - val_loss: 0.2544 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.01414\n",
            "Epoch 23/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.2414 - accuracy: 0.9634 - val_loss: 0.2284 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.01414\n",
            "Epoch 24/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.2165 - accuracy: 0.9675 - val_loss: 0.2081 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.01414\n",
            "Epoch 25/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1696 - accuracy: 0.9881 - val_loss: 0.1911 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.01414\n",
            "Epoch 26/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.2127 - accuracy: 0.9470 - val_loss: 0.1754 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.01414\n",
            "Epoch 27/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1607 - accuracy: 0.9494 - val_loss: 0.1908 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.01414\n",
            "Epoch 28/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1815 - accuracy: 0.9695 - val_loss: 0.1672 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.01414\n",
            "Epoch 29/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1783 - accuracy: 0.9450 - val_loss: 0.1518 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.01414\n",
            "Epoch 30/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1879 - accuracy: 0.9720 - val_loss: 0.1414 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.01414\n",
            "Epoch 31/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1474 - accuracy: 0.9610 - val_loss: 0.1395 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.01414\n",
            "Epoch 32/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1342 - accuracy: 0.9636 - val_loss: 0.1268 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.01414\n",
            "Epoch 33/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1258 - accuracy: 0.9618 - val_loss: 0.1232 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.01414\n",
            "Epoch 34/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1378 - accuracy: 0.9609 - val_loss: 0.1144 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.01414\n",
            "Epoch 35/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1090 - accuracy: 0.9824 - val_loss: 0.1098 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.01414\n",
            "Epoch 36/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1082 - accuracy: 0.9614 - val_loss: 0.1091 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.01414\n",
            "Epoch 37/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1203 - accuracy: 0.9710 - val_loss: 0.1158 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.01414\n",
            "Epoch 38/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1073 - accuracy: 0.9874 - val_loss: 0.1308 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.01414\n",
            "Epoch 39/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1275 - accuracy: 0.9644 - val_loss: 0.1028 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.01414\n",
            "Epoch 40/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1158 - accuracy: 0.9640 - val_loss: 0.1052 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.01414\n",
            "Epoch 41/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1112 - accuracy: 0.9708 - val_loss: 0.0906 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.01414\n",
            "Epoch 42/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1082 - accuracy: 0.9745 - val_loss: 0.0860 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.01414\n",
            "Epoch 43/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0873 - accuracy: 0.9787 - val_loss: 0.0921 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.01414\n",
            "Epoch 44/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1132 - accuracy: 0.9859 - val_loss: 0.0817 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.01414\n",
            "Epoch 45/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1163 - accuracy: 0.9219 - val_loss: 0.0812 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.01414\n",
            "Epoch 46/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.0927 - accuracy: 0.9761 - val_loss: 0.0779 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.01414\n",
            "Epoch 47/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.0880 - accuracy: 0.9675 - val_loss: 0.0809 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.01414\n",
            "Epoch 48/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1173 - accuracy: 0.9655 - val_loss: 0.0746 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.01414\n",
            "Epoch 49/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1669 - accuracy: 0.9194 - val_loss: 0.0912 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.01414\n",
            "Epoch 50/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.0750 - accuracy: 0.9916 - val_loss: 0.0719 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.01414\n",
            "Epoch 51/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.0831 - accuracy: 0.9849 - val_loss: 0.0787 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.01414\n",
            "Epoch 52/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0654 - accuracy: 0.9791 - val_loss: 0.0707 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.01414\n",
            "Epoch 53/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0815 - accuracy: 0.9776 - val_loss: 0.0901 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.01414\n",
            "Epoch 54/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0923 - accuracy: 0.9686 - val_loss: 0.0640 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.01414\n",
            "Epoch 55/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1143 - accuracy: 0.9550 - val_loss: 0.0640 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.01414\n",
            "Epoch 56/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0984 - accuracy: 0.9650 - val_loss: 0.0701 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.01414\n",
            "Epoch 57/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0844 - accuracy: 0.9828 - val_loss: 0.0663 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.01414\n",
            "Epoch 58/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.0771 - accuracy: 0.9826 - val_loss: 0.0594 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.01414\n",
            "Epoch 59/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.0770 - accuracy: 0.9644 - val_loss: 0.0621 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.01414\n",
            "Epoch 60/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.0739 - accuracy: 0.9744 - val_loss: 0.0603 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.01414\n",
            "Epoch 61/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0743 - accuracy: 0.9592 - val_loss: 0.0583 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.01414\n",
            "Epoch 62/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1441 - accuracy: 0.9235 - val_loss: 0.0561 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.01414\n",
            "Epoch 63/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.0816 - accuracy: 0.9709 - val_loss: 0.0547 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.01414\n",
            "Epoch 64/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0800 - accuracy: 0.9614 - val_loss: 0.0744 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.01414\n",
            "Epoch 65/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.0937 - accuracy: 0.9689 - val_loss: 0.0637 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.01414\n",
            "Epoch 66/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.0646 - accuracy: 0.9753 - val_loss: 0.0523 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.01414\n",
            "Epoch 67/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1184 - accuracy: 0.9445 - val_loss: 0.0519 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.01414\n",
            "Epoch 68/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0697 - accuracy: 0.9653 - val_loss: 0.0508 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.01414\n",
            "Epoch 69/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1128 - accuracy: 0.9507 - val_loss: 0.0595 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.01414\n",
            "Epoch 70/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.0744 - accuracy: 0.9598 - val_loss: 0.0502 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.01414\n",
            "Epoch 71/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.0765 - accuracy: 0.9721 - val_loss: 0.0567 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.01414\n",
            "Epoch 72/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.0853 - accuracy: 0.9698 - val_loss: 0.0526 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.01414\n",
            "Epoch 73/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0949 - accuracy: 0.9563 - val_loss: 0.0600 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.01414\n",
            "Epoch 74/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0722 - accuracy: 0.9786 - val_loss: 0.0775 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.01414\n",
            "Epoch 75/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0872 - accuracy: 0.9811 - val_loss: 0.0653 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.01414\n",
            "Epoch 76/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.0715 - accuracy: 0.9716 - val_loss: 0.0471 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.01414\n",
            "Epoch 77/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0679 - accuracy: 0.9846 - val_loss: 0.0510 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.01414\n",
            "Epoch 78/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.0546 - accuracy: 0.9692 - val_loss: 0.0501 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.01414\n",
            "Epoch 79/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1161 - accuracy: 0.9455 - val_loss: 0.0464 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.01414\n",
            "Epoch 80/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0547 - accuracy: 0.9726 - val_loss: 0.0441 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.01414\n",
            "Epoch 81/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0773 - accuracy: 0.9700 - val_loss: 0.0435 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.01414\n",
            "Epoch 82/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0621 - accuracy: 0.9682 - val_loss: 0.0440 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.01414\n",
            "Epoch 83/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0679 - accuracy: 0.9782 - val_loss: 0.0523 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.01414\n",
            "Epoch 84/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.0535 - accuracy: 0.9771 - val_loss: 0.0490 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.01414\n",
            "Epoch 85/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1294 - accuracy: 0.9386 - val_loss: 0.0653 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.01414\n",
            "Epoch 86/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1282 - accuracy: 0.9644 - val_loss: 0.0435 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.01414\n",
            "Epoch 87/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.0645 - accuracy: 0.9753 - val_loss: 0.0415 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.01414\n",
            "Epoch 88/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1077 - accuracy: 0.9421 - val_loss: 0.0477 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.01414\n",
            "Epoch 89/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0488 - accuracy: 0.9894 - val_loss: 0.0565 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.01414\n",
            "Epoch 90/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0659 - accuracy: 0.9774 - val_loss: 0.0572 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.01414\n",
            "Epoch 91/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0788 - accuracy: 0.9712 - val_loss: 0.0394 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.01414\n",
            "Epoch 92/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.0763 - accuracy: 0.9441 - val_loss: 0.0423 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.01414\n",
            "Epoch 93/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0775 - accuracy: 0.9608 - val_loss: 0.0401 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.01414\n",
            "Epoch 94/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.0905 - accuracy: 0.9394 - val_loss: 0.0516 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.01414\n",
            "Epoch 95/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.0577 - accuracy: 0.9746 - val_loss: 0.0389 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.01414\n",
            "Epoch 96/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.0721 - accuracy: 0.9661 - val_loss: 0.0490 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.01414\n",
            "Epoch 97/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0923 - accuracy: 0.9530 - val_loss: 0.0396 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.01414\n",
            "Epoch 98/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.0491 - accuracy: 0.9918 - val_loss: 0.0408 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.01414\n",
            "Epoch 99/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.0396 - accuracy: 0.9799 - val_loss: 0.0386 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.01414\n",
            "Epoch 100/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0531 - accuracy: 0.9723 - val_loss: 0.0374 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.01414\n",
            "1/1 [==============================] - 0s 303ms/step - loss: 0.0374 - accuracy: 1.0000\n",
            "\n",
            " 10 fold accuracy:  ['1.0000', '1.0000']\n",
            "Epoch 1/100\n",
            "27/27 [==============================] - 1s 15ms/step - loss: 0.9452 - accuracy: 0.7194 - val_loss: 0.8177 - val_accuracy: 0.8000\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 0.01414\n",
            "Epoch 2/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.7941 - accuracy: 0.8118 - val_loss: 0.7369 - val_accuracy: 0.7333\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.01414\n",
            "Epoch 3/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.7090 - accuracy: 0.8938 - val_loss: 0.6588 - val_accuracy: 0.7333\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.01414\n",
            "Epoch 4/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.6429 - accuracy: 0.7082 - val_loss: 0.5995 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.01414\n",
            "Epoch 5/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.5878 - accuracy: 0.8181 - val_loss: 0.5486 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.01414\n",
            "Epoch 6/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.5434 - accuracy: 0.9370 - val_loss: 0.5052 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.01414\n",
            "Epoch 7/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.5210 - accuracy: 0.8645 - val_loss: 0.4714 - val_accuracy: 0.8667\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.01414\n",
            "Epoch 8/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.4572 - accuracy: 0.8388 - val_loss: 0.4375 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.01414\n",
            "Epoch 9/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.4514 - accuracy: 0.9486 - val_loss: 0.4125 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.01414\n",
            "Epoch 10/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.3954 - accuracy: 0.9610 - val_loss: 0.3892 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.01414\n",
            "Epoch 11/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.3772 - accuracy: 0.9166 - val_loss: 0.3613 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.01414\n",
            "Epoch 12/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.3711 - accuracy: 0.9594 - val_loss: 0.3324 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.01414\n",
            "Epoch 13/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.3572 - accuracy: 0.9226 - val_loss: 0.3101 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.01414\n",
            "Epoch 14/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.3356 - accuracy: 0.9664 - val_loss: 0.2926 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.01414\n",
            "Epoch 15/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.3127 - accuracy: 0.9670 - val_loss: 0.2776 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.01414\n",
            "Epoch 16/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.2910 - accuracy: 0.9779 - val_loss: 0.2666 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.01414\n",
            "Epoch 17/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.3059 - accuracy: 0.9548 - val_loss: 0.2462 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.01414\n",
            "Epoch 18/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.2720 - accuracy: 0.9643 - val_loss: 0.2270 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.01414\n",
            "Epoch 19/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.2651 - accuracy: 0.9789 - val_loss: 0.2191 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.01414\n",
            "Epoch 20/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.2422 - accuracy: 0.9742 - val_loss: 0.2077 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.01414\n",
            "Epoch 21/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.2523 - accuracy: 0.9662 - val_loss: 0.1859 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.01414\n",
            "Epoch 22/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.2375 - accuracy: 0.9357 - val_loss: 0.1751 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.01414\n",
            "Epoch 23/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.2046 - accuracy: 0.9770 - val_loss: 0.1644 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.01414\n",
            "Epoch 24/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.2037 - accuracy: 0.9627 - val_loss: 0.1867 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.01414\n",
            "Epoch 25/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1875 - accuracy: 0.9331 - val_loss: 0.1513 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.01414\n",
            "Epoch 26/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.2327 - accuracy: 0.9432 - val_loss: 0.1459 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.01414\n",
            "Epoch 27/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1879 - accuracy: 0.9688 - val_loss: 0.1344 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.01414\n",
            "Epoch 28/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1651 - accuracy: 0.9851 - val_loss: 0.1271 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.01414\n",
            "Epoch 29/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1998 - accuracy: 0.9538 - val_loss: 0.1178 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.01414\n",
            "Epoch 30/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1913 - accuracy: 0.9430 - val_loss: 0.1328 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.01414\n",
            "Epoch 31/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1681 - accuracy: 0.9698 - val_loss: 0.1040 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.01414\n",
            "Epoch 32/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1511 - accuracy: 0.9857 - val_loss: 0.0984 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.01414\n",
            "Epoch 33/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1786 - accuracy: 0.9433 - val_loss: 0.0994 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.01414\n",
            "Epoch 34/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1290 - accuracy: 0.9706 - val_loss: 0.0892 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.01414\n",
            "Epoch 35/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1492 - accuracy: 0.9820 - val_loss: 0.0843 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.01414\n",
            "Epoch 36/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1616 - accuracy: 0.9622 - val_loss: 0.0847 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.01414\n",
            "Epoch 37/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1526 - accuracy: 0.9799 - val_loss: 0.0863 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.01414\n",
            "Epoch 38/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1585 - accuracy: 0.9699 - val_loss: 0.0876 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.01414\n",
            "Epoch 39/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1309 - accuracy: 0.9825 - val_loss: 0.0708 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.01414\n",
            "Epoch 40/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1464 - accuracy: 0.9591 - val_loss: 0.0741 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.01414\n",
            "Epoch 41/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1450 - accuracy: 0.9874 - val_loss: 0.0813 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.01414\n",
            "Epoch 42/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1171 - accuracy: 0.9707 - val_loss: 0.0631 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.01414\n",
            "Epoch 43/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1583 - accuracy: 0.9334 - val_loss: 0.0687 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.01414\n",
            "Epoch 44/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1235 - accuracy: 0.9692 - val_loss: 0.0607 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.01414\n",
            "Epoch 45/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1356 - accuracy: 0.9531 - val_loss: 0.0695 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.01414\n",
            "Epoch 46/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1223 - accuracy: 0.9770 - val_loss: 0.0542 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.01414\n",
            "Epoch 47/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1026 - accuracy: 0.9874 - val_loss: 0.0572 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.01414\n",
            "Epoch 48/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1214 - accuracy: 0.9786 - val_loss: 0.0489 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.01414\n",
            "Epoch 49/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.0964 - accuracy: 0.9653 - val_loss: 0.0465 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.01414\n",
            "Epoch 50/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1137 - accuracy: 0.9726 - val_loss: 0.0490 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.01414\n",
            "Epoch 51/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.0800 - accuracy: 0.9837 - val_loss: 0.0626 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.01414\n",
            "Epoch 52/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0895 - accuracy: 0.9746 - val_loss: 0.0448 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.01414\n",
            "Epoch 53/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.0975 - accuracy: 0.9803 - val_loss: 0.0472 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.01414\n",
            "Epoch 54/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1323 - accuracy: 0.9404 - val_loss: 0.0583 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.01414\n",
            "Epoch 55/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0960 - accuracy: 0.9625 - val_loss: 0.0382 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.01414\n",
            "Epoch 56/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1302 - accuracy: 0.9551 - val_loss: 0.0438 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.01414\n",
            "Epoch 57/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0971 - accuracy: 0.9829 - val_loss: 0.0357 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.01414\n",
            "Epoch 58/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1384 - accuracy: 0.9781 - val_loss: 0.0348 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.01414\n",
            "Epoch 59/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1173 - accuracy: 0.9703 - val_loss: 0.0372 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.01414\n",
            "Epoch 60/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0856 - accuracy: 0.9745 - val_loss: 0.0428 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.01414\n",
            "Epoch 61/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1178 - accuracy: 0.9667 - val_loss: 0.0398 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.01414\n",
            "Epoch 62/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0933 - accuracy: 0.9787 - val_loss: 0.0329 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.01414\n",
            "Epoch 63/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0990 - accuracy: 0.9835 - val_loss: 0.0310 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.01414\n",
            "Epoch 64/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0834 - accuracy: 0.9660 - val_loss: 0.0628 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.01414\n",
            "Epoch 65/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.0914 - accuracy: 0.9733 - val_loss: 0.0312 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.01414\n",
            "Epoch 66/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1117 - accuracy: 0.9488 - val_loss: 0.0412 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.01414\n",
            "Epoch 67/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1015 - accuracy: 0.9814 - val_loss: 0.0310 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.01414\n",
            "Epoch 68/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0795 - accuracy: 0.9805 - val_loss: 0.0886 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.01414\n",
            "Epoch 69/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1399 - accuracy: 0.9773 - val_loss: 0.0274 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.01414\n",
            "Epoch 70/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0970 - accuracy: 0.9890 - val_loss: 0.0269 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.01414\n",
            "Epoch 71/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0834 - accuracy: 0.9865 - val_loss: 0.0334 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.01414\n",
            "Epoch 72/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0912 - accuracy: 0.9883 - val_loss: 0.0237 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.01414\n",
            "Epoch 73/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.0938 - accuracy: 0.9952 - val_loss: 0.0241 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.01414\n",
            "Epoch 74/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1246 - accuracy: 0.9041 - val_loss: 0.0410 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.01414\n",
            "Epoch 75/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1455 - accuracy: 0.9532 - val_loss: 0.0252 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.01414\n",
            "Epoch 76/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1039 - accuracy: 0.9718 - val_loss: 0.0221 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.01414\n",
            "Epoch 77/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1358 - accuracy: 0.9570 - val_loss: 0.0255 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.01414\n",
            "Epoch 78/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0913 - accuracy: 0.9746 - val_loss: 0.0256 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.01414\n",
            "Epoch 79/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0769 - accuracy: 0.9766 - val_loss: 0.0275 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.01414\n",
            "Epoch 80/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.0854 - accuracy: 0.9547 - val_loss: 0.0247 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.01414\n",
            "Epoch 81/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1151 - accuracy: 0.9482 - val_loss: 0.0232 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.01414\n",
            "Epoch 82/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0754 - accuracy: 0.9713 - val_loss: 0.0259 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.01414\n",
            "Epoch 83/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0797 - accuracy: 0.9676 - val_loss: 0.0229 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.01414\n",
            "Epoch 84/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0753 - accuracy: 0.9869 - val_loss: 0.0189 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.01414\n",
            "Epoch 85/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1057 - accuracy: 0.9888 - val_loss: 0.0238 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.01414\n",
            "Epoch 86/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0760 - accuracy: 0.9760 - val_loss: 0.0226 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.01414\n",
            "Epoch 87/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.0820 - accuracy: 0.9717 - val_loss: 0.0174 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.01414\n",
            "Epoch 88/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0897 - accuracy: 0.9590 - val_loss: 0.0177 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.01414\n",
            "Epoch 89/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0690 - accuracy: 0.9826 - val_loss: 0.0240 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.01414\n",
            "Epoch 90/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0629 - accuracy: 0.9815 - val_loss: 0.0165 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.01414\n",
            "Epoch 91/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.0635 - accuracy: 0.9896 - val_loss: 0.0174 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.01414\n",
            "Epoch 92/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.0726 - accuracy: 0.9721 - val_loss: 0.0193 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.01414\n",
            "Epoch 93/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1029 - accuracy: 0.9667 - val_loss: 0.0207 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.01414\n",
            "Epoch 94/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.0639 - accuracy: 0.9793 - val_loss: 0.0192 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.01414\n",
            "Epoch 95/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.0383 - accuracy: 0.9939 - val_loss: 0.0153 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.01414\n",
            "Epoch 96/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.0685 - accuracy: 0.9878 - val_loss: 0.0152 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.01414\n",
            "Epoch 97/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1088 - accuracy: 0.9669 - val_loss: 0.0253 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.01414\n",
            "Epoch 98/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.0584 - accuracy: 0.9839 - val_loss: 0.0165 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.01414\n",
            "Epoch 99/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1537 - accuracy: 0.9187 - val_loss: 0.0148 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.01414\n",
            "Epoch 100/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1360 - accuracy: 0.9632 - val_loss: 0.0145 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.01414\n",
            "1/1 [==============================] - 0s 283ms/step - loss: 0.0145 - accuracy: 1.0000\n",
            "\n",
            " 10 fold accuracy:  ['1.0000', '1.0000', '1.0000']\n",
            "Epoch 1/100\n",
            "27/27 [==============================] - 1s 15ms/step - loss: 2.0410 - accuracy: 0.2990 - val_loss: 1.4854 - val_accuracy: 0.4000\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 0.01414\n",
            "Epoch 2/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 1.4059 - accuracy: 0.3307 - val_loss: 1.1615 - val_accuracy: 0.6000\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.01414\n",
            "Epoch 3/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 1.1116 - accuracy: 0.4701 - val_loss: 1.0664 - val_accuracy: 0.4000\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.01414\n",
            "Epoch 4/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 1.0583 - accuracy: 0.3390 - val_loss: 1.0364 - val_accuracy: 0.6000\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.01414\n",
            "Epoch 5/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 1.0312 - accuracy: 0.5991 - val_loss: 1.0036 - val_accuracy: 0.6667\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.01414\n",
            "Epoch 6/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.9799 - accuracy: 0.7244 - val_loss: 0.9593 - val_accuracy: 0.7333\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.01414\n",
            "Epoch 7/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.9323 - accuracy: 0.8944 - val_loss: 0.9180 - val_accuracy: 0.7333\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.01414\n",
            "Epoch 8/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.8836 - accuracy: 0.8555 - val_loss: 0.8697 - val_accuracy: 0.7333\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.01414\n",
            "Epoch 9/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.8840 - accuracy: 0.8082 - val_loss: 0.8229 - val_accuracy: 0.8667\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.01414\n",
            "Epoch 10/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.8295 - accuracy: 0.8005 - val_loss: 0.7877 - val_accuracy: 0.6667\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.01414\n",
            "Epoch 11/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.7649 - accuracy: 0.8472 - val_loss: 0.7466 - val_accuracy: 0.7333\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.01414\n",
            "Epoch 12/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.7308 - accuracy: 0.8905 - val_loss: 0.7188 - val_accuracy: 0.7333\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.01414\n",
            "Epoch 13/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.7292 - accuracy: 0.8349 - val_loss: 0.6921 - val_accuracy: 0.7333\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.01414\n",
            "Epoch 14/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.7284 - accuracy: 0.8345 - val_loss: 0.6684 - val_accuracy: 0.7333\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.01414\n",
            "Epoch 15/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.6640 - accuracy: 0.8888 - val_loss: 0.6564 - val_accuracy: 0.6667\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.01414\n",
            "Epoch 16/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.7010 - accuracy: 0.7465 - val_loss: 0.6336 - val_accuracy: 0.7333\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.01414\n",
            "Epoch 17/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.6665 - accuracy: 0.8500 - val_loss: 0.6264 - val_accuracy: 0.6667\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.01414\n",
            "Epoch 18/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.6503 - accuracy: 0.8208 - val_loss: 0.5970 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.01414\n",
            "Epoch 19/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.5983 - accuracy: 0.9592 - val_loss: 0.5914 - val_accuracy: 0.7333\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.01414\n",
            "Epoch 20/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.6217 - accuracy: 0.8646 - val_loss: 0.5840 - val_accuracy: 0.6667\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.01414\n",
            "Epoch 21/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.6195 - accuracy: 0.7950 - val_loss: 0.5666 - val_accuracy: 0.7333\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.01414\n",
            "Epoch 22/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.5576 - accuracy: 0.8413 - val_loss: 0.5411 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.01414\n",
            "Epoch 23/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.5330 - accuracy: 0.9343 - val_loss: 0.5422 - val_accuracy: 0.8000\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.01414\n",
            "Epoch 24/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.5731 - accuracy: 0.8853 - val_loss: 0.5455 - val_accuracy: 0.6667\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.01414\n",
            "Epoch 25/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.5291 - accuracy: 0.8906 - val_loss: 0.5355 - val_accuracy: 0.7333\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.01414\n",
            "Epoch 26/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.4827 - accuracy: 0.8540 - val_loss: 0.5180 - val_accuracy: 0.7333\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.01414\n",
            "Epoch 27/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.4761 - accuracy: 0.9257 - val_loss: 0.5066 - val_accuracy: 0.8000\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.01414\n",
            "Epoch 28/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.4483 - accuracy: 0.9454 - val_loss: 0.5027 - val_accuracy: 0.7333\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.01414\n",
            "Epoch 29/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.4849 - accuracy: 0.8672 - val_loss: 0.4761 - val_accuracy: 0.8000\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.01414\n",
            "Epoch 30/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.4317 - accuracy: 0.9197 - val_loss: 0.4602 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.01414\n",
            "Epoch 31/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.4433 - accuracy: 0.9834 - val_loss: 0.4717 - val_accuracy: 0.8000\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.01414\n",
            "Epoch 32/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.4093 - accuracy: 0.9416 - val_loss: 0.4452 - val_accuracy: 0.8667\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.01414\n",
            "Epoch 33/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.3828 - accuracy: 0.9757 - val_loss: 0.4423 - val_accuracy: 0.8000\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.01414\n",
            "Epoch 34/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.4212 - accuracy: 0.9367 - val_loss: 0.4359 - val_accuracy: 0.8000\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.01414\n",
            "Epoch 35/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.3761 - accuracy: 0.9540 - val_loss: 0.4151 - val_accuracy: 0.8667\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.01414\n",
            "Epoch 36/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.3195 - accuracy: 0.9619 - val_loss: 0.3765 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.01414\n",
            "Epoch 37/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.3697 - accuracy: 0.9607 - val_loss: 0.3742 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.01414\n",
            "Epoch 38/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.3059 - accuracy: 0.9882 - val_loss: 0.3721 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.01414\n",
            "Epoch 39/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.3108 - accuracy: 0.9757 - val_loss: 0.3612 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.01414\n",
            "Epoch 40/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.2913 - accuracy: 0.9619 - val_loss: 0.3375 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.01414\n",
            "Epoch 41/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.2596 - accuracy: 0.9503 - val_loss: 0.3473 - val_accuracy: 0.8667\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.01414\n",
            "Epoch 42/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.2748 - accuracy: 0.9637 - val_loss: 0.3077 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.01414\n",
            "Epoch 43/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.2240 - accuracy: 0.9843 - val_loss: 0.3014 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.01414\n",
            "Epoch 44/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.2112 - accuracy: 0.9635 - val_loss: 0.2666 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.01414\n",
            "Epoch 45/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1893 - accuracy: 0.9734 - val_loss: 0.2495 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.01414\n",
            "Epoch 46/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1997 - accuracy: 0.9758 - val_loss: 0.2183 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.01414\n",
            "Epoch 47/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1796 - accuracy: 0.9645 - val_loss: 0.2217 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.01414\n",
            "Epoch 48/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1832 - accuracy: 0.9525 - val_loss: 0.2479 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.01414\n",
            "Epoch 49/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1543 - accuracy: 0.9794 - val_loss: 0.2527 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.01414\n",
            "Epoch 50/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1375 - accuracy: 0.9661 - val_loss: 0.1784 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.01414\n",
            "Epoch 51/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1280 - accuracy: 0.9801 - val_loss: 0.1846 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.01414\n",
            "Epoch 52/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1154 - accuracy: 0.9659 - val_loss: 0.1689 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.01414\n",
            "Epoch 53/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1236 - accuracy: 0.9576 - val_loss: 0.1833 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.01414\n",
            "Epoch 54/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1125 - accuracy: 0.9812 - val_loss: 0.1549 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.01414\n",
            "Epoch 55/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.0970 - accuracy: 0.9849 - val_loss: 0.1462 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.01414\n",
            "Epoch 56/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1011 - accuracy: 0.9664 - val_loss: 0.1668 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.01414\n",
            "Epoch 57/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0891 - accuracy: 0.9766 - val_loss: 0.1420 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.01414\n",
            "Epoch 58/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.0991 - accuracy: 0.9825 - val_loss: 0.1564 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.01414\n",
            "Epoch 59/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1227 - accuracy: 0.9467 - val_loss: 0.1313 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.01414\n",
            "Epoch 60/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.0754 - accuracy: 0.9836 - val_loss: 0.1295 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.01414\n",
            "Epoch 61/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0903 - accuracy: 0.9556 - val_loss: 0.1250 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.01414\n",
            "Epoch 62/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0863 - accuracy: 0.9833 - val_loss: 0.1393 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.01414\n",
            "Epoch 63/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.0924 - accuracy: 0.9576 - val_loss: 0.1229 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.01414\n",
            "Epoch 64/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.0879 - accuracy: 0.9877 - val_loss: 0.1785 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.01414\n",
            "Epoch 65/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1021 - accuracy: 0.9592 - val_loss: 0.1205 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.01414\n",
            "Epoch 66/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.0848 - accuracy: 0.9801 - val_loss: 0.1642 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.01414\n",
            "Epoch 67/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0660 - accuracy: 0.9821 - val_loss: 0.1336 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.01414\n",
            "Epoch 68/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0783 - accuracy: 0.9842 - val_loss: 0.1267 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.01414\n",
            "Epoch 69/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.0686 - accuracy: 0.9797 - val_loss: 0.1068 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.01414\n",
            "Epoch 70/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0660 - accuracy: 0.9737 - val_loss: 0.1070 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.01414\n",
            "Epoch 71/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0806 - accuracy: 0.9738 - val_loss: 0.1239 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.01414\n",
            "Epoch 72/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0630 - accuracy: 0.9775 - val_loss: 0.1158 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.01414\n",
            "Epoch 73/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.0592 - accuracy: 0.9839 - val_loss: 0.1178 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.01414\n",
            "Epoch 74/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.0633 - accuracy: 0.9865 - val_loss: 0.1597 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.01414\n",
            "Epoch 75/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0711 - accuracy: 0.9844 - val_loss: 0.0999 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.01414\n",
            "Epoch 76/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1113 - accuracy: 0.9535 - val_loss: 0.1647 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.01414\n",
            "Epoch 77/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0699 - accuracy: 0.9690 - val_loss: 0.1064 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.01414\n",
            "Epoch 78/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0930 - accuracy: 0.9543 - val_loss: 0.1036 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.01414\n",
            "Epoch 79/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0832 - accuracy: 0.9689 - val_loss: 0.1527 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.01414\n",
            "Epoch 80/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0772 - accuracy: 0.9724 - val_loss: 0.1267 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.01414\n",
            "Epoch 81/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0785 - accuracy: 0.9786 - val_loss: 0.1591 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.01414\n",
            "Epoch 82/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0495 - accuracy: 0.9916 - val_loss: 0.0958 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.01414\n",
            "Epoch 83/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.0744 - accuracy: 0.9794 - val_loss: 0.1026 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.01414\n",
            "Epoch 84/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0738 - accuracy: 0.9618 - val_loss: 0.1032 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.01414\n",
            "Epoch 85/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.0956 - accuracy: 0.9388 - val_loss: 0.1047 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.01414\n",
            "Epoch 86/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.0874 - accuracy: 0.9698 - val_loss: 0.1109 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.01414\n",
            "Epoch 87/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.0374 - accuracy: 0.9909 - val_loss: 0.1467 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.01414\n",
            "Epoch 88/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.0741 - accuracy: 0.9668 - val_loss: 0.1436 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.01414\n",
            "Epoch 89/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0681 - accuracy: 0.9750 - val_loss: 0.0887 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.01414\n",
            "Epoch 90/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0716 - accuracy: 0.9692 - val_loss: 0.1171 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.01414\n",
            "Epoch 91/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0916 - accuracy: 0.9646 - val_loss: 0.0884 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.01414\n",
            "Epoch 92/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.0587 - accuracy: 0.9860 - val_loss: 0.1355 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.01414\n",
            "Epoch 93/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.0489 - accuracy: 0.9834 - val_loss: 0.0993 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.01414\n",
            "Epoch 94/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.0718 - accuracy: 0.9864 - val_loss: 0.1088 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.01414\n",
            "Epoch 95/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1026 - accuracy: 0.9501 - val_loss: 0.0841 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.01414\n",
            "Epoch 96/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0549 - accuracy: 0.9907 - val_loss: 0.1568 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.01414\n",
            "Epoch 97/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0612 - accuracy: 0.9816 - val_loss: 0.0872 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.01414\n",
            "Epoch 98/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0525 - accuracy: 0.9875 - val_loss: 0.1506 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.01414\n",
            "Epoch 99/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0417 - accuracy: 0.9913 - val_loss: 0.1128 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.01414\n",
            "Epoch 100/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.0810 - accuracy: 0.9487 - val_loss: 0.1985 - val_accuracy: 0.8667\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.01414\n",
            "1/1 [==============================] - 0s 265ms/step - loss: 0.1985 - accuracy: 0.8667\n",
            "\n",
            " 10 fold accuracy:  ['1.0000', '1.0000', '1.0000', '0.8667']\n",
            "Epoch 1/100\n",
            "27/27 [==============================] - 2s 14ms/step - loss: 1.4265 - accuracy: 0.3465 - val_loss: 1.2162 - val_accuracy: 0.1333\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 0.01414\n",
            "Epoch 2/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 1.0873 - accuracy: 0.3393 - val_loss: 1.0641 - val_accuracy: 0.1333\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.01414\n",
            "Epoch 3/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.9965 - accuracy: 0.3177 - val_loss: 0.9707 - val_accuracy: 0.2667\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.01414\n",
            "Epoch 4/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.9109 - accuracy: 0.5704 - val_loss: 0.8745 - val_accuracy: 0.6000\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.01414\n",
            "Epoch 5/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.8320 - accuracy: 0.7838 - val_loss: 0.7700 - val_accuracy: 0.8667\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.01414\n",
            "Epoch 6/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.7679 - accuracy: 0.6712 - val_loss: 0.7724 - val_accuracy: 0.8667\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.01414\n",
            "Epoch 7/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.8010 - accuracy: 0.7133 - val_loss: 0.7395 - val_accuracy: 0.8667\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.01414\n",
            "Epoch 8/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.7615 - accuracy: 0.6218 - val_loss: 0.7189 - val_accuracy: 0.8667\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.01414\n",
            "Epoch 9/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.7631 - accuracy: 0.7157 - val_loss: 0.7088 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.01414\n",
            "Epoch 10/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.7115 - accuracy: 0.6865 - val_loss: 0.7073 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.01414\n",
            "Epoch 11/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.7075 - accuracy: 0.9402 - val_loss: 0.6734 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.01414\n",
            "Epoch 12/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.7352 - accuracy: 0.8466 - val_loss: 0.6659 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.01414\n",
            "Epoch 13/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.6666 - accuracy: 0.8654 - val_loss: 0.6545 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.01414\n",
            "Epoch 14/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.6568 - accuracy: 0.9478 - val_loss: 0.6443 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.01414\n",
            "Epoch 15/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.6406 - accuracy: 0.8798 - val_loss: 0.6283 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.01414\n",
            "Epoch 16/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.6116 - accuracy: 0.9576 - val_loss: 0.6346 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.01414\n",
            "Epoch 17/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.6208 - accuracy: 0.9569 - val_loss: 0.5717 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.01414\n",
            "Epoch 18/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.5827 - accuracy: 0.9113 - val_loss: 0.6027 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.01414\n",
            "Epoch 19/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.5804 - accuracy: 0.9768 - val_loss: 0.5500 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.01414\n",
            "Epoch 20/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.5615 - accuracy: 0.9146 - val_loss: 0.4983 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.01414\n",
            "Epoch 21/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.5535 - accuracy: 0.8663 - val_loss: 0.5371 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.01414\n",
            "Epoch 22/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.5426 - accuracy: 0.9684 - val_loss: 0.4667 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.01414\n",
            "Epoch 23/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.5348 - accuracy: 0.9164 - val_loss: 0.4802 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.01414\n",
            "Epoch 24/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.5217 - accuracy: 0.9488 - val_loss: 0.4856 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.01414\n",
            "Epoch 25/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.4697 - accuracy: 0.9605 - val_loss: 0.4324 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.01414\n",
            "Epoch 26/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.4709 - accuracy: 0.9448 - val_loss: 0.4467 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.01414\n",
            "Epoch 27/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.4862 - accuracy: 0.9617 - val_loss: 0.4244 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.01414\n",
            "Epoch 28/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.4634 - accuracy: 0.9839 - val_loss: 0.4206 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.01414\n",
            "Epoch 29/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.4262 - accuracy: 0.9598 - val_loss: 0.4344 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.01414\n",
            "Epoch 30/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.4357 - accuracy: 0.9751 - val_loss: 0.3703 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.01414\n",
            "Epoch 31/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.4278 - accuracy: 0.9163 - val_loss: 0.3758 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.01414\n",
            "Epoch 32/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.4002 - accuracy: 0.9903 - val_loss: 0.3857 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.01414\n",
            "Epoch 33/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.3840 - accuracy: 0.9604 - val_loss: 0.3781 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.01414\n",
            "Epoch 34/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.3564 - accuracy: 0.9691 - val_loss: 0.3614 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.01414\n",
            "Epoch 35/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.3571 - accuracy: 0.9684 - val_loss: 0.3283 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.01414\n",
            "Epoch 36/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.3656 - accuracy: 0.9581 - val_loss: 0.3655 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.01414\n",
            "Epoch 37/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.3307 - accuracy: 0.9767 - val_loss: 0.4084 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.01414\n",
            "Epoch 38/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.3532 - accuracy: 0.9503 - val_loss: 0.3008 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.01414\n",
            "Epoch 39/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.3244 - accuracy: 0.9585 - val_loss: 0.3280 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.01414\n",
            "Epoch 40/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.3072 - accuracy: 0.9861 - val_loss: 0.3318 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.01414\n",
            "Epoch 41/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.2922 - accuracy: 0.9865 - val_loss: 0.3012 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.01414\n",
            "Epoch 42/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.2759 - accuracy: 0.9877 - val_loss: 0.3218 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.01414\n",
            "Epoch 43/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.2645 - accuracy: 0.9829 - val_loss: 0.2909 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.01414\n",
            "Epoch 44/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.2710 - accuracy: 0.9916 - val_loss: 0.3305 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.01414\n",
            "Epoch 45/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.2544 - accuracy: 0.9716 - val_loss: 0.2997 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.01414\n",
            "Epoch 46/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.2823 - accuracy: 0.9617 - val_loss: 0.2686 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.01414\n",
            "Epoch 47/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.2562 - accuracy: 0.9900 - val_loss: 0.2636 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.01414\n",
            "Epoch 48/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.2358 - accuracy: 0.9874 - val_loss: 0.2649 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.01414\n",
            "Epoch 49/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.2552 - accuracy: 0.9693 - val_loss: 0.2611 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.01414\n",
            "Epoch 50/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.2436 - accuracy: 0.9926 - val_loss: 0.2465 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.01414\n",
            "Epoch 51/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.2392 - accuracy: 0.9908 - val_loss: 0.2423 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.01414\n",
            "Epoch 52/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.2456 - accuracy: 0.9912 - val_loss: 0.2527 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.01414\n",
            "Epoch 53/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.2194 - accuracy: 0.9776 - val_loss: 0.2752 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.01414\n",
            "Epoch 54/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.2365 - accuracy: 0.9754 - val_loss: 0.2211 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.01414\n",
            "Epoch 55/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.2113 - accuracy: 0.9831 - val_loss: 0.2618 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.01414\n",
            "Epoch 56/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.2344 - accuracy: 0.9594 - val_loss: 0.2123 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.01414\n",
            "Epoch 57/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.2385 - accuracy: 0.9724 - val_loss: 0.2176 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.01414\n",
            "Epoch 58/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.2320 - accuracy: 0.9809 - val_loss: 0.2106 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.01414\n",
            "Epoch 59/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1783 - accuracy: 0.9695 - val_loss: 0.2096 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.01414\n",
            "Epoch 60/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.2286 - accuracy: 0.9654 - val_loss: 0.2153 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.01414\n",
            "Epoch 61/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.2059 - accuracy: 0.9719 - val_loss: 0.1802 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.01414\n",
            "Epoch 62/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.2109 - accuracy: 0.9595 - val_loss: 0.2316 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.01414\n",
            "Epoch 63/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1573 - accuracy: 0.9978 - val_loss: 0.1870 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.01414\n",
            "Epoch 64/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1948 - accuracy: 0.9707 - val_loss: 0.1698 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.01414\n",
            "Epoch 65/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1714 - accuracy: 0.9908 - val_loss: 0.2208 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.01414\n",
            "Epoch 66/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1622 - accuracy: 0.9961 - val_loss: 0.1865 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.01414\n",
            "Epoch 67/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1851 - accuracy: 0.9751 - val_loss: 0.1984 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.01414\n",
            "Epoch 68/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1725 - accuracy: 0.9705 - val_loss: 0.1724 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.01414\n",
            "Epoch 69/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1658 - accuracy: 0.9769 - val_loss: 0.1903 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.01414\n",
            "Epoch 70/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1582 - accuracy: 0.9943 - val_loss: 0.2126 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.01414\n",
            "Epoch 71/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1804 - accuracy: 0.9883 - val_loss: 0.1794 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.01414\n",
            "Epoch 72/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1822 - accuracy: 0.9698 - val_loss: 0.1866 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.01414\n",
            "Epoch 73/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1665 - accuracy: 0.9867 - val_loss: 0.2063 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.01414\n",
            "Epoch 74/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1428 - accuracy: 0.9897 - val_loss: 0.1690 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.01414\n",
            "Epoch 75/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1467 - accuracy: 0.9902 - val_loss: 0.2422 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.01414\n",
            "Epoch 76/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.2097 - accuracy: 0.9157 - val_loss: 0.1365 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.01414\n",
            "Epoch 77/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1672 - accuracy: 0.9417 - val_loss: 0.1739 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.01414\n",
            "Epoch 78/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1327 - accuracy: 0.9856 - val_loss: 0.2434 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.01414\n",
            "Epoch 79/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1522 - accuracy: 0.9791 - val_loss: 0.2013 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.01414\n",
            "Epoch 80/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1453 - accuracy: 0.9812 - val_loss: 0.1392 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.01414\n",
            "Epoch 81/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1849 - accuracy: 0.9569 - val_loss: 0.1258 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.01414\n",
            "Epoch 82/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1428 - accuracy: 0.9713 - val_loss: 0.1901 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.01414\n",
            "Epoch 83/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1470 - accuracy: 0.9802 - val_loss: 0.1498 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.01414\n",
            "Epoch 84/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1390 - accuracy: 0.9896 - val_loss: 0.1708 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.01414\n",
            "Epoch 85/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1575 - accuracy: 0.9711 - val_loss: 0.1395 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.01414\n",
            "Epoch 86/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1174 - accuracy: 0.9984 - val_loss: 0.1379 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.01414\n",
            "Epoch 87/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1839 - accuracy: 0.9417 - val_loss: 0.1325 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.01414\n",
            "Epoch 88/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1438 - accuracy: 0.9766 - val_loss: 0.1554 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.01414\n",
            "Epoch 89/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1370 - accuracy: 0.9803 - val_loss: 0.1101 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.01414\n",
            "Epoch 90/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1312 - accuracy: 0.9792 - val_loss: 0.1698 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.01414\n",
            "Epoch 91/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1166 - accuracy: 0.9897 - val_loss: 0.2173 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.01414\n",
            "Epoch 92/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1668 - accuracy: 0.9664 - val_loss: 0.1456 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.01414\n",
            "Epoch 93/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1450 - accuracy: 0.9415 - val_loss: 0.1063 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.01414\n",
            "Epoch 94/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1102 - accuracy: 0.9930 - val_loss: 0.2164 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.01414\n",
            "Epoch 95/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1311 - accuracy: 0.9807 - val_loss: 0.1035 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.01414\n",
            "Epoch 96/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1348 - accuracy: 0.9599 - val_loss: 0.1339 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.01414\n",
            "Epoch 97/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1181 - accuracy: 0.9962 - val_loss: 0.1729 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.01414\n",
            "Epoch 98/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1262 - accuracy: 0.9921 - val_loss: 0.1988 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.01414\n",
            "Epoch 99/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1160 - accuracy: 0.9808 - val_loss: 0.1295 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.01414\n",
            "Epoch 100/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1120 - accuracy: 0.9785 - val_loss: 0.1577 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.01414\n",
            "1/1 [==============================] - 0s 288ms/step - loss: 0.1577 - accuracy: 0.9333\n",
            "\n",
            " 10 fold accuracy:  ['1.0000', '1.0000', '1.0000', '0.8667', '0.9333']\n",
            "Epoch 1/100\n",
            "27/27 [==============================] - 1s 15ms/step - loss: 2.5666 - accuracy: 0.2603 - val_loss: 0.8927 - val_accuracy: 0.6667\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 0.01414\n",
            "Epoch 2/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 1.2266 - accuracy: 0.3520 - val_loss: 1.0053 - val_accuracy: 0.1333\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.01414\n",
            "Epoch 3/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.9868 - accuracy: 0.3208 - val_loss: 0.9402 - val_accuracy: 0.1333\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.01414\n",
            "Epoch 4/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.7934 - accuracy: 0.5797 - val_loss: 0.9307 - val_accuracy: 0.3333\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.01414\n",
            "Epoch 5/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.7008 - accuracy: 0.7581 - val_loss: 0.8481 - val_accuracy: 0.3333\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.01414\n",
            "Epoch 6/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.6617 - accuracy: 0.8159 - val_loss: 0.8153 - val_accuracy: 0.4000\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.01414\n",
            "Epoch 7/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.5987 - accuracy: 0.7433 - val_loss: 0.7504 - val_accuracy: 0.6667\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.01414\n",
            "Epoch 8/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.5191 - accuracy: 0.7917 - val_loss: 0.7061 - val_accuracy: 0.6667\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.01414\n",
            "Epoch 9/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.4846 - accuracy: 0.8821 - val_loss: 0.6442 - val_accuracy: 0.6667\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.01414\n",
            "Epoch 10/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.4748 - accuracy: 0.8240 - val_loss: 0.6413 - val_accuracy: 0.6667\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.01414\n",
            "Epoch 11/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.4131 - accuracy: 0.9726 - val_loss: 0.5956 - val_accuracy: 0.7333\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.01414\n",
            "Epoch 12/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.3937 - accuracy: 0.9023 - val_loss: 0.5335 - val_accuracy: 0.8000\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.01414\n",
            "Epoch 13/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.3588 - accuracy: 0.9716 - val_loss: 0.5480 - val_accuracy: 0.8000\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.01414\n",
            "Epoch 14/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.3231 - accuracy: 0.9642 - val_loss: 0.5095 - val_accuracy: 0.8000\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.01414\n",
            "Epoch 15/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.3190 - accuracy: 0.9709 - val_loss: 0.4537 - val_accuracy: 0.8000\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.01414\n",
            "Epoch 16/100\n",
            "27/27 [==============================] - 0s 5ms/step - loss: 0.2901 - accuracy: 0.9642 - val_loss: 0.5041 - val_accuracy: 0.8000\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.01414\n",
            "Epoch 17/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.2846 - accuracy: 0.9504 - val_loss: 0.4055 - val_accuracy: 0.8000\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.01414\n",
            "Epoch 18/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.2463 - accuracy: 0.9739 - val_loss: 0.3934 - val_accuracy: 0.8000\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.01414\n",
            "Epoch 19/100\n",
            "27/27 [==============================] - 0s 5ms/step - loss: 0.2538 - accuracy: 0.9835 - val_loss: 0.4601 - val_accuracy: 0.8000\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.01414\n",
            "Epoch 20/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1969 - accuracy: 0.9848 - val_loss: 0.4052 - val_accuracy: 0.8000\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.01414\n",
            "Epoch 21/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1895 - accuracy: 0.9738 - val_loss: 0.3416 - val_accuracy: 0.8667\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.01414\n",
            "Epoch 22/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1773 - accuracy: 0.9864 - val_loss: 0.3231 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.01414\n",
            "Epoch 23/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1558 - accuracy: 0.9930 - val_loss: 0.4055 - val_accuracy: 0.8000\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.01414\n",
            "Epoch 24/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1879 - accuracy: 0.9921 - val_loss: 0.3280 - val_accuracy: 0.8000\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.01414\n",
            "Epoch 25/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1575 - accuracy: 0.9924 - val_loss: 0.3479 - val_accuracy: 0.8000\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.01414\n",
            "Epoch 26/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1466 - accuracy: 0.9905 - val_loss: 0.3907 - val_accuracy: 0.8000\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.01414\n",
            "Epoch 27/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1380 - accuracy: 0.9826 - val_loss: 0.3376 - val_accuracy: 0.8000\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.01414\n",
            "Epoch 28/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1455 - accuracy: 0.9905 - val_loss: 0.4178 - val_accuracy: 0.8000\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.01414\n",
            "Epoch 29/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1351 - accuracy: 0.9961 - val_loss: 0.4193 - val_accuracy: 0.8000\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.01414\n",
            "Epoch 30/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1386 - accuracy: 0.9719 - val_loss: 0.4233 - val_accuracy: 0.8000\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.01414\n",
            "Epoch 31/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1111 - accuracy: 0.9969 - val_loss: 0.3025 - val_accuracy: 0.8667\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.01414\n",
            "Epoch 32/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1050 - accuracy: 0.9906 - val_loss: 0.2816 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.01414\n",
            "Epoch 33/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1055 - accuracy: 0.9992 - val_loss: 0.3728 - val_accuracy: 0.8000\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.01414\n",
            "Epoch 34/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1025 - accuracy: 0.9695 - val_loss: 0.3064 - val_accuracy: 0.8000\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.01414\n",
            "Epoch 35/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0900 - accuracy: 0.9989 - val_loss: 0.3643 - val_accuracy: 0.8000\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.01414\n",
            "Epoch 36/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0969 - accuracy: 0.9863 - val_loss: 0.4449 - val_accuracy: 0.8000\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.01414\n",
            "Epoch 37/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0972 - accuracy: 0.9965 - val_loss: 0.3135 - val_accuracy: 0.8000\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.01414\n",
            "Epoch 38/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0965 - accuracy: 0.9791 - val_loss: 0.4161 - val_accuracy: 0.8000\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.01414\n",
            "Epoch 39/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1057 - accuracy: 0.9791 - val_loss: 0.2941 - val_accuracy: 0.8667\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.01414\n",
            "Epoch 40/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0709 - accuracy: 0.9914 - val_loss: 0.3338 - val_accuracy: 0.8000\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.01414\n",
            "Epoch 41/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1070 - accuracy: 0.9894 - val_loss: 0.4172 - val_accuracy: 0.8000\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.01414\n",
            "Epoch 42/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0900 - accuracy: 0.9866 - val_loss: 0.3716 - val_accuracy: 0.8000\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.01414\n",
            "Epoch 43/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1239 - accuracy: 0.9546 - val_loss: 0.3197 - val_accuracy: 0.8000\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.01414\n",
            "Epoch 44/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0707 - accuracy: 0.9848 - val_loss: 0.3061 - val_accuracy: 0.8667\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.01414\n",
            "Epoch 45/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0543 - accuracy: 0.9935 - val_loss: 0.3743 - val_accuracy: 0.8000\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.01414\n",
            "Epoch 46/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0766 - accuracy: 0.9748 - val_loss: 0.3410 - val_accuracy: 0.8000\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.01414\n",
            "Epoch 47/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0721 - accuracy: 0.9941 - val_loss: 0.3650 - val_accuracy: 0.8000\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.01414\n",
            "Epoch 48/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0628 - accuracy: 0.9852 - val_loss: 0.2309 - val_accuracy: 0.8667\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.01414\n",
            "Epoch 49/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0574 - accuracy: 0.9938 - val_loss: 0.2523 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.01414\n",
            "Epoch 50/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0833 - accuracy: 0.9600 - val_loss: 0.3703 - val_accuracy: 0.8000\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.01414\n",
            "Epoch 51/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0588 - accuracy: 0.9965 - val_loss: 0.4508 - val_accuracy: 0.8000\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.01414\n",
            "Epoch 52/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0543 - accuracy: 0.9899 - val_loss: 0.4336 - val_accuracy: 0.8000\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.01414\n",
            "Epoch 53/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0706 - accuracy: 0.9740 - val_loss: 0.3318 - val_accuracy: 0.8667\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.01414\n",
            "Epoch 54/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0537 - accuracy: 0.9965 - val_loss: 0.2969 - val_accuracy: 0.8667\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.01414\n",
            "Epoch 55/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0663 - accuracy: 0.9791 - val_loss: 0.3701 - val_accuracy: 0.8000\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.01414\n",
            "Epoch 56/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0492 - accuracy: 0.9989 - val_loss: 0.3190 - val_accuracy: 0.8667\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.01414\n",
            "Epoch 57/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0469 - accuracy: 0.9929 - val_loss: 0.3881 - val_accuracy: 0.8000\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.01414\n",
            "Epoch 58/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0613 - accuracy: 0.9827 - val_loss: 0.3445 - val_accuracy: 0.8667\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.01414\n",
            "Epoch 59/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0574 - accuracy: 0.9850 - val_loss: 0.3978 - val_accuracy: 0.8000\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.01414\n",
            "Epoch 60/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0503 - accuracy: 0.9961 - val_loss: 0.3183 - val_accuracy: 0.8667\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.01414\n",
            "Epoch 61/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0688 - accuracy: 0.9719 - val_loss: 0.3924 - val_accuracy: 0.8000\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.01414\n",
            "Epoch 62/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0602 - accuracy: 0.9868 - val_loss: 0.4134 - val_accuracy: 0.8000\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.01414\n",
            "Epoch 63/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0573 - accuracy: 0.9983 - val_loss: 0.3719 - val_accuracy: 0.8000\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.01414\n",
            "Epoch 64/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0392 - accuracy: 0.9952 - val_loss: 0.3114 - val_accuracy: 0.8667\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.01414\n",
            "Epoch 65/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0586 - accuracy: 0.9935 - val_loss: 0.3448 - val_accuracy: 0.8667\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.01414\n",
            "Epoch 66/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0334 - accuracy: 0.9992 - val_loss: 0.3013 - val_accuracy: 0.8667\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.01414\n",
            "Epoch 67/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0788 - accuracy: 0.9625 - val_loss: 0.3475 - val_accuracy: 0.8667\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.01414\n",
            "Epoch 68/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0321 - accuracy: 0.9992 - val_loss: 0.3120 - val_accuracy: 0.8667\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.01414\n",
            "Epoch 69/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0318 - accuracy: 0.9965 - val_loss: 0.3364 - val_accuracy: 0.8667\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.01414\n",
            "Epoch 70/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0556 - accuracy: 0.9935 - val_loss: 0.2508 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.01414\n",
            "Epoch 71/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0625 - accuracy: 0.9850 - val_loss: 0.4655 - val_accuracy: 0.8000\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.01414\n",
            "Epoch 72/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0557 - accuracy: 0.9868 - val_loss: 0.5876 - val_accuracy: 0.8000\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.01414\n",
            "Epoch 73/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0417 - accuracy: 0.9862 - val_loss: 0.2377 - val_accuracy: 0.8667\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.01414\n",
            "Epoch 74/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.0565 - accuracy: 0.9921 - val_loss: 0.3234 - val_accuracy: 0.8667\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.01414\n",
            "Epoch 75/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0428 - accuracy: 0.9877 - val_loss: 0.4484 - val_accuracy: 0.8000\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.01414\n",
            "Epoch 76/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.9976 - val_loss: 0.2893 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.01414\n",
            "Epoch 77/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0340 - accuracy: 0.9894 - val_loss: 0.4687 - val_accuracy: 0.8000\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.01414\n",
            "Epoch 78/100\n",
            "27/27 [==============================] - 0s 5ms/step - loss: 0.0539 - accuracy: 0.9624 - val_loss: 0.5155 - val_accuracy: 0.8000\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.01414\n",
            "Epoch 79/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0586 - accuracy: 0.9855 - val_loss: 0.2717 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.01414\n",
            "Epoch 80/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0310 - accuracy: 0.9974 - val_loss: 0.3525 - val_accuracy: 0.8667\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.01414\n",
            "Epoch 81/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0395 - accuracy: 0.9995 - val_loss: 0.4003 - val_accuracy: 0.8000\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.01414\n",
            "Epoch 82/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0328 - accuracy: 0.9961 - val_loss: 0.3303 - val_accuracy: 0.8667\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.01414\n",
            "Epoch 83/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0423 - accuracy: 0.9956 - val_loss: 0.3645 - val_accuracy: 0.8667\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.01414\n",
            "Epoch 84/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0307 - accuracy: 0.9989 - val_loss: 0.3046 - val_accuracy: 0.8667\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.01414\n",
            "Epoch 85/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0456 - accuracy: 0.9986 - val_loss: 0.3973 - val_accuracy: 0.8667\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.01414\n",
            "Epoch 86/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0535 - accuracy: 0.9797 - val_loss: 0.4988 - val_accuracy: 0.8000\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.01414\n",
            "Epoch 87/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0244 - accuracy: 0.9980 - val_loss: 0.2991 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.01414\n",
            "Epoch 88/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0366 - accuracy: 0.9961 - val_loss: 0.2978 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.01414\n",
            "Epoch 89/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0466 - accuracy: 0.9911 - val_loss: 0.3086 - val_accuracy: 0.8667\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.01414\n",
            "Epoch 90/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0403 - accuracy: 0.9842 - val_loss: 0.3639 - val_accuracy: 0.8667\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.01414\n",
            "Epoch 91/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0376 - accuracy: 0.9868 - val_loss: 0.3742 - val_accuracy: 0.8667\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.01414\n",
            "Epoch 92/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0395 - accuracy: 0.9905 - val_loss: 0.4906 - val_accuracy: 0.8000\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.01414\n",
            "Epoch 93/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.0218 - accuracy: 0.9869 - val_loss: 0.3300 - val_accuracy: 0.8667\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.01414\n",
            "Epoch 94/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0300 - accuracy: 0.9989 - val_loss: 0.3407 - val_accuracy: 0.8667\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.01414\n",
            "Epoch 95/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0273 - accuracy: 0.9992 - val_loss: 0.5500 - val_accuracy: 0.8000\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.01414\n",
            "Epoch 96/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0413 - accuracy: 0.9866 - val_loss: 0.3571 - val_accuracy: 0.8667\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.01414\n",
            "Epoch 97/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0457 - accuracy: 0.9868 - val_loss: 0.3743 - val_accuracy: 0.8667\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.01414\n",
            "Epoch 98/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0253 - accuracy: 0.9941 - val_loss: 0.4097 - val_accuracy: 0.8667\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.01414\n",
            "Epoch 99/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0318 - accuracy: 0.9952 - val_loss: 0.3182 - val_accuracy: 0.8667\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.01414\n",
            "Epoch 100/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0363 - accuracy: 0.9894 - val_loss: 0.6317 - val_accuracy: 0.8000\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.01414\n",
            "1/1 [==============================] - 0s 297ms/step - loss: 0.6317 - accuracy: 0.8000\n",
            "\n",
            " 10 fold accuracy:  ['1.0000', '1.0000', '1.0000', '0.8667', '0.9333', '0.8000']\n",
            "Epoch 1/100\n",
            "27/27 [==============================] - 1s 16ms/step - loss: 1.1105 - accuracy: 0.3295 - val_loss: 0.8765 - val_accuracy: 0.8000\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 0.01414\n",
            "Epoch 2/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.8869 - accuracy: 0.6302 - val_loss: 0.7369 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.01414\n",
            "Epoch 3/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.7539 - accuracy: 0.6366 - val_loss: 0.5754 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.01414\n",
            "Epoch 4/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.5927 - accuracy: 0.9458 - val_loss: 0.4848 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.01414\n",
            "Epoch 5/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.5267 - accuracy: 0.9508 - val_loss: 0.4063 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.01414\n",
            "Epoch 6/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.4694 - accuracy: 0.8246 - val_loss: 0.3940 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.01414\n",
            "Epoch 7/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.4337 - accuracy: 0.8853 - val_loss: 0.3192 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.01414\n",
            "Epoch 8/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.3924 - accuracy: 0.9414 - val_loss: 0.3270 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.01414\n",
            "Epoch 9/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.3391 - accuracy: 0.9838 - val_loss: 0.2797 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.01414\n",
            "Epoch 10/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.3400 - accuracy: 0.9582 - val_loss: 0.2523 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.01414\n",
            "Epoch 11/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.3447 - accuracy: 0.8947 - val_loss: 0.2384 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.01414\n",
            "Epoch 12/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.2990 - accuracy: 0.9481 - val_loss: 0.2377 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.01414\n",
            "Epoch 13/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.2667 - accuracy: 0.9760 - val_loss: 0.2200 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.01414\n",
            "Epoch 14/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.2209 - accuracy: 0.9741 - val_loss: 0.2441 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.01414\n",
            "Epoch 15/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.2634 - accuracy: 0.9258 - val_loss: 0.1996 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.01414\n",
            "Epoch 16/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.2443 - accuracy: 0.9600 - val_loss: 0.1825 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.01414\n",
            "Epoch 17/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.2367 - accuracy: 0.9508 - val_loss: 0.1784 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.01414\n",
            "Epoch 18/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.2169 - accuracy: 0.9656 - val_loss: 0.1726 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.01414\n",
            "Epoch 19/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.2095 - accuracy: 0.9838 - val_loss: 0.1695 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.01414\n",
            "Epoch 20/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1993 - accuracy: 0.9930 - val_loss: 0.1606 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.01414\n",
            "Epoch 21/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1733 - accuracy: 0.9554 - val_loss: 0.1734 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.01414\n",
            "Epoch 22/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1508 - accuracy: 0.9935 - val_loss: 0.1556 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.01414\n",
            "Epoch 23/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1764 - accuracy: 0.9794 - val_loss: 0.1505 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.01414\n",
            "Epoch 24/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1952 - accuracy: 0.9783 - val_loss: 0.1458 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.01414\n",
            "Epoch 25/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1450 - accuracy: 0.9902 - val_loss: 0.1399 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.01414\n",
            "Epoch 26/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1383 - accuracy: 0.9777 - val_loss: 0.1356 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.01414\n",
            "Epoch 27/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1573 - accuracy: 0.9724 - val_loss: 0.1337 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.01414\n",
            "Epoch 28/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1525 - accuracy: 0.9768 - val_loss: 0.1304 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.01414\n",
            "Epoch 29/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1231 - accuracy: 0.9819 - val_loss: 0.1300 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.01414\n",
            "Epoch 30/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1123 - accuracy: 0.9934 - val_loss: 0.1296 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.01414\n",
            "Epoch 31/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1134 - accuracy: 0.9811 - val_loss: 0.1247 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.01414\n",
            "Epoch 32/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1161 - accuracy: 0.9875 - val_loss: 0.1328 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.01414\n",
            "Epoch 33/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1337 - accuracy: 0.9573 - val_loss: 0.1239 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.01414\n",
            "Epoch 34/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1241 - accuracy: 0.9863 - val_loss: 0.1230 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.01414\n",
            "Epoch 35/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1323 - accuracy: 0.9505 - val_loss: 0.1184 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.01414\n",
            "Epoch 36/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1208 - accuracy: 0.9874 - val_loss: 0.1165 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.01414\n",
            "Epoch 37/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1067 - accuracy: 0.9914 - val_loss: 0.1247 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.01414\n",
            "Epoch 38/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1074 - accuracy: 0.9786 - val_loss: 0.1145 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.01414\n",
            "Epoch 39/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1288 - accuracy: 0.9650 - val_loss: 0.1164 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.01414\n",
            "Epoch 40/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1155 - accuracy: 0.9696 - val_loss: 0.1133 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.01414\n",
            "Epoch 41/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0929 - accuracy: 0.9921 - val_loss: 0.1174 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.01414\n",
            "Epoch 42/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0880 - accuracy: 0.9968 - val_loss: 0.1152 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.01414\n",
            "Epoch 43/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1093 - accuracy: 0.9705 - val_loss: 0.1117 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.01414\n",
            "Epoch 44/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1031 - accuracy: 0.9935 - val_loss: 0.1193 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.01414\n",
            "Epoch 45/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1050 - accuracy: 0.9918 - val_loss: 0.1078 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.01414\n",
            "Epoch 46/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1271 - accuracy: 0.9568 - val_loss: 0.1142 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.01414\n",
            "Epoch 47/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1203 - accuracy: 0.9365 - val_loss: 0.1171 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.01414\n",
            "Epoch 48/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0932 - accuracy: 0.9810 - val_loss: 0.1065 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.01414\n",
            "Epoch 49/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0992 - accuracy: 0.9731 - val_loss: 0.1052 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.01414\n",
            "Epoch 50/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0850 - accuracy: 0.9813 - val_loss: 0.1130 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.01414\n",
            "Epoch 51/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1014 - accuracy: 0.9447 - val_loss: 0.1124 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.01414\n",
            "Epoch 52/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0804 - accuracy: 0.9848 - val_loss: 0.1098 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.01414\n",
            "Epoch 53/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0997 - accuracy: 0.9695 - val_loss: 0.1051 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.01414\n",
            "Epoch 54/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1165 - accuracy: 0.9509 - val_loss: 0.1070 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.01414\n",
            "Epoch 55/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0873 - accuracy: 0.9644 - val_loss: 0.1091 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.01414\n",
            "Epoch 56/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0733 - accuracy: 0.9869 - val_loss: 0.1111 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.01414\n",
            "Epoch 57/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0823 - accuracy: 0.9881 - val_loss: 0.1013 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.01414\n",
            "Epoch 58/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0527 - accuracy: 0.9848 - val_loss: 0.1174 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.01414\n",
            "Epoch 59/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1255 - accuracy: 0.9698 - val_loss: 0.1029 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.01414\n",
            "Epoch 60/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0648 - accuracy: 0.9850 - val_loss: 0.1174 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.01414\n",
            "Epoch 61/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0755 - accuracy: 0.9938 - val_loss: 0.1029 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.01414\n",
            "Epoch 62/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0936 - accuracy: 0.9549 - val_loss: 0.1032 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.01414\n",
            "Epoch 63/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0734 - accuracy: 0.9686 - val_loss: 0.1009 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.01414\n",
            "Epoch 64/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0558 - accuracy: 0.9830 - val_loss: 0.1003 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.01414\n",
            "Epoch 65/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0686 - accuracy: 0.9840 - val_loss: 0.0989 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.01414\n",
            "Epoch 66/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0666 - accuracy: 0.9842 - val_loss: 0.1023 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.01414\n",
            "Epoch 67/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0737 - accuracy: 0.9845 - val_loss: 0.1189 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.01414\n",
            "Epoch 68/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0894 - accuracy: 0.9770 - val_loss: 0.0981 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.01414\n",
            "Epoch 69/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0842 - accuracy: 0.9788 - val_loss: 0.1015 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.01414\n",
            "Epoch 70/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1270 - accuracy: 0.9606 - val_loss: 0.0976 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.01414\n",
            "Epoch 71/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0463 - accuracy: 0.9910 - val_loss: 0.0965 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.01414\n",
            "Epoch 72/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0855 - accuracy: 0.9630 - val_loss: 0.1269 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.01414\n",
            "Epoch 73/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0813 - accuracy: 0.9692 - val_loss: 0.0990 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.01414\n",
            "Epoch 74/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0710 - accuracy: 0.9763 - val_loss: 0.1237 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.01414\n",
            "Epoch 75/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0844 - accuracy: 0.9700 - val_loss: 0.1126 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.01414\n",
            "Epoch 76/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0721 - accuracy: 0.9767 - val_loss: 0.0961 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.01414\n",
            "Epoch 77/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0627 - accuracy: 0.9875 - val_loss: 0.1090 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.01414\n",
            "Epoch 78/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0984 - accuracy: 0.9797 - val_loss: 0.0948 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.01414\n",
            "Epoch 79/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0661 - accuracy: 0.9863 - val_loss: 0.0952 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.01414\n",
            "Epoch 80/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0775 - accuracy: 0.9876 - val_loss: 0.0954 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.01414\n",
            "Epoch 81/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0643 - accuracy: 0.9862 - val_loss: 0.1035 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.01414\n",
            "Epoch 82/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0392 - accuracy: 0.9951 - val_loss: 0.0973 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.01414\n",
            "Epoch 83/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0550 - accuracy: 0.9921 - val_loss: 0.1025 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.01414\n",
            "Epoch 84/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0479 - accuracy: 0.9981 - val_loss: 0.0933 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.01414\n",
            "Epoch 85/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0613 - accuracy: 0.9937 - val_loss: 0.0976 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.01414\n",
            "Epoch 86/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1125 - accuracy: 0.9487 - val_loss: 0.0993 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.01414\n",
            "Epoch 87/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0355 - accuracy: 0.9956 - val_loss: 0.0928 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.01414\n",
            "Epoch 88/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0764 - accuracy: 0.9746 - val_loss: 0.1079 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.01414\n",
            "Epoch 89/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0529 - accuracy: 0.9784 - val_loss: 0.0924 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.01414\n",
            "Epoch 90/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0456 - accuracy: 0.9951 - val_loss: 0.1005 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.01414\n",
            "Epoch 91/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0765 - accuracy: 0.9661 - val_loss: 0.0921 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.01414\n",
            "Epoch 92/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0736 - accuracy: 0.9790 - val_loss: 0.0925 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.01414\n",
            "Epoch 93/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0725 - accuracy: 0.9892 - val_loss: 0.1051 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.01414\n",
            "Epoch 94/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0612 - accuracy: 0.9718 - val_loss: 0.0914 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.01414\n",
            "Epoch 95/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0677 - accuracy: 0.9905 - val_loss: 0.0913 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.01414\n",
            "Epoch 96/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0653 - accuracy: 0.9876 - val_loss: 0.0915 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.01414\n",
            "Epoch 97/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0588 - accuracy: 0.9804 - val_loss: 0.0950 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.01414\n",
            "Epoch 98/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0689 - accuracy: 0.9777 - val_loss: 0.1023 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.01414\n",
            "Epoch 99/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0869 - accuracy: 0.9620 - val_loss: 0.0910 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.01414\n",
            "Epoch 100/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0752 - accuracy: 0.9723 - val_loss: 0.0945 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.01414\n",
            "1/1 [==============================] - 0s 267ms/step - loss: 0.0945 - accuracy: 0.9333\n",
            "\n",
            " 10 fold accuracy:  ['1.0000', '1.0000', '1.0000', '0.8667', '0.9333', '0.8000', '0.9333']\n",
            "Epoch 1/100\n",
            "27/27 [==============================] - 1s 14ms/step - loss: 1.0019 - accuracy: 0.4432 - val_loss: 0.8581 - val_accuracy: 0.6000\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 0.01414\n",
            "Epoch 2/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.7686 - accuracy: 0.7307 - val_loss: 0.7570 - val_accuracy: 0.6000\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.01414\n",
            "Epoch 3/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.7281 - accuracy: 0.7611 - val_loss: 0.6956 - val_accuracy: 0.6667\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.01414\n",
            "Epoch 4/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.6731 - accuracy: 0.8250 - val_loss: 0.6399 - val_accuracy: 0.7333\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.01414\n",
            "Epoch 5/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.5950 - accuracy: 0.8222 - val_loss: 0.5833 - val_accuracy: 0.8667\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.01414\n",
            "Epoch 6/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.5759 - accuracy: 0.8201 - val_loss: 0.5422 - val_accuracy: 0.8667\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.01414\n",
            "Epoch 7/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.5242 - accuracy: 0.8906 - val_loss: 0.5037 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.01414\n",
            "Epoch 8/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.5064 - accuracy: 0.8496 - val_loss: 0.4752 - val_accuracy: 0.8667\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.01414\n",
            "Epoch 9/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.4291 - accuracy: 0.8563 - val_loss: 0.4425 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.01414\n",
            "Epoch 10/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.4281 - accuracy: 0.9513 - val_loss: 0.4165 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.01414\n",
            "Epoch 11/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.4127 - accuracy: 0.9253 - val_loss: 0.3963 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.01414\n",
            "Epoch 12/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.3914 - accuracy: 0.9482 - val_loss: 0.3777 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.01414\n",
            "Epoch 13/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.3455 - accuracy: 0.9712 - val_loss: 0.3613 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.01414\n",
            "Epoch 14/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.3807 - accuracy: 0.9571 - val_loss: 0.3551 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.01414\n",
            "Epoch 15/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.3237 - accuracy: 0.9309 - val_loss: 0.3291 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.01414\n",
            "Epoch 16/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.3530 - accuracy: 0.9258 - val_loss: 0.3177 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.01414\n",
            "Epoch 17/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.3367 - accuracy: 0.9313 - val_loss: 0.3003 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.01414\n",
            "Epoch 18/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.3302 - accuracy: 0.9019 - val_loss: 0.2928 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.01414\n",
            "Epoch 19/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.2763 - accuracy: 0.9787 - val_loss: 0.2723 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.01414\n",
            "Epoch 20/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.2675 - accuracy: 0.9664 - val_loss: 0.2611 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.01414\n",
            "Epoch 21/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.2649 - accuracy: 0.9662 - val_loss: 0.2667 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.01414\n",
            "Epoch 22/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.2692 - accuracy: 0.9281 - val_loss: 0.2440 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.01414\n",
            "Epoch 23/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.2749 - accuracy: 0.9608 - val_loss: 0.2520 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.01414\n",
            "Epoch 24/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.2506 - accuracy: 0.9768 - val_loss: 0.2219 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.01414\n",
            "Epoch 25/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.2236 - accuracy: 0.9709 - val_loss: 0.2183 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.01414\n",
            "Epoch 26/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.2113 - accuracy: 0.9843 - val_loss: 0.1990 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.01414\n",
            "Epoch 27/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.2179 - accuracy: 0.9709 - val_loss: 0.1906 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.01414\n",
            "Epoch 28/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.2447 - accuracy: 0.9333 - val_loss: 0.1900 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.01414\n",
            "Epoch 29/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.2265 - accuracy: 0.9572 - val_loss: 0.1719 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.01414\n",
            "Epoch 30/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1914 - accuracy: 0.9730 - val_loss: 0.1713 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.01414\n",
            "Epoch 31/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.2003 - accuracy: 0.9813 - val_loss: 0.1722 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.01414\n",
            "Epoch 32/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.2124 - accuracy: 0.9541 - val_loss: 0.1534 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.01414\n",
            "Epoch 33/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1848 - accuracy: 0.9808 - val_loss: 0.1420 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.01414\n",
            "Epoch 34/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1795 - accuracy: 0.9618 - val_loss: 0.1402 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.01414\n",
            "Epoch 35/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1795 - accuracy: 0.9896 - val_loss: 0.1309 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.01414\n",
            "Epoch 36/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1614 - accuracy: 0.9523 - val_loss: 0.1422 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.01414\n",
            "Epoch 37/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1491 - accuracy: 0.9772 - val_loss: 0.1245 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.01414\n",
            "Epoch 38/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1479 - accuracy: 0.9855 - val_loss: 0.1145 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.01414\n",
            "Epoch 39/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1489 - accuracy: 0.9755 - val_loss: 0.1333 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.01414\n",
            "Epoch 40/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1434 - accuracy: 0.9875 - val_loss: 0.1154 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.01414\n",
            "Epoch 41/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1408 - accuracy: 0.9559 - val_loss: 0.1392 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.01414\n",
            "Epoch 42/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1558 - accuracy: 0.9764 - val_loss: 0.1083 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.01414\n",
            "Epoch 43/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1336 - accuracy: 0.9634 - val_loss: 0.0946 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.01414\n",
            "Epoch 44/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1466 - accuracy: 0.9636 - val_loss: 0.1264 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.01414\n",
            "Epoch 45/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1623 - accuracy: 0.9613 - val_loss: 0.0901 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.01414\n",
            "Epoch 46/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1306 - accuracy: 0.9687 - val_loss: 0.0904 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.01414\n",
            "Epoch 47/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1245 - accuracy: 0.9890 - val_loss: 0.0802 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.01414\n",
            "Epoch 48/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1385 - accuracy: 0.9527 - val_loss: 0.0893 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.01414\n",
            "Epoch 49/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1300 - accuracy: 0.9711 - val_loss: 0.0910 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.01414\n",
            "Epoch 50/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1148 - accuracy: 0.9765 - val_loss: 0.0705 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.01414\n",
            "Epoch 51/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1257 - accuracy: 0.9663 - val_loss: 0.0824 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.01414\n",
            "Epoch 52/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1285 - accuracy: 0.9680 - val_loss: 0.0713 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.01414\n",
            "Epoch 53/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1344 - accuracy: 0.9617 - val_loss: 0.0714 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.01414\n",
            "Epoch 54/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0946 - accuracy: 0.9856 - val_loss: 0.0632 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.01414\n",
            "Epoch 55/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1150 - accuracy: 0.9742 - val_loss: 0.0600 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.01414\n",
            "Epoch 56/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1623 - accuracy: 0.9235 - val_loss: 0.0872 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.01414\n",
            "Epoch 57/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1149 - accuracy: 0.9861 - val_loss: 0.0585 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.01414\n",
            "Epoch 58/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1203 - accuracy: 0.9676 - val_loss: 0.0554 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.01414\n",
            "Epoch 59/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1497 - accuracy: 0.9318 - val_loss: 0.0642 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.01414\n",
            "Epoch 60/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1044 - accuracy: 0.9697 - val_loss: 0.0530 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.01414\n",
            "Epoch 61/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0970 - accuracy: 0.9629 - val_loss: 0.0488 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.01414\n",
            "Epoch 62/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1131 - accuracy: 0.9706 - val_loss: 0.0901 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.01414\n",
            "Epoch 63/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1296 - accuracy: 0.9471 - val_loss: 0.0463 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.01414\n",
            "Epoch 64/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0833 - accuracy: 0.9749 - val_loss: 0.0524 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.01414\n",
            "Epoch 65/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1117 - accuracy: 0.9729 - val_loss: 0.0442 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.01414\n",
            "Epoch 66/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1044 - accuracy: 0.9549 - val_loss: 0.0496 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.01414\n",
            "Epoch 67/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0839 - accuracy: 0.9836 - val_loss: 0.0452 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.01414\n",
            "Epoch 68/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0954 - accuracy: 0.9787 - val_loss: 0.0427 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.01414\n",
            "Epoch 69/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1342 - accuracy: 0.9339 - val_loss: 0.0534 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.01414\n",
            "Epoch 70/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0993 - accuracy: 0.9768 - val_loss: 0.0405 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.01414\n",
            "Epoch 71/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1236 - accuracy: 0.9441 - val_loss: 0.0544 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.01414\n",
            "Epoch 72/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1335 - accuracy: 0.9384 - val_loss: 0.0364 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.01414\n",
            "Epoch 73/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0698 - accuracy: 0.9832 - val_loss: 0.0363 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.01414\n",
            "Epoch 74/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0800 - accuracy: 0.9742 - val_loss: 0.0391 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.01414\n",
            "Epoch 75/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0896 - accuracy: 0.9918 - val_loss: 0.0330 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.01414\n",
            "Epoch 76/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1087 - accuracy: 0.9708 - val_loss: 0.0759 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.01414\n",
            "Epoch 77/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0784 - accuracy: 0.9784 - val_loss: 0.0317 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.01414\n",
            "Epoch 78/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0924 - accuracy: 0.9926 - val_loss: 0.0379 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.01414\n",
            "Epoch 79/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0665 - accuracy: 0.9872 - val_loss: 0.0303 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.01414\n",
            "Epoch 80/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0667 - accuracy: 0.9916 - val_loss: 0.0398 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.01414\n",
            "Epoch 81/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1188 - accuracy: 0.9530 - val_loss: 0.0319 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.01414\n",
            "Epoch 82/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0622 - accuracy: 0.9815 - val_loss: 0.0300 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.01414\n",
            "Epoch 83/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1013 - accuracy: 0.9578 - val_loss: 0.0353 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.01414\n",
            "Epoch 84/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1031 - accuracy: 0.9741 - val_loss: 0.0264 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.01414\n",
            "Epoch 85/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0771 - accuracy: 0.9609 - val_loss: 0.0346 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.01414\n",
            "Epoch 86/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0922 - accuracy: 0.9726 - val_loss: 0.0253 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.01414\n",
            "Epoch 87/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0923 - accuracy: 0.9665 - val_loss: 0.0260 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.01414\n",
            "Epoch 88/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0855 - accuracy: 0.9717 - val_loss: 0.0243 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.01414\n",
            "Epoch 89/100\n",
            "27/27 [==============================] - 0s 5ms/step - loss: 0.0905 - accuracy: 0.9761 - val_loss: 0.0264 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.01414\n",
            "Epoch 90/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0892 - accuracy: 0.9751 - val_loss: 0.0252 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.01414\n",
            "Epoch 91/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1051 - accuracy: 0.9611 - val_loss: 0.0262 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.01414\n",
            "Epoch 92/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0728 - accuracy: 0.9727 - val_loss: 0.0244 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.01414\n",
            "Epoch 93/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1057 - accuracy: 0.9465 - val_loss: 0.0265 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.01414\n",
            "Epoch 94/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0709 - accuracy: 0.9731 - val_loss: 0.0255 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.01414\n",
            "Epoch 95/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0882 - accuracy: 0.9725 - val_loss: 0.0223 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.01414\n",
            "Epoch 96/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0903 - accuracy: 0.9587 - val_loss: 0.0289 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.01414\n",
            "Epoch 97/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1138 - accuracy: 0.9521 - val_loss: 0.0208 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.01414\n",
            "Epoch 98/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0882 - accuracy: 0.9621 - val_loss: 0.0202 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.01414\n",
            "Epoch 99/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0665 - accuracy: 0.9927 - val_loss: 0.0222 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.01414\n",
            "Epoch 100/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0579 - accuracy: 0.9857 - val_loss: 0.0212 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.01414\n",
            "1/1 [==============================] - 0s 278ms/step - loss: 0.0212 - accuracy: 1.0000\n",
            "\n",
            " 10 fold accuracy:  ['1.0000', '1.0000', '1.0000', '0.8667', '0.9333', '0.8000', '0.9333', '1.0000']\n",
            "Epoch 1/100\n",
            "27/27 [==============================] - 1s 16ms/step - loss: 2.1613 - accuracy: 0.4683 - val_loss: 2.1440 - val_accuracy: 0.5333\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 0.01414\n",
            "Epoch 2/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 1.6211 - accuracy: 0.6561 - val_loss: 1.5485 - val_accuracy: 0.5333\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.01414\n",
            "Epoch 3/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 1.0916 - accuracy: 0.7072 - val_loss: 1.1830 - val_accuracy: 0.5333\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.01414\n",
            "Epoch 4/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.9004 - accuracy: 0.6997 - val_loss: 0.9490 - val_accuracy: 0.5333\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.01414\n",
            "Epoch 5/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.7804 - accuracy: 0.6770 - val_loss: 0.7856 - val_accuracy: 0.5333\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.01414\n",
            "Epoch 6/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.6760 - accuracy: 0.6688 - val_loss: 0.6605 - val_accuracy: 0.7333\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.01414\n",
            "Epoch 7/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.5719 - accuracy: 0.7982 - val_loss: 0.5806 - val_accuracy: 0.8667\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.01414\n",
            "Epoch 8/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.4889 - accuracy: 0.9110 - val_loss: 0.5075 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.01414\n",
            "Epoch 9/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.4441 - accuracy: 0.9467 - val_loss: 0.4558 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.01414\n",
            "Epoch 10/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.4228 - accuracy: 0.9044 - val_loss: 0.3973 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.01414\n",
            "Epoch 11/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.3584 - accuracy: 0.9320 - val_loss: 0.3570 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.01414\n",
            "Epoch 12/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.3448 - accuracy: 0.9726 - val_loss: 0.3126 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.01414\n",
            "Epoch 13/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.2855 - accuracy: 0.9806 - val_loss: 0.2790 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.01414\n",
            "Epoch 14/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.2860 - accuracy: 0.9816 - val_loss: 0.2518 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.01414\n",
            "Epoch 15/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.2562 - accuracy: 0.9607 - val_loss: 0.2254 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.01414\n",
            "Epoch 16/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.2161 - accuracy: 0.9950 - val_loss: 0.2044 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.01414\n",
            "Epoch 17/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.2466 - accuracy: 0.9676 - val_loss: 0.1829 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.01414\n",
            "Epoch 18/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.2132 - accuracy: 0.9588 - val_loss: 0.1774 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.01414\n",
            "Epoch 19/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.2075 - accuracy: 0.9564 - val_loss: 0.1611 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.01414\n",
            "Epoch 20/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1892 - accuracy: 0.9636 - val_loss: 0.1378 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.01414\n",
            "Epoch 21/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1742 - accuracy: 0.9750 - val_loss: 0.1257 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.01414\n",
            "Epoch 22/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1666 - accuracy: 0.9881 - val_loss: 0.1158 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.01414\n",
            "Epoch 23/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1553 - accuracy: 0.9952 - val_loss: 0.1107 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.01414\n",
            "Epoch 24/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1500 - accuracy: 0.9797 - val_loss: 0.1200 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.01414\n",
            "Epoch 25/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1600 - accuracy: 0.9365 - val_loss: 0.1009 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.01414\n",
            "Epoch 26/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1417 - accuracy: 0.9570 - val_loss: 0.0861 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.01414\n",
            "Epoch 27/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1790 - accuracy: 0.9384 - val_loss: 0.1156 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.01414\n",
            "Epoch 28/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.2098 - accuracy: 0.9168 - val_loss: 0.1118 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.01414\n",
            "Epoch 29/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1548 - accuracy: 0.9504 - val_loss: 0.0713 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.01414\n",
            "Epoch 30/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1292 - accuracy: 0.9739 - val_loss: 0.0687 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.01414\n",
            "Epoch 31/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1098 - accuracy: 0.9739 - val_loss: 0.0676 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.01414\n",
            "Epoch 32/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1347 - accuracy: 0.9550 - val_loss: 0.0591 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.01414\n",
            "Epoch 33/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1057 - accuracy: 0.9748 - val_loss: 0.0569 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.01414\n",
            "Epoch 34/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1277 - accuracy: 0.9558 - val_loss: 0.0540 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.01414\n",
            "Epoch 35/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1431 - accuracy: 0.9517 - val_loss: 0.0506 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.01414\n",
            "Epoch 36/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1277 - accuracy: 0.9678 - val_loss: 0.0496 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.01414\n",
            "Epoch 37/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1039 - accuracy: 0.9723 - val_loss: 0.0467 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.01414\n",
            "Epoch 38/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1260 - accuracy: 0.9413 - val_loss: 0.0502 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.01414\n",
            "Epoch 39/100\n",
            "27/27 [==============================] - 0s 5ms/step - loss: 0.0884 - accuracy: 0.9885 - val_loss: 0.0425 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.01414\n",
            "Epoch 40/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1100 - accuracy: 0.9306 - val_loss: 0.0445 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.01414\n",
            "Epoch 41/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1531 - accuracy: 0.9436 - val_loss: 0.0396 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.01414\n",
            "Epoch 42/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1093 - accuracy: 0.9635 - val_loss: 0.0369 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.01414\n",
            "Epoch 43/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1152 - accuracy: 0.9546 - val_loss: 0.0355 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.01414\n",
            "Epoch 44/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0739 - accuracy: 0.9727 - val_loss: 0.0350 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.01414\n",
            "Epoch 45/100\n",
            "27/27 [==============================] - 0s 5ms/step - loss: 0.1122 - accuracy: 0.9588 - val_loss: 0.0399 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.01414\n",
            "Epoch 46/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0761 - accuracy: 0.9731 - val_loss: 0.0402 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.01414\n",
            "Epoch 47/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0986 - accuracy: 0.9600 - val_loss: 0.0305 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.01414\n",
            "Epoch 48/100\n",
            "27/27 [==============================] - 0s 5ms/step - loss: 0.1433 - accuracy: 0.9213 - val_loss: 0.0388 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.01414\n",
            "Epoch 49/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0753 - accuracy: 0.9643 - val_loss: 0.0287 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.01414\n",
            "Epoch 50/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1164 - accuracy: 0.9572 - val_loss: 0.0293 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.01414\n",
            "Epoch 51/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0681 - accuracy: 0.9701 - val_loss: 0.0276 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.01414\n",
            "Epoch 52/100\n",
            "27/27 [==============================] - 0s 5ms/step - loss: 0.0812 - accuracy: 0.9688 - val_loss: 0.0261 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.01414\n",
            "Epoch 53/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0810 - accuracy: 0.9755 - val_loss: 0.0255 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.01414\n",
            "Epoch 54/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0569 - accuracy: 0.9897 - val_loss: 0.0247 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.01414\n",
            "Epoch 55/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0819 - accuracy: 0.9752 - val_loss: 0.0232 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.01414\n",
            "Epoch 56/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0948 - accuracy: 0.9650 - val_loss: 0.0236 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.01414\n",
            "Epoch 57/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0864 - accuracy: 0.9681 - val_loss: 0.0218 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.01414\n",
            "Epoch 58/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0780 - accuracy: 0.9515 - val_loss: 0.0211 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.01414\n",
            "Epoch 59/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0729 - accuracy: 0.9671 - val_loss: 0.0205 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.01414\n",
            "Epoch 60/100\n",
            "27/27 [==============================] - 0s 5ms/step - loss: 0.0773 - accuracy: 0.9678 - val_loss: 0.0198 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.01414\n",
            "Epoch 61/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0512 - accuracy: 0.9837 - val_loss: 0.0193 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.01414\n",
            "Epoch 62/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0759 - accuracy: 0.9811 - val_loss: 0.0211 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.01414\n",
            "Epoch 63/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1037 - accuracy: 0.9809 - val_loss: 0.0268 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.01414\n",
            "Epoch 64/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1021 - accuracy: 0.9696 - val_loss: 0.0336 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.01414\n",
            "Epoch 65/100\n",
            "27/27 [==============================] - 0s 5ms/step - loss: 0.0738 - accuracy: 0.9770 - val_loss: 0.0178 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.01414\n",
            "Epoch 66/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0816 - accuracy: 0.9688 - val_loss: 0.0170 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.01414\n",
            "Epoch 67/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0844 - accuracy: 0.9606 - val_loss: 0.0169 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.01414\n",
            "Epoch 68/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1031 - accuracy: 0.9496 - val_loss: 0.0164 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.01414\n",
            "Epoch 69/100\n",
            "27/27 [==============================] - 0s 5ms/step - loss: 0.0836 - accuracy: 0.9710 - val_loss: 0.0206 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.01414\n",
            "Epoch 70/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0589 - accuracy: 0.9807 - val_loss: 0.0189 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.01414\n",
            "Epoch 71/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1257 - accuracy: 0.9574 - val_loss: 0.0150 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.01414\n",
            "Epoch 72/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0628 - accuracy: 0.9874 - val_loss: 0.0171 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.01414\n",
            "Epoch 73/100\n",
            "27/27 [==============================] - 0s 5ms/step - loss: 0.0645 - accuracy: 0.9700 - val_loss: 0.0159 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.01414\n",
            "Epoch 74/100\n",
            "27/27 [==============================] - 0s 5ms/step - loss: 0.0841 - accuracy: 0.9805 - val_loss: 0.0146 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.01414\n",
            "Epoch 75/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0665 - accuracy: 0.9787 - val_loss: 0.0184 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.01414\n",
            "Epoch 76/100\n",
            "27/27 [==============================] - 0s 5ms/step - loss: 0.0955 - accuracy: 0.9748 - val_loss: 0.0134 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00076: val_loss improved from 0.01414 to 0.01339, saving model to ./model/76-0.0134.hdf5\n",
            "Epoch 77/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0651 - accuracy: 0.9855 - val_loss: 0.0187 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.01339\n",
            "Epoch 78/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0695 - accuracy: 0.9825 - val_loss: 0.0148 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.01339\n",
            "Epoch 79/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0574 - accuracy: 0.9765 - val_loss: 0.0182 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.01339\n",
            "Epoch 80/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0824 - accuracy: 0.9579 - val_loss: 0.0125 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00080: val_loss improved from 0.01339 to 0.01249, saving model to ./model/80-0.0125.hdf5\n",
            "Epoch 81/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0726 - accuracy: 0.9696 - val_loss: 0.0128 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.01249\n",
            "Epoch 82/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0692 - accuracy: 0.9854 - val_loss: 0.0195 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.01249\n",
            "Epoch 83/100\n",
            "27/27 [==============================] - 0s 5ms/step - loss: 0.0618 - accuracy: 0.9708 - val_loss: 0.0119 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00083: val_loss improved from 0.01249 to 0.01195, saving model to ./model/83-0.0119.hdf5\n",
            "Epoch 84/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0579 - accuracy: 0.9847 - val_loss: 0.0120 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.01195\n",
            "Epoch 85/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0880 - accuracy: 0.9676 - val_loss: 0.0115 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00085: val_loss improved from 0.01195 to 0.01150, saving model to ./model/85-0.0115.hdf5\n",
            "Epoch 86/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0411 - accuracy: 0.9857 - val_loss: 0.0109 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00086: val_loss improved from 0.01150 to 0.01092, saving model to ./model/86-0.0109.hdf5\n",
            "Epoch 87/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0371 - accuracy: 0.9920 - val_loss: 0.0108 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00087: val_loss improved from 0.01092 to 0.01081, saving model to ./model/87-0.0108.hdf5\n",
            "Epoch 88/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0900 - accuracy: 0.9700 - val_loss: 0.0110 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.01081\n",
            "Epoch 89/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0562 - accuracy: 0.9795 - val_loss: 0.0105 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00089: val_loss improved from 0.01081 to 0.01048, saving model to ./model/89-0.0105.hdf5\n",
            "Epoch 90/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0607 - accuracy: 0.9681 - val_loss: 0.0227 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.01048\n",
            "Epoch 91/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0837 - accuracy: 0.9638 - val_loss: 0.0141 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.01048\n",
            "Epoch 92/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1659 - accuracy: 0.9564 - val_loss: 0.0101 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00092: val_loss improved from 0.01048 to 0.01008, saving model to ./model/92-0.0101.hdf5\n",
            "Epoch 93/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0829 - accuracy: 0.9630 - val_loss: 0.0100 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00093: val_loss improved from 0.01008 to 0.00995, saving model to ./model/93-0.0100.hdf5\n",
            "Epoch 94/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0756 - accuracy: 0.9614 - val_loss: 0.0098 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00094: val_loss improved from 0.00995 to 0.00976, saving model to ./model/94-0.0098.hdf5\n",
            "Epoch 95/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0978 - accuracy: 0.9564 - val_loss: 0.0100 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.00976\n",
            "Epoch 96/100\n",
            "27/27 [==============================] - 0s 5ms/step - loss: 0.0511 - accuracy: 0.9892 - val_loss: 0.0103 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.00976\n",
            "Epoch 97/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0462 - accuracy: 0.9933 - val_loss: 0.0157 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.00976\n",
            "Epoch 98/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0795 - accuracy: 0.9600 - val_loss: 0.0097 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00098: val_loss improved from 0.00976 to 0.00965, saving model to ./model/98-0.0097.hdf5\n",
            "Epoch 99/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0525 - accuracy: 0.9900 - val_loss: 0.0102 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.00965\n",
            "Epoch 100/100\n",
            "27/27 [==============================] - 0s 5ms/step - loss: 0.0655 - accuracy: 0.9806 - val_loss: 0.0090 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00100: val_loss improved from 0.00965 to 0.00899, saving model to ./model/100-0.0090.hdf5\n",
            "1/1 [==============================] - 0s 291ms/step - loss: 0.0090 - accuracy: 1.0000\n",
            "\n",
            " 10 fold accuracy:  ['1.0000', '1.0000', '1.0000', '0.8667', '0.9333', '0.8000', '0.9333', '1.0000', '1.0000']\n",
            "Epoch 1/100\n",
            "27/27 [==============================] - 1s 15ms/step - loss: 1.3319 - accuracy: 0.3310 - val_loss: 0.8885 - val_accuracy: 0.5333\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 0.00899\n",
            "Epoch 2/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.8580 - accuracy: 0.5311 - val_loss: 0.7271 - val_accuracy: 0.8000\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.00899\n",
            "Epoch 3/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.7273 - accuracy: 0.7926 - val_loss: 0.6731 - val_accuracy: 0.8000\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.00899\n",
            "Epoch 4/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.6010 - accuracy: 0.7534 - val_loss: 0.5912 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.00899\n",
            "Epoch 5/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.5318 - accuracy: 0.9651 - val_loss: 0.5533 - val_accuracy: 0.8000\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.00899\n",
            "Epoch 6/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.5067 - accuracy: 0.7724 - val_loss: 0.5089 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.00899\n",
            "Epoch 7/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.4473 - accuracy: 0.9758 - val_loss: 0.4978 - val_accuracy: 0.8000\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00899\n",
            "Epoch 8/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.4224 - accuracy: 0.9078 - val_loss: 0.4739 - val_accuracy: 0.8667\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00899\n",
            "Epoch 9/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.4252 - accuracy: 0.8405 - val_loss: 0.4369 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00899\n",
            "Epoch 10/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.3596 - accuracy: 0.9367 - val_loss: 0.4337 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00899\n",
            "Epoch 11/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.3850 - accuracy: 0.9519 - val_loss: 0.4091 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00899\n",
            "Epoch 12/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.3934 - accuracy: 0.9689 - val_loss: 0.4161 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.00899\n",
            "Epoch 13/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.3590 - accuracy: 0.8417 - val_loss: 0.3860 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.00899\n",
            "Epoch 14/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.3435 - accuracy: 0.9356 - val_loss: 0.3746 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.00899\n",
            "Epoch 15/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.3564 - accuracy: 0.9362 - val_loss: 0.4028 - val_accuracy: 0.8000\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.00899\n",
            "Epoch 16/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.3608 - accuracy: 0.8626 - val_loss: 0.3507 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.00899\n",
            "Epoch 17/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.3197 - accuracy: 0.9785 - val_loss: 0.3606 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.00899\n",
            "Epoch 18/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.3052 - accuracy: 0.9503 - val_loss: 0.3526 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.00899\n",
            "Epoch 19/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.3246 - accuracy: 0.9701 - val_loss: 0.3189 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.00899\n",
            "Epoch 20/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.3108 - accuracy: 0.9667 - val_loss: 0.3335 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.00899\n",
            "Epoch 21/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.2597 - accuracy: 0.9795 - val_loss: 0.3012 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.00899\n",
            "Epoch 22/100\n",
            "27/27 [==============================] - 0s 5ms/step - loss: 0.2828 - accuracy: 0.9582 - val_loss: 0.3145 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.00899\n",
            "Epoch 23/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.2622 - accuracy: 0.9564 - val_loss: 0.2832 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.00899\n",
            "Epoch 24/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.2769 - accuracy: 0.9251 - val_loss: 0.3096 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.00899\n",
            "Epoch 25/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.2812 - accuracy: 0.9527 - val_loss: 0.2671 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.00899\n",
            "Epoch 26/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.2282 - accuracy: 0.9777 - val_loss: 0.2651 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.00899\n",
            "Epoch 27/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.2213 - accuracy: 0.9600 - val_loss: 0.2748 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.00899\n",
            "Epoch 28/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.2660 - accuracy: 0.9228 - val_loss: 0.2486 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.00899\n",
            "Epoch 29/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.2072 - accuracy: 0.9706 - val_loss: 0.2370 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.00899\n",
            "Epoch 30/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.2161 - accuracy: 0.9807 - val_loss: 0.2309 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.00899\n",
            "Epoch 31/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.2101 - accuracy: 0.9794 - val_loss: 0.2555 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.00899\n",
            "Epoch 32/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.2304 - accuracy: 0.9532 - val_loss: 0.2226 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.00899\n",
            "Epoch 33/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.2153 - accuracy: 0.9906 - val_loss: 0.2091 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.00899\n",
            "Epoch 34/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1956 - accuracy: 0.9622 - val_loss: 0.2295 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.00899\n",
            "Epoch 35/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1967 - accuracy: 0.9553 - val_loss: 0.2076 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.00899\n",
            "Epoch 36/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1754 - accuracy: 0.9812 - val_loss: 0.1939 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.00899\n",
            "Epoch 37/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1793 - accuracy: 0.9869 - val_loss: 0.1898 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.00899\n",
            "Epoch 38/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1631 - accuracy: 0.9716 - val_loss: 0.1850 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.00899\n",
            "Epoch 39/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1631 - accuracy: 0.9606 - val_loss: 0.1822 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.00899\n",
            "Epoch 40/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1448 - accuracy: 0.9815 - val_loss: 0.1775 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.00899\n",
            "Epoch 41/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1673 - accuracy: 0.9630 - val_loss: 0.1782 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.00899\n",
            "Epoch 42/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1561 - accuracy: 0.9651 - val_loss: 0.1648 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.00899\n",
            "Epoch 43/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1362 - accuracy: 0.9797 - val_loss: 0.1685 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.00899\n",
            "Epoch 44/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1350 - accuracy: 0.9935 - val_loss: 0.1563 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.00899\n",
            "Epoch 45/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1821 - accuracy: 0.9321 - val_loss: 0.1851 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.00899\n",
            "Epoch 46/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1583 - accuracy: 0.9539 - val_loss: 0.1484 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.00899\n",
            "Epoch 47/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1195 - accuracy: 0.9888 - val_loss: 0.1450 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.00899\n",
            "Epoch 48/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1391 - accuracy: 0.9846 - val_loss: 0.1420 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.00899\n",
            "Epoch 49/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1380 - accuracy: 0.9684 - val_loss: 0.1395 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.00899\n",
            "Epoch 50/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1521 - accuracy: 0.9593 - val_loss: 0.1352 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.00899\n",
            "Epoch 51/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1579 - accuracy: 0.9382 - val_loss: 0.1497 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.00899\n",
            "Epoch 52/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1670 - accuracy: 0.9345 - val_loss: 0.1317 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.00899\n",
            "Epoch 53/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1335 - accuracy: 0.9528 - val_loss: 0.1263 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.00899\n",
            "Epoch 54/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1143 - accuracy: 0.9760 - val_loss: 0.1242 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.00899\n",
            "Epoch 55/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1118 - accuracy: 0.9857 - val_loss: 0.1222 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.00899\n",
            "Epoch 56/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1187 - accuracy: 0.9874 - val_loss: 0.1186 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.00899\n",
            "Epoch 57/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1029 - accuracy: 0.9725 - val_loss: 0.1322 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.00899\n",
            "Epoch 58/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1074 - accuracy: 0.9708 - val_loss: 0.1224 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.00899\n",
            "Epoch 59/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1506 - accuracy: 0.9296 - val_loss: 0.1120 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.00899\n",
            "Epoch 60/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0995 - accuracy: 0.9825 - val_loss: 0.1101 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.00899\n",
            "Epoch 61/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0997 - accuracy: 0.9775 - val_loss: 0.1083 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.00899\n",
            "Epoch 62/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1195 - accuracy: 0.9760 - val_loss: 0.1093 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.00899\n",
            "Epoch 63/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1001 - accuracy: 0.9898 - val_loss: 0.1132 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.00899\n",
            "Epoch 64/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1163 - accuracy: 0.9801 - val_loss: 0.1090 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.00899\n",
            "Epoch 65/100\n",
            "27/27 [==============================] - 0s 5ms/step - loss: 0.1047 - accuracy: 0.9873 - val_loss: 0.1030 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.00899\n",
            "Epoch 66/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0939 - accuracy: 0.9791 - val_loss: 0.1016 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.00899\n",
            "Epoch 67/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0871 - accuracy: 0.9838 - val_loss: 0.0987 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.00899\n",
            "Epoch 68/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0696 - accuracy: 0.9934 - val_loss: 0.0964 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.00899\n",
            "Epoch 69/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0691 - accuracy: 0.9879 - val_loss: 0.1081 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.00899\n",
            "Epoch 70/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0773 - accuracy: 0.9770 - val_loss: 0.0975 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.00899\n",
            "Epoch 71/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0923 - accuracy: 0.9768 - val_loss: 0.0945 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.00899\n",
            "Epoch 72/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0861 - accuracy: 0.9836 - val_loss: 0.0900 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.00899\n",
            "Epoch 73/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0927 - accuracy: 0.9713 - val_loss: 0.0936 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.00899\n",
            "Epoch 74/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1009 - accuracy: 0.9608 - val_loss: 0.0873 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.00899\n",
            "Epoch 75/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1074 - accuracy: 0.9397 - val_loss: 0.0861 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.00899\n",
            "Epoch 76/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1122 - accuracy: 0.9762 - val_loss: 0.0850 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.00899\n",
            "Epoch 77/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1350 - accuracy: 0.9621 - val_loss: 0.0813 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.00899\n",
            "Epoch 78/100\n",
            "27/27 [==============================] - 0s 5ms/step - loss: 0.0859 - accuracy: 0.9680 - val_loss: 0.0902 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.00899\n",
            "Epoch 79/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0933 - accuracy: 0.9764 - val_loss: 0.0810 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.00899\n",
            "Epoch 80/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1003 - accuracy: 0.9860 - val_loss: 0.0795 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.00899\n",
            "Epoch 81/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0703 - accuracy: 0.9868 - val_loss: 0.0799 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.00899\n",
            "Epoch 82/100\n",
            "27/27 [==============================] - 0s 5ms/step - loss: 0.1096 - accuracy: 0.9549 - val_loss: 0.0768 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.00899\n",
            "Epoch 83/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0822 - accuracy: 0.9781 - val_loss: 0.0762 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.00899\n",
            "Epoch 84/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0824 - accuracy: 0.9833 - val_loss: 0.0757 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.00899\n",
            "Epoch 85/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0784 - accuracy: 0.9662 - val_loss: 0.0752 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.00899\n",
            "Epoch 86/100\n",
            "27/27 [==============================] - 0s 5ms/step - loss: 0.1048 - accuracy: 0.9751 - val_loss: 0.0744 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.00899\n",
            "Epoch 87/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0699 - accuracy: 0.9829 - val_loss: 0.0699 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.00899\n",
            "Epoch 88/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0749 - accuracy: 0.9900 - val_loss: 0.0716 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.00899\n",
            "Epoch 89/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0768 - accuracy: 0.9626 - val_loss: 0.0709 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.00899\n",
            "Epoch 90/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1260 - accuracy: 0.9606 - val_loss: 0.0685 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.00899\n",
            "Epoch 91/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1131 - accuracy: 0.9376 - val_loss: 0.0680 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.00899\n",
            "Epoch 92/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0724 - accuracy: 0.9902 - val_loss: 0.0673 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.00899\n",
            "Epoch 93/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1157 - accuracy: 0.9600 - val_loss: 0.1147 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.00899\n",
            "Epoch 94/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1451 - accuracy: 0.9368 - val_loss: 0.0781 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.00899\n",
            "Epoch 95/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0545 - accuracy: 0.9930 - val_loss: 0.0732 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.00899\n",
            "Epoch 96/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0763 - accuracy: 0.9646 - val_loss: 0.0650 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.00899\n",
            "Epoch 97/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0811 - accuracy: 0.9785 - val_loss: 0.0656 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.00899\n",
            "Epoch 98/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0809 - accuracy: 0.9787 - val_loss: 0.0640 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.00899\n",
            "Epoch 99/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0740 - accuracy: 0.9841 - val_loss: 0.0668 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.00899\n",
            "Epoch 100/100\n",
            "27/27 [==============================] - 0s 5ms/step - loss: 0.0612 - accuracy: 0.9865 - val_loss: 0.0606 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.00899\n",
            "1/1 [==============================] - 0s 306ms/step - loss: 0.0606 - accuracy: 1.0000\n",
            "\n",
            " 10 fold accuracy:  ['1.0000', '1.0000', '1.0000', '0.8667', '0.9333', '0.8000', '0.9333', '1.0000', '1.0000', '1.0000']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wbo318jCPWKJ",
        "outputId": "652d094b-f524-4fe8-a114-002f610e25f4"
      },
      "source": [
        "print('% f fold accuracy: ' %n_fold, accuracy)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " 10.000000 fold accuracy:  ['1.0000', '1.0000', '1.0000', '0.8667', '0.9333', '0.8000', '0.9333', '1.0000', '1.0000', '1.0000']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPgUAMGyX4_W"
      },
      "source": [
        "# 미션. \n",
        "- StratifiedKFold 검증을 실시할 것(이때 y데이터는 라벨인코딩한 값을 먼저 넣고, for 안에서 원핫인코딩해서 모델링할 것)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hO3iezimVBg7"
      },
      "source": [
        "#계층화 k-fold...\n",
        "skf=StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=100)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nBgqPzUrU7ZG",
        "outputId": "867cb597-49ab-4ecd-e4fb-2ab3dfc03268"
      },
      "source": [
        "for train, test in skf.split(X,y_obj): #stratified k-fold에 넣을때 y는 원핫인코딩상태여선 안됨. 그냥 obj 라벨이나 0, 1, 2 라벨인코딩상태까지만 가능. \n",
        "                                        #원핫인코딩에서는 ( , 3)의 형태이고, 라벨인코딩한 y_le나 원래 데이터는 ( ,1 )의 shape이기 때문\n",
        "  print('-------train----------'*10)\n",
        "  print(train)\n",
        "  print('--------------------X train----------------------')\n",
        "  print(X[train], len(X[train]), np.shape(X[train])) #위의 숫자 인덱스에 해당하는 x[train] 데이터 보기 .\n",
        "  print('-------------------X test-----------------------')\n",
        "  print(X[test], len(X[test]), np.shape(X[test]))\n",
        "  print('--------------------y train----------------------')\n",
        "  print(y_obj[train], len(y_obj[train]), np.shape(y_obj[train])) \n",
        "  print('-------------------y test-----------------------')\n",
        "  print(y_obj[test], len(y_obj[test]), np.shape(y_obj[test]))"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-------train-----------------train-----------------train-----------------train-----------------train-----------------train-----------------train-----------------train-----------------train-----------------train----------\n",
            "[  0   1   2   3   4   5   6   7   8   9  11  12  13  14  15  16  17  18\n",
            "  19  20  21  22  23  24  25  28  29  30  31  32  33  34  35  36  37  38\n",
            "  39  41  42  43  44  45  46  48  49  50  52  53  54  56  57  58  59  60\n",
            "  61  62  63  64  65  66  67  68  69  70  71  72  74  75  76  77  78  79\n",
            "  80  81  82  83  85  86  87  88  89  90  91  93  94  95  96  97  98  99\n",
            " 100 101 102 103 105 106 107 108 109 110 111 112 113 114 115 116 117 118\n",
            " 119 120 121 123 124 125 126 127 128 129 131 132 133 134 135 136 137 138\n",
            " 139 140 142 143 145 146 147 148 149]\n",
            "--------------------X train----------------------\n",
            "[[5.1 3.5 1.4 0.2]\n",
            " [4.9 3.  1.4 0.2]\n",
            " [4.7 3.2 1.3 0.2]\n",
            " [4.6 3.1 1.5 0.2]\n",
            " [5.  3.6 1.4 0.2]\n",
            " [5.4 3.9 1.7 0.4]\n",
            " [4.6 3.4 1.4 0.3]\n",
            " [5.  3.4 1.5 0.2]\n",
            " [4.4 2.9 1.4 0.2]\n",
            " [4.9 3.1 1.5 0.1]\n",
            " [4.8 3.4 1.6 0.2]\n",
            " [4.8 3.  1.4 0.1]\n",
            " [4.3 3.  1.1 0.1]\n",
            " [5.8 4.  1.2 0.2]\n",
            " [5.7 4.4 1.5 0.4]\n",
            " [5.4 3.9 1.3 0.4]\n",
            " [5.1 3.5 1.4 0.3]\n",
            " [5.7 3.8 1.7 0.3]\n",
            " [5.1 3.8 1.5 0.3]\n",
            " [5.4 3.4 1.7 0.2]\n",
            " [5.1 3.7 1.5 0.4]\n",
            " [4.6 3.6 1.  0.2]\n",
            " [5.1 3.3 1.7 0.5]\n",
            " [4.8 3.4 1.9 0.2]\n",
            " [5.  3.  1.6 0.2]\n",
            " [5.2 3.4 1.4 0.2]\n",
            " [4.7 3.2 1.6 0.2]\n",
            " [4.8 3.1 1.6 0.2]\n",
            " [5.4 3.4 1.5 0.4]\n",
            " [5.2 4.1 1.5 0.1]\n",
            " [5.5 4.2 1.4 0.2]\n",
            " [4.9 3.1 1.5 0.1]\n",
            " [5.  3.2 1.2 0.2]\n",
            " [5.5 3.5 1.3 0.2]\n",
            " [4.9 3.1 1.5 0.1]\n",
            " [4.4 3.  1.3 0.2]\n",
            " [5.1 3.4 1.5 0.2]\n",
            " [4.5 2.3 1.3 0.3]\n",
            " [4.4 3.2 1.3 0.2]\n",
            " [5.  3.5 1.6 0.6]\n",
            " [5.1 3.8 1.9 0.4]\n",
            " [4.8 3.  1.4 0.3]\n",
            " [5.1 3.8 1.6 0.2]\n",
            " [5.3 3.7 1.5 0.2]\n",
            " [5.  3.3 1.4 0.2]\n",
            " [7.  3.2 4.7 1.4]\n",
            " [6.9 3.1 4.9 1.5]\n",
            " [5.5 2.3 4.  1.3]\n",
            " [6.5 2.8 4.6 1.5]\n",
            " [6.3 3.3 4.7 1.6]\n",
            " [4.9 2.4 3.3 1. ]\n",
            " [6.6 2.9 4.6 1.3]\n",
            " [5.2 2.7 3.9 1.4]\n",
            " [5.  2.  3.5 1. ]\n",
            " [5.9 3.  4.2 1.5]\n",
            " [6.  2.2 4.  1. ]\n",
            " [6.1 2.9 4.7 1.4]\n",
            " [5.6 2.9 3.6 1.3]\n",
            " [6.7 3.1 4.4 1.4]\n",
            " [5.6 3.  4.5 1.5]\n",
            " [5.8 2.7 4.1 1. ]\n",
            " [6.2 2.2 4.5 1.5]\n",
            " [5.6 2.5 3.9 1.1]\n",
            " [5.9 3.2 4.8 1.8]\n",
            " [6.1 2.8 4.  1.3]\n",
            " [6.3 2.5 4.9 1.5]\n",
            " [6.4 2.9 4.3 1.3]\n",
            " [6.6 3.  4.4 1.4]\n",
            " [6.8 2.8 4.8 1.4]\n",
            " [6.7 3.  5.  1.7]\n",
            " [6.  2.9 4.5 1.5]\n",
            " [5.7 2.6 3.5 1. ]\n",
            " [5.5 2.4 3.8 1.1]\n",
            " [5.5 2.4 3.7 1. ]\n",
            " [5.8 2.7 3.9 1.2]\n",
            " [6.  2.7 5.1 1.6]\n",
            " [6.  3.4 4.5 1.6]\n",
            " [6.7 3.1 4.7 1.5]\n",
            " [6.3 2.3 4.4 1.3]\n",
            " [5.6 3.  4.1 1.3]\n",
            " [5.5 2.5 4.  1.3]\n",
            " [5.5 2.6 4.4 1.2]\n",
            " [6.1 3.  4.6 1.4]\n",
            " [5.  2.3 3.3 1. ]\n",
            " [5.6 2.7 4.2 1.3]\n",
            " [5.7 3.  4.2 1.2]\n",
            " [5.7 2.9 4.2 1.3]\n",
            " [6.2 2.9 4.3 1.3]\n",
            " [5.1 2.5 3.  1.1]\n",
            " [5.7 2.8 4.1 1.3]\n",
            " [6.3 3.3 6.  2.5]\n",
            " [5.8 2.7 5.1 1.9]\n",
            " [7.1 3.  5.9 2.1]\n",
            " [6.3 2.9 5.6 1.8]\n",
            " [7.6 3.  6.6 2.1]\n",
            " [4.9 2.5 4.5 1.7]\n",
            " [7.3 2.9 6.3 1.8]\n",
            " [6.7 2.5 5.8 1.8]\n",
            " [7.2 3.6 6.1 2.5]\n",
            " [6.5 3.2 5.1 2. ]\n",
            " [6.4 2.7 5.3 1.9]\n",
            " [6.8 3.  5.5 2.1]\n",
            " [5.7 2.5 5.  2. ]\n",
            " [5.8 2.8 5.1 2.4]\n",
            " [6.4 3.2 5.3 2.3]\n",
            " [6.5 3.  5.5 1.8]\n",
            " [7.7 3.8 6.7 2.2]\n",
            " [7.7 2.6 6.9 2.3]\n",
            " [6.  2.2 5.  1.5]\n",
            " [6.9 3.2 5.7 2.3]\n",
            " [5.6 2.8 4.9 2. ]\n",
            " [6.3 2.7 4.9 1.8]\n",
            " [6.7 3.3 5.7 2.1]\n",
            " [7.2 3.2 6.  1.8]\n",
            " [6.2 2.8 4.8 1.8]\n",
            " [6.1 3.  4.9 1.8]\n",
            " [6.4 2.8 5.6 2.1]\n",
            " [7.2 3.  5.8 1.6]\n",
            " [7.9 3.8 6.4 2. ]\n",
            " [6.4 2.8 5.6 2.2]\n",
            " [6.3 2.8 5.1 1.5]\n",
            " [6.1 2.6 5.6 1.4]\n",
            " [7.7 3.  6.1 2.3]\n",
            " [6.3 3.4 5.6 2.4]\n",
            " [6.4 3.1 5.5 1.8]\n",
            " [6.  3.  4.8 1.8]\n",
            " [6.9 3.1 5.4 2.1]\n",
            " [6.7 3.1 5.6 2.4]\n",
            " [5.8 2.7 5.1 1.9]\n",
            " [6.8 3.2 5.9 2.3]\n",
            " [6.7 3.  5.2 2.3]\n",
            " [6.3 2.5 5.  1.9]\n",
            " [6.5 3.  5.2 2. ]\n",
            " [6.2 3.4 5.4 2.3]\n",
            " [5.9 3.  5.1 1.8]] 135 (135, 4)\n",
            "-------------------X test-----------------------\n",
            "[[5.4 3.7 1.5 0.2]\n",
            " [5.  3.4 1.6 0.4]\n",
            " [5.2 3.5 1.5 0.2]\n",
            " [5.  3.5 1.3 0.3]\n",
            " [4.6 3.2 1.4 0.2]\n",
            " [6.4 3.2 4.5 1.5]\n",
            " [5.7 2.8 4.5 1.3]\n",
            " [6.1 2.8 4.7 1.2]\n",
            " [5.4 3.  4.5 1.5]\n",
            " [5.8 2.6 4.  1.2]\n",
            " [6.5 3.  5.8 2.2]\n",
            " [7.7 2.8 6.7 2. ]\n",
            " [7.4 2.8 6.1 1.9]\n",
            " [6.9 3.1 5.1 2.3]\n",
            " [6.7 3.3 5.7 2.5]] 15 (15, 4)\n",
            "--------------------y train----------------------\n",
            "['Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
            " 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
            " 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
            " 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
            " 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
            " 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
            " 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
            " 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
            " 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
            " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
            " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
            " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
            " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
            " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
            " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
            " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
            " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
            " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
            " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
            " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
            " 'Iris-versicolor' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
            " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
            " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
            " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
            " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
            " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
            " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
            " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
            " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
            " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
            " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
            " 'Iris-virginica' 'Iris-virginica'] 135 (135,)\n",
            "-------------------y test-----------------------\n",
            "['Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
            " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
            " 'Iris-versicolor' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
            " 'Iris-virginica' 'Iris-virginica'] 15 (15,)\n",
            "-------train-----------------train-----------------train-----------------train-----------------train-----------------train-----------------train-----------------train-----------------train-----------------train----------\n",
            "[  1   2   3   4   5   7   8   9  10  11  12  13  14  15  16  17  18  20\n",
            "  21  22  23  24  25  26  27  28  29  30  31  32  34  35  36  37  38  39\n",
            "  40  41  42  43  44  45  46  47  48  50  51  52  53  54  55  56  57  58\n",
            "  61  62  63  64  66  67  68  69  70  71  73  74  75  76  77  78  79  80\n",
            "  81  82  83  84  85  86  87  89  90  91  92  93  94  95  96  97  98  99\n",
            " 100 102 103 104 105 106 107 108 109 110 111 113 114 115 116 117 118 119\n",
            " 120 121 122 123 125 126 127 128 129 130 131 132 133 135 136 138 139 140\n",
            " 141 142 143 144 145 146 147 148 149]\n",
            "--------------------X train----------------------\n",
            "[[4.9 3.  1.4 0.2]\n",
            " [4.7 3.2 1.3 0.2]\n",
            " [4.6 3.1 1.5 0.2]\n",
            " [5.  3.6 1.4 0.2]\n",
            " [5.4 3.9 1.7 0.4]\n",
            " [5.  3.4 1.5 0.2]\n",
            " [4.4 2.9 1.4 0.2]\n",
            " [4.9 3.1 1.5 0.1]\n",
            " [5.4 3.7 1.5 0.2]\n",
            " [4.8 3.4 1.6 0.2]\n",
            " [4.8 3.  1.4 0.1]\n",
            " [4.3 3.  1.1 0.1]\n",
            " [5.8 4.  1.2 0.2]\n",
            " [5.7 4.4 1.5 0.4]\n",
            " [5.4 3.9 1.3 0.4]\n",
            " [5.1 3.5 1.4 0.3]\n",
            " [5.7 3.8 1.7 0.3]\n",
            " [5.4 3.4 1.7 0.2]\n",
            " [5.1 3.7 1.5 0.4]\n",
            " [4.6 3.6 1.  0.2]\n",
            " [5.1 3.3 1.7 0.5]\n",
            " [4.8 3.4 1.9 0.2]\n",
            " [5.  3.  1.6 0.2]\n",
            " [5.  3.4 1.6 0.4]\n",
            " [5.2 3.5 1.5 0.2]\n",
            " [5.2 3.4 1.4 0.2]\n",
            " [4.7 3.2 1.6 0.2]\n",
            " [4.8 3.1 1.6 0.2]\n",
            " [5.4 3.4 1.5 0.4]\n",
            " [5.2 4.1 1.5 0.1]\n",
            " [4.9 3.1 1.5 0.1]\n",
            " [5.  3.2 1.2 0.2]\n",
            " [5.5 3.5 1.3 0.2]\n",
            " [4.9 3.1 1.5 0.1]\n",
            " [4.4 3.  1.3 0.2]\n",
            " [5.1 3.4 1.5 0.2]\n",
            " [5.  3.5 1.3 0.3]\n",
            " [4.5 2.3 1.3 0.3]\n",
            " [4.4 3.2 1.3 0.2]\n",
            " [5.  3.5 1.6 0.6]\n",
            " [5.1 3.8 1.9 0.4]\n",
            " [4.8 3.  1.4 0.3]\n",
            " [5.1 3.8 1.6 0.2]\n",
            " [4.6 3.2 1.4 0.2]\n",
            " [5.3 3.7 1.5 0.2]\n",
            " [7.  3.2 4.7 1.4]\n",
            " [6.4 3.2 4.5 1.5]\n",
            " [6.9 3.1 4.9 1.5]\n",
            " [5.5 2.3 4.  1.3]\n",
            " [6.5 2.8 4.6 1.5]\n",
            " [5.7 2.8 4.5 1.3]\n",
            " [6.3 3.3 4.7 1.6]\n",
            " [4.9 2.4 3.3 1. ]\n",
            " [6.6 2.9 4.6 1.3]\n",
            " [5.9 3.  4.2 1.5]\n",
            " [6.  2.2 4.  1. ]\n",
            " [6.1 2.9 4.7 1.4]\n",
            " [5.6 2.9 3.6 1.3]\n",
            " [5.6 3.  4.5 1.5]\n",
            " [5.8 2.7 4.1 1. ]\n",
            " [6.2 2.2 4.5 1.5]\n",
            " [5.6 2.5 3.9 1.1]\n",
            " [5.9 3.2 4.8 1.8]\n",
            " [6.1 2.8 4.  1.3]\n",
            " [6.1 2.8 4.7 1.2]\n",
            " [6.4 2.9 4.3 1.3]\n",
            " [6.6 3.  4.4 1.4]\n",
            " [6.8 2.8 4.8 1.4]\n",
            " [6.7 3.  5.  1.7]\n",
            " [6.  2.9 4.5 1.5]\n",
            " [5.7 2.6 3.5 1. ]\n",
            " [5.5 2.4 3.8 1.1]\n",
            " [5.5 2.4 3.7 1. ]\n",
            " [5.8 2.7 3.9 1.2]\n",
            " [6.  2.7 5.1 1.6]\n",
            " [5.4 3.  4.5 1.5]\n",
            " [6.  3.4 4.5 1.6]\n",
            " [6.7 3.1 4.7 1.5]\n",
            " [6.3 2.3 4.4 1.3]\n",
            " [5.5 2.5 4.  1.3]\n",
            " [5.5 2.6 4.4 1.2]\n",
            " [6.1 3.  4.6 1.4]\n",
            " [5.8 2.6 4.  1.2]\n",
            " [5.  2.3 3.3 1. ]\n",
            " [5.6 2.7 4.2 1.3]\n",
            " [5.7 3.  4.2 1.2]\n",
            " [5.7 2.9 4.2 1.3]\n",
            " [6.2 2.9 4.3 1.3]\n",
            " [5.1 2.5 3.  1.1]\n",
            " [5.7 2.8 4.1 1.3]\n",
            " [6.3 3.3 6.  2.5]\n",
            " [7.1 3.  5.9 2.1]\n",
            " [6.3 2.9 5.6 1.8]\n",
            " [6.5 3.  5.8 2.2]\n",
            " [7.6 3.  6.6 2.1]\n",
            " [4.9 2.5 4.5 1.7]\n",
            " [7.3 2.9 6.3 1.8]\n",
            " [6.7 2.5 5.8 1.8]\n",
            " [7.2 3.6 6.1 2.5]\n",
            " [6.5 3.2 5.1 2. ]\n",
            " [6.4 2.7 5.3 1.9]\n",
            " [5.7 2.5 5.  2. ]\n",
            " [5.8 2.8 5.1 2.4]\n",
            " [6.4 3.2 5.3 2.3]\n",
            " [6.5 3.  5.5 1.8]\n",
            " [7.7 3.8 6.7 2.2]\n",
            " [7.7 2.6 6.9 2.3]\n",
            " [6.  2.2 5.  1.5]\n",
            " [6.9 3.2 5.7 2.3]\n",
            " [5.6 2.8 4.9 2. ]\n",
            " [7.7 2.8 6.7 2. ]\n",
            " [6.3 2.7 4.9 1.8]\n",
            " [7.2 3.2 6.  1.8]\n",
            " [6.2 2.8 4.8 1.8]\n",
            " [6.1 3.  4.9 1.8]\n",
            " [6.4 2.8 5.6 2.1]\n",
            " [7.2 3.  5.8 1.6]\n",
            " [7.4 2.8 6.1 1.9]\n",
            " [7.9 3.8 6.4 2. ]\n",
            " [6.4 2.8 5.6 2.2]\n",
            " [6.3 2.8 5.1 1.5]\n",
            " [7.7 3.  6.1 2.3]\n",
            " [6.3 3.4 5.6 2.4]\n",
            " [6.  3.  4.8 1.8]\n",
            " [6.9 3.1 5.4 2.1]\n",
            " [6.7 3.1 5.6 2.4]\n",
            " [6.9 3.1 5.1 2.3]\n",
            " [5.8 2.7 5.1 1.9]\n",
            " [6.8 3.2 5.9 2.3]\n",
            " [6.7 3.3 5.7 2.5]\n",
            " [6.7 3.  5.2 2.3]\n",
            " [6.3 2.5 5.  1.9]\n",
            " [6.5 3.  5.2 2. ]\n",
            " [6.2 3.4 5.4 2.3]\n",
            " [5.9 3.  5.1 1.8]] 135 (135, 4)\n",
            "-------------------X test-----------------------\n",
            "[[5.1 3.5 1.4 0.2]\n",
            " [4.6 3.4 1.4 0.3]\n",
            " [5.1 3.8 1.5 0.3]\n",
            " [5.5 4.2 1.4 0.2]\n",
            " [5.  3.3 1.4 0.2]\n",
            " [5.2 2.7 3.9 1.4]\n",
            " [5.  2.  3.5 1. ]\n",
            " [6.7 3.1 4.4 1.4]\n",
            " [6.3 2.5 4.9 1.5]\n",
            " [5.6 3.  4.1 1.3]\n",
            " [5.8 2.7 5.1 1.9]\n",
            " [6.8 3.  5.5 2.1]\n",
            " [6.7 3.3 5.7 2.1]\n",
            " [6.1 2.6 5.6 1.4]\n",
            " [6.4 3.1 5.5 1.8]] 15 (15, 4)\n",
            "--------------------y train----------------------\n",
            "['Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
            " 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
            " 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
            " 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
            " 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
            " 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
            " 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
            " 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
            " 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
            " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
            " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
            " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
            " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
            " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
            " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
            " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
            " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
            " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
            " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
            " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
            " 'Iris-versicolor' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
            " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
            " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
            " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
            " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
            " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
            " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
            " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
            " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
            " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
            " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
            " 'Iris-virginica' 'Iris-virginica'] 135 (135,)\n",
            "-------------------y test-----------------------\n",
            "['Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
            " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
            " 'Iris-versicolor' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
            " 'Iris-virginica' 'Iris-virginica'] 15 (15,)\n",
            "-------train-----------------train-----------------train-----------------train-----------------train-----------------train-----------------train-----------------train-----------------train-----------------train----------\n",
            "[  0   1   2   3   4   5   6   7   8   9  10  12  13  14  15  16  17  18\n",
            "  19  20  21  22  23  24  26  27  29  30  31  32  33  34  35  36  37  39\n",
            "  40  41  42  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58\n",
            "  59  60  61  63  64  65  66  67  68  69  70  71  72  73  74  76  77  78\n",
            "  80  81  82  83  84  85  86  88  89  91  92  93  94  95  96  97  98  99\n",
            " 100 101 102 103 104 105 108 109 110 111 112 114 115 116 117 118 119 120\n",
            " 121 122 123 124 126 127 128 129 130 131 132 133 134 135 136 137 138 139\n",
            " 140 141 142 143 144 145 146 147 148]\n",
            "--------------------X train----------------------\n",
            "[[5.1 3.5 1.4 0.2]\n",
            " [4.9 3.  1.4 0.2]\n",
            " [4.7 3.2 1.3 0.2]\n",
            " [4.6 3.1 1.5 0.2]\n",
            " [5.  3.6 1.4 0.2]\n",
            " [5.4 3.9 1.7 0.4]\n",
            " [4.6 3.4 1.4 0.3]\n",
            " [5.  3.4 1.5 0.2]\n",
            " [4.4 2.9 1.4 0.2]\n",
            " [4.9 3.1 1.5 0.1]\n",
            " [5.4 3.7 1.5 0.2]\n",
            " [4.8 3.  1.4 0.1]\n",
            " [4.3 3.  1.1 0.1]\n",
            " [5.8 4.  1.2 0.2]\n",
            " [5.7 4.4 1.5 0.4]\n",
            " [5.4 3.9 1.3 0.4]\n",
            " [5.1 3.5 1.4 0.3]\n",
            " [5.7 3.8 1.7 0.3]\n",
            " [5.1 3.8 1.5 0.3]\n",
            " [5.4 3.4 1.7 0.2]\n",
            " [5.1 3.7 1.5 0.4]\n",
            " [4.6 3.6 1.  0.2]\n",
            " [5.1 3.3 1.7 0.5]\n",
            " [4.8 3.4 1.9 0.2]\n",
            " [5.  3.4 1.6 0.4]\n",
            " [5.2 3.5 1.5 0.2]\n",
            " [4.7 3.2 1.6 0.2]\n",
            " [4.8 3.1 1.6 0.2]\n",
            " [5.4 3.4 1.5 0.4]\n",
            " [5.2 4.1 1.5 0.1]\n",
            " [5.5 4.2 1.4 0.2]\n",
            " [4.9 3.1 1.5 0.1]\n",
            " [5.  3.2 1.2 0.2]\n",
            " [5.5 3.5 1.3 0.2]\n",
            " [4.9 3.1 1.5 0.1]\n",
            " [5.1 3.4 1.5 0.2]\n",
            " [5.  3.5 1.3 0.3]\n",
            " [4.5 2.3 1.3 0.3]\n",
            " [4.4 3.2 1.3 0.2]\n",
            " [5.1 3.8 1.9 0.4]\n",
            " [4.8 3.  1.4 0.3]\n",
            " [5.1 3.8 1.6 0.2]\n",
            " [4.6 3.2 1.4 0.2]\n",
            " [5.3 3.7 1.5 0.2]\n",
            " [5.  3.3 1.4 0.2]\n",
            " [7.  3.2 4.7 1.4]\n",
            " [6.4 3.2 4.5 1.5]\n",
            " [6.9 3.1 4.9 1.5]\n",
            " [5.5 2.3 4.  1.3]\n",
            " [6.5 2.8 4.6 1.5]\n",
            " [5.7 2.8 4.5 1.3]\n",
            " [6.3 3.3 4.7 1.6]\n",
            " [4.9 2.4 3.3 1. ]\n",
            " [6.6 2.9 4.6 1.3]\n",
            " [5.2 2.7 3.9 1.4]\n",
            " [5.  2.  3.5 1. ]\n",
            " [5.9 3.  4.2 1.5]\n",
            " [6.1 2.9 4.7 1.4]\n",
            " [5.6 2.9 3.6 1.3]\n",
            " [6.7 3.1 4.4 1.4]\n",
            " [5.6 3.  4.5 1.5]\n",
            " [5.8 2.7 4.1 1. ]\n",
            " [6.2 2.2 4.5 1.5]\n",
            " [5.6 2.5 3.9 1.1]\n",
            " [5.9 3.2 4.8 1.8]\n",
            " [6.1 2.8 4.  1.3]\n",
            " [6.3 2.5 4.9 1.5]\n",
            " [6.1 2.8 4.7 1.2]\n",
            " [6.4 2.9 4.3 1.3]\n",
            " [6.8 2.8 4.8 1.4]\n",
            " [6.7 3.  5.  1.7]\n",
            " [6.  2.9 4.5 1.5]\n",
            " [5.5 2.4 3.8 1.1]\n",
            " [5.5 2.4 3.7 1. ]\n",
            " [5.8 2.7 3.9 1.2]\n",
            " [6.  2.7 5.1 1.6]\n",
            " [5.4 3.  4.5 1.5]\n",
            " [6.  3.4 4.5 1.6]\n",
            " [6.7 3.1 4.7 1.5]\n",
            " [5.6 3.  4.1 1.3]\n",
            " [5.5 2.5 4.  1.3]\n",
            " [6.1 3.  4.6 1.4]\n",
            " [5.8 2.6 4.  1.2]\n",
            " [5.  2.3 3.3 1. ]\n",
            " [5.6 2.7 4.2 1.3]\n",
            " [5.7 3.  4.2 1.2]\n",
            " [5.7 2.9 4.2 1.3]\n",
            " [6.2 2.9 4.3 1.3]\n",
            " [5.1 2.5 3.  1.1]\n",
            " [5.7 2.8 4.1 1.3]\n",
            " [6.3 3.3 6.  2.5]\n",
            " [5.8 2.7 5.1 1.9]\n",
            " [7.1 3.  5.9 2.1]\n",
            " [6.3 2.9 5.6 1.8]\n",
            " [6.5 3.  5.8 2.2]\n",
            " [7.6 3.  6.6 2.1]\n",
            " [6.7 2.5 5.8 1.8]\n",
            " [7.2 3.6 6.1 2.5]\n",
            " [6.5 3.2 5.1 2. ]\n",
            " [6.4 2.7 5.3 1.9]\n",
            " [6.8 3.  5.5 2.1]\n",
            " [5.8 2.8 5.1 2.4]\n",
            " [6.4 3.2 5.3 2.3]\n",
            " [6.5 3.  5.5 1.8]\n",
            " [7.7 3.8 6.7 2.2]\n",
            " [7.7 2.6 6.9 2.3]\n",
            " [6.  2.2 5.  1.5]\n",
            " [6.9 3.2 5.7 2.3]\n",
            " [5.6 2.8 4.9 2. ]\n",
            " [7.7 2.8 6.7 2. ]\n",
            " [6.3 2.7 4.9 1.8]\n",
            " [6.7 3.3 5.7 2.1]\n",
            " [6.2 2.8 4.8 1.8]\n",
            " [6.1 3.  4.9 1.8]\n",
            " [6.4 2.8 5.6 2.1]\n",
            " [7.2 3.  5.8 1.6]\n",
            " [7.4 2.8 6.1 1.9]\n",
            " [7.9 3.8 6.4 2. ]\n",
            " [6.4 2.8 5.6 2.2]\n",
            " [6.3 2.8 5.1 1.5]\n",
            " [6.1 2.6 5.6 1.4]\n",
            " [7.7 3.  6.1 2.3]\n",
            " [6.3 3.4 5.6 2.4]\n",
            " [6.4 3.1 5.5 1.8]\n",
            " [6.  3.  4.8 1.8]\n",
            " [6.9 3.1 5.4 2.1]\n",
            " [6.7 3.1 5.6 2.4]\n",
            " [6.9 3.1 5.1 2.3]\n",
            " [5.8 2.7 5.1 1.9]\n",
            " [6.8 3.2 5.9 2.3]\n",
            " [6.7 3.3 5.7 2.5]\n",
            " [6.7 3.  5.2 2.3]\n",
            " [6.3 2.5 5.  1.9]\n",
            " [6.5 3.  5.2 2. ]\n",
            " [6.2 3.4 5.4 2.3]] 135 (135, 4)\n",
            "-------------------X test-----------------------\n",
            "[[4.8 3.4 1.6 0.2]\n",
            " [5.  3.  1.6 0.2]\n",
            " [5.2 3.4 1.4 0.2]\n",
            " [4.4 3.  1.3 0.2]\n",
            " [5.  3.5 1.6 0.6]\n",
            " [6.  2.2 4.  1. ]\n",
            " [6.6 3.  4.4 1.4]\n",
            " [5.7 2.6 3.5 1. ]\n",
            " [6.3 2.3 4.4 1.3]\n",
            " [5.5 2.6 4.4 1.2]\n",
            " [4.9 2.5 4.5 1.7]\n",
            " [7.3 2.9 6.3 1.8]\n",
            " [5.7 2.5 5.  2. ]\n",
            " [7.2 3.2 6.  1.8]\n",
            " [5.9 3.  5.1 1.8]] 15 (15, 4)\n",
            "--------------------y train----------------------\n",
            "['Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
            " 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
            " 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
            " 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
            " 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
            " 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
            " 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
            " 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
            " 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
            " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
            " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
            " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
            " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
            " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
            " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
            " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
            " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
            " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
            " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
            " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
            " 'Iris-versicolor' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
            " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
            " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
            " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
            " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
            " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
            " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
            " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
            " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
            " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
            " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
            " 'Iris-virginica' 'Iris-virginica'] 135 (135,)\n",
            "-------------------y test-----------------------\n",
            "['Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
            " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
            " 'Iris-versicolor' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
            " 'Iris-virginica' 'Iris-virginica'] 15 (15,)\n",
            "-------train-----------------train-----------------train-----------------train-----------------train-----------------train-----------------train-----------------train-----------------train-----------------train----------\n",
            "[  0   1   2   3   4   5   6   7   8   9  10  11  13  14  15  16  17  19\n",
            "  20  21  22  24  25  26  27  28  29  30  31  32  33  35  36  37  38  39\n",
            "  40  41  42  43  45  46  47  48  49  51  52  53  54  55  56  57  58  59\n",
            "  60  61  62  63  64  65  66  67  68  70  71  72  73  74  75  76  77  78\n",
            "  79  80  82  83  84  85  86  87  88  90  91  92  94  95  96  97  98  99\n",
            " 100 101 102 103 104 105 106 107 108 109 111 112 113 114 115 116 117 118\n",
            " 120 122 123 124 125 127 128 129 130 131 132 133 134 135 136 137 138 139\n",
            " 140 141 143 144 145 146 147 148 149]\n",
            "--------------------X train----------------------\n",
            "[[5.1 3.5 1.4 0.2]\n",
            " [4.9 3.  1.4 0.2]\n",
            " [4.7 3.2 1.3 0.2]\n",
            " [4.6 3.1 1.5 0.2]\n",
            " [5.  3.6 1.4 0.2]\n",
            " [5.4 3.9 1.7 0.4]\n",
            " [4.6 3.4 1.4 0.3]\n",
            " [5.  3.4 1.5 0.2]\n",
            " [4.4 2.9 1.4 0.2]\n",
            " [4.9 3.1 1.5 0.1]\n",
            " [5.4 3.7 1.5 0.2]\n",
            " [4.8 3.4 1.6 0.2]\n",
            " [4.3 3.  1.1 0.1]\n",
            " [5.8 4.  1.2 0.2]\n",
            " [5.7 4.4 1.5 0.4]\n",
            " [5.4 3.9 1.3 0.4]\n",
            " [5.1 3.5 1.4 0.3]\n",
            " [5.1 3.8 1.5 0.3]\n",
            " [5.4 3.4 1.7 0.2]\n",
            " [5.1 3.7 1.5 0.4]\n",
            " [4.6 3.6 1.  0.2]\n",
            " [4.8 3.4 1.9 0.2]\n",
            " [5.  3.  1.6 0.2]\n",
            " [5.  3.4 1.6 0.4]\n",
            " [5.2 3.5 1.5 0.2]\n",
            " [5.2 3.4 1.4 0.2]\n",
            " [4.7 3.2 1.6 0.2]\n",
            " [4.8 3.1 1.6 0.2]\n",
            " [5.4 3.4 1.5 0.4]\n",
            " [5.2 4.1 1.5 0.1]\n",
            " [5.5 4.2 1.4 0.2]\n",
            " [5.  3.2 1.2 0.2]\n",
            " [5.5 3.5 1.3 0.2]\n",
            " [4.9 3.1 1.5 0.1]\n",
            " [4.4 3.  1.3 0.2]\n",
            " [5.1 3.4 1.5 0.2]\n",
            " [5.  3.5 1.3 0.3]\n",
            " [4.5 2.3 1.3 0.3]\n",
            " [4.4 3.2 1.3 0.2]\n",
            " [5.  3.5 1.6 0.6]\n",
            " [4.8 3.  1.4 0.3]\n",
            " [5.1 3.8 1.6 0.2]\n",
            " [4.6 3.2 1.4 0.2]\n",
            " [5.3 3.7 1.5 0.2]\n",
            " [5.  3.3 1.4 0.2]\n",
            " [6.4 3.2 4.5 1.5]\n",
            " [6.9 3.1 4.9 1.5]\n",
            " [5.5 2.3 4.  1.3]\n",
            " [6.5 2.8 4.6 1.5]\n",
            " [5.7 2.8 4.5 1.3]\n",
            " [6.3 3.3 4.7 1.6]\n",
            " [4.9 2.4 3.3 1. ]\n",
            " [6.6 2.9 4.6 1.3]\n",
            " [5.2 2.7 3.9 1.4]\n",
            " [5.  2.  3.5 1. ]\n",
            " [5.9 3.  4.2 1.5]\n",
            " [6.  2.2 4.  1. ]\n",
            " [6.1 2.9 4.7 1.4]\n",
            " [5.6 2.9 3.6 1.3]\n",
            " [6.7 3.1 4.4 1.4]\n",
            " [5.6 3.  4.5 1.5]\n",
            " [5.8 2.7 4.1 1. ]\n",
            " [6.2 2.2 4.5 1.5]\n",
            " [5.9 3.2 4.8 1.8]\n",
            " [6.1 2.8 4.  1.3]\n",
            " [6.3 2.5 4.9 1.5]\n",
            " [6.1 2.8 4.7 1.2]\n",
            " [6.4 2.9 4.3 1.3]\n",
            " [6.6 3.  4.4 1.4]\n",
            " [6.8 2.8 4.8 1.4]\n",
            " [6.7 3.  5.  1.7]\n",
            " [6.  2.9 4.5 1.5]\n",
            " [5.7 2.6 3.5 1. ]\n",
            " [5.5 2.4 3.8 1.1]\n",
            " [5.8 2.7 3.9 1.2]\n",
            " [6.  2.7 5.1 1.6]\n",
            " [5.4 3.  4.5 1.5]\n",
            " [6.  3.4 4.5 1.6]\n",
            " [6.7 3.1 4.7 1.5]\n",
            " [6.3 2.3 4.4 1.3]\n",
            " [5.6 3.  4.1 1.3]\n",
            " [5.5 2.6 4.4 1.2]\n",
            " [6.1 3.  4.6 1.4]\n",
            " [5.8 2.6 4.  1.2]\n",
            " [5.6 2.7 4.2 1.3]\n",
            " [5.7 3.  4.2 1.2]\n",
            " [5.7 2.9 4.2 1.3]\n",
            " [6.2 2.9 4.3 1.3]\n",
            " [5.1 2.5 3.  1.1]\n",
            " [5.7 2.8 4.1 1.3]\n",
            " [6.3 3.3 6.  2.5]\n",
            " [5.8 2.7 5.1 1.9]\n",
            " [7.1 3.  5.9 2.1]\n",
            " [6.3 2.9 5.6 1.8]\n",
            " [6.5 3.  5.8 2.2]\n",
            " [7.6 3.  6.6 2.1]\n",
            " [4.9 2.5 4.5 1.7]\n",
            " [7.3 2.9 6.3 1.8]\n",
            " [6.7 2.5 5.8 1.8]\n",
            " [7.2 3.6 6.1 2.5]\n",
            " [6.4 2.7 5.3 1.9]\n",
            " [6.8 3.  5.5 2.1]\n",
            " [5.7 2.5 5.  2. ]\n",
            " [5.8 2.8 5.1 2.4]\n",
            " [6.4 3.2 5.3 2.3]\n",
            " [6.5 3.  5.5 1.8]\n",
            " [7.7 3.8 6.7 2.2]\n",
            " [7.7 2.6 6.9 2.3]\n",
            " [6.9 3.2 5.7 2.3]\n",
            " [7.7 2.8 6.7 2. ]\n",
            " [6.3 2.7 4.9 1.8]\n",
            " [6.7 3.3 5.7 2.1]\n",
            " [7.2 3.2 6.  1.8]\n",
            " [6.1 3.  4.9 1.8]\n",
            " [6.4 2.8 5.6 2.1]\n",
            " [7.2 3.  5.8 1.6]\n",
            " [7.4 2.8 6.1 1.9]\n",
            " [7.9 3.8 6.4 2. ]\n",
            " [6.4 2.8 5.6 2.2]\n",
            " [6.3 2.8 5.1 1.5]\n",
            " [6.1 2.6 5.6 1.4]\n",
            " [7.7 3.  6.1 2.3]\n",
            " [6.3 3.4 5.6 2.4]\n",
            " [6.4 3.1 5.5 1.8]\n",
            " [6.  3.  4.8 1.8]\n",
            " [6.9 3.1 5.4 2.1]\n",
            " [6.7 3.1 5.6 2.4]\n",
            " [6.9 3.1 5.1 2.3]\n",
            " [6.8 3.2 5.9 2.3]\n",
            " [6.7 3.3 5.7 2.5]\n",
            " [6.7 3.  5.2 2.3]\n",
            " [6.3 2.5 5.  1.9]\n",
            " [6.5 3.  5.2 2. ]\n",
            " [6.2 3.4 5.4 2.3]\n",
            " [5.9 3.  5.1 1.8]] 135 (135, 4)\n",
            "-------------------X test-----------------------\n",
            "[[4.8 3.  1.4 0.1]\n",
            " [5.7 3.8 1.7 0.3]\n",
            " [5.1 3.3 1.7 0.5]\n",
            " [4.9 3.1 1.5 0.1]\n",
            " [5.1 3.8 1.9 0.4]\n",
            " [7.  3.2 4.7 1.4]\n",
            " [5.6 2.5 3.9 1.1]\n",
            " [5.5 2.4 3.7 1. ]\n",
            " [5.5 2.5 4.  1.3]\n",
            " [5.  2.3 3.3 1. ]\n",
            " [6.5 3.2 5.1 2. ]\n",
            " [6.  2.2 5.  1.5]\n",
            " [5.6 2.8 4.9 2. ]\n",
            " [6.2 2.8 4.8 1.8]\n",
            " [5.8 2.7 5.1 1.9]] 15 (15, 4)\n",
            "--------------------y train----------------------\n",
            "['Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
            " 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
            " 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
            " 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
            " 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
            " 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
            " 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
            " 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
            " 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
            " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
            " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
            " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
            " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
            " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
            " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
            " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
            " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
            " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
            " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
            " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
            " 'Iris-versicolor' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
            " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
            " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
            " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
            " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
            " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
            " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
            " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
            " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
            " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
            " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
            " 'Iris-virginica' 'Iris-virginica'] 135 (135,)\n",
            "-------------------y test-----------------------\n",
            "['Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
            " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
            " 'Iris-versicolor' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
            " 'Iris-virginica' 'Iris-virginica'] 15 (15,)\n",
            "-------train-----------------train-----------------train-----------------train-----------------train-----------------train-----------------train-----------------train-----------------train-----------------train----------\n",
            "[  0   1   2   3   4   5   6   7   9  10  11  12  13  14  15  16  17  18\n",
            "  19  20  21  22  23  25  26  27  28  29  30  32  33  34  35  36  37  38\n",
            "  39  40  41  42  43  44  46  47  49  50  51  52  53  54  55  56  57  59\n",
            "  60  61  62  63  64  65  66  67  69  72  73  74  75  76  77  78  79  81\n",
            "  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99\n",
            " 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 118\n",
            " 119 120 121 122 123 124 125 126 127 129 130 131 132 133 134 135 137 139\n",
            " 140 141 142 143 144 146 147 148 149]\n",
            "--------------------X train----------------------\n",
            "[[5.1 3.5 1.4 0.2]\n",
            " [4.9 3.  1.4 0.2]\n",
            " [4.7 3.2 1.3 0.2]\n",
            " [4.6 3.1 1.5 0.2]\n",
            " [5.  3.6 1.4 0.2]\n",
            " [5.4 3.9 1.7 0.4]\n",
            " [4.6 3.4 1.4 0.3]\n",
            " [5.  3.4 1.5 0.2]\n",
            " [4.9 3.1 1.5 0.1]\n",
            " [5.4 3.7 1.5 0.2]\n",
            " [4.8 3.4 1.6 0.2]\n",
            " [4.8 3.  1.4 0.1]\n",
            " [4.3 3.  1.1 0.1]\n",
            " [5.8 4.  1.2 0.2]\n",
            " [5.7 4.4 1.5 0.4]\n",
            " [5.4 3.9 1.3 0.4]\n",
            " [5.1 3.5 1.4 0.3]\n",
            " [5.7 3.8 1.7 0.3]\n",
            " [5.1 3.8 1.5 0.3]\n",
            " [5.4 3.4 1.7 0.2]\n",
            " [5.1 3.7 1.5 0.4]\n",
            " [4.6 3.6 1.  0.2]\n",
            " [5.1 3.3 1.7 0.5]\n",
            " [5.  3.  1.6 0.2]\n",
            " [5.  3.4 1.6 0.4]\n",
            " [5.2 3.5 1.5 0.2]\n",
            " [5.2 3.4 1.4 0.2]\n",
            " [4.7 3.2 1.6 0.2]\n",
            " [4.8 3.1 1.6 0.2]\n",
            " [5.2 4.1 1.5 0.1]\n",
            " [5.5 4.2 1.4 0.2]\n",
            " [4.9 3.1 1.5 0.1]\n",
            " [5.  3.2 1.2 0.2]\n",
            " [5.5 3.5 1.3 0.2]\n",
            " [4.9 3.1 1.5 0.1]\n",
            " [4.4 3.  1.3 0.2]\n",
            " [5.1 3.4 1.5 0.2]\n",
            " [5.  3.5 1.3 0.3]\n",
            " [4.5 2.3 1.3 0.3]\n",
            " [4.4 3.2 1.3 0.2]\n",
            " [5.  3.5 1.6 0.6]\n",
            " [5.1 3.8 1.9 0.4]\n",
            " [5.1 3.8 1.6 0.2]\n",
            " [4.6 3.2 1.4 0.2]\n",
            " [5.  3.3 1.4 0.2]\n",
            " [7.  3.2 4.7 1.4]\n",
            " [6.4 3.2 4.5 1.5]\n",
            " [6.9 3.1 4.9 1.5]\n",
            " [5.5 2.3 4.  1.3]\n",
            " [6.5 2.8 4.6 1.5]\n",
            " [5.7 2.8 4.5 1.3]\n",
            " [6.3 3.3 4.7 1.6]\n",
            " [4.9 2.4 3.3 1. ]\n",
            " [5.2 2.7 3.9 1.4]\n",
            " [5.  2.  3.5 1. ]\n",
            " [5.9 3.  4.2 1.5]\n",
            " [6.  2.2 4.  1. ]\n",
            " [6.1 2.9 4.7 1.4]\n",
            " [5.6 2.9 3.6 1.3]\n",
            " [6.7 3.1 4.4 1.4]\n",
            " [5.6 3.  4.5 1.5]\n",
            " [5.8 2.7 4.1 1. ]\n",
            " [5.6 2.5 3.9 1.1]\n",
            " [6.3 2.5 4.9 1.5]\n",
            " [6.1 2.8 4.7 1.2]\n",
            " [6.4 2.9 4.3 1.3]\n",
            " [6.6 3.  4.4 1.4]\n",
            " [6.8 2.8 4.8 1.4]\n",
            " [6.7 3.  5.  1.7]\n",
            " [6.  2.9 4.5 1.5]\n",
            " [5.7 2.6 3.5 1. ]\n",
            " [5.5 2.4 3.7 1. ]\n",
            " [5.8 2.7 3.9 1.2]\n",
            " [6.  2.7 5.1 1.6]\n",
            " [5.4 3.  4.5 1.5]\n",
            " [6.  3.4 4.5 1.6]\n",
            " [6.7 3.1 4.7 1.5]\n",
            " [6.3 2.3 4.4 1.3]\n",
            " [5.6 3.  4.1 1.3]\n",
            " [5.5 2.5 4.  1.3]\n",
            " [5.5 2.6 4.4 1.2]\n",
            " [6.1 3.  4.6 1.4]\n",
            " [5.8 2.6 4.  1.2]\n",
            " [5.  2.3 3.3 1. ]\n",
            " [5.6 2.7 4.2 1.3]\n",
            " [5.7 3.  4.2 1.2]\n",
            " [5.7 2.9 4.2 1.3]\n",
            " [6.2 2.9 4.3 1.3]\n",
            " [5.1 2.5 3.  1.1]\n",
            " [5.7 2.8 4.1 1.3]\n",
            " [6.3 3.3 6.  2.5]\n",
            " [5.8 2.7 5.1 1.9]\n",
            " [7.1 3.  5.9 2.1]\n",
            " [6.3 2.9 5.6 1.8]\n",
            " [6.5 3.  5.8 2.2]\n",
            " [7.6 3.  6.6 2.1]\n",
            " [4.9 2.5 4.5 1.7]\n",
            " [7.3 2.9 6.3 1.8]\n",
            " [6.7 2.5 5.8 1.8]\n",
            " [7.2 3.6 6.1 2.5]\n",
            " [6.5 3.2 5.1 2. ]\n",
            " [6.4 2.7 5.3 1.9]\n",
            " [6.8 3.  5.5 2.1]\n",
            " [5.7 2.5 5.  2. ]\n",
            " [5.8 2.8 5.1 2.4]\n",
            " [6.4 3.2 5.3 2.3]\n",
            " [6.5 3.  5.5 1.8]\n",
            " [7.7 2.6 6.9 2.3]\n",
            " [6.  2.2 5.  1.5]\n",
            " [6.9 3.2 5.7 2.3]\n",
            " [5.6 2.8 4.9 2. ]\n",
            " [7.7 2.8 6.7 2. ]\n",
            " [6.3 2.7 4.9 1.8]\n",
            " [6.7 3.3 5.7 2.1]\n",
            " [7.2 3.2 6.  1.8]\n",
            " [6.2 2.8 4.8 1.8]\n",
            " [6.1 3.  4.9 1.8]\n",
            " [7.2 3.  5.8 1.6]\n",
            " [7.4 2.8 6.1 1.9]\n",
            " [7.9 3.8 6.4 2. ]\n",
            " [6.4 2.8 5.6 2.2]\n",
            " [6.3 2.8 5.1 1.5]\n",
            " [6.1 2.6 5.6 1.4]\n",
            " [7.7 3.  6.1 2.3]\n",
            " [6.4 3.1 5.5 1.8]\n",
            " [6.9 3.1 5.4 2.1]\n",
            " [6.7 3.1 5.6 2.4]\n",
            " [6.9 3.1 5.1 2.3]\n",
            " [5.8 2.7 5.1 1.9]\n",
            " [6.8 3.2 5.9 2.3]\n",
            " [6.7 3.3 5.7 2.5]\n",
            " [6.3 2.5 5.  1.9]\n",
            " [6.5 3.  5.2 2. ]\n",
            " [6.2 3.4 5.4 2.3]\n",
            " [5.9 3.  5.1 1.8]] 135 (135, 4)\n",
            "-------------------X test-----------------------\n",
            "[[4.4 2.9 1.4 0.2]\n",
            " [4.8 3.4 1.9 0.2]\n",
            " [5.4 3.4 1.5 0.4]\n",
            " [4.8 3.  1.4 0.3]\n",
            " [5.3 3.7 1.5 0.2]\n",
            " [6.6 2.9 4.6 1.3]\n",
            " [6.2 2.2 4.5 1.5]\n",
            " [5.9 3.2 4.8 1.8]\n",
            " [6.1 2.8 4.  1.3]\n",
            " [5.5 2.4 3.8 1.1]\n",
            " [7.7 3.8 6.7 2.2]\n",
            " [6.4 2.8 5.6 2.1]\n",
            " [6.3 3.4 5.6 2.4]\n",
            " [6.  3.  4.8 1.8]\n",
            " [6.7 3.  5.2 2.3]] 15 (15, 4)\n",
            "--------------------y train----------------------\n",
            "['Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
            " 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
            " 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
            " 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
            " 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
            " 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
            " 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
            " 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
            " 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
            " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
            " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
            " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
            " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
            " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
            " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
            " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
            " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
            " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
            " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
            " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
            " 'Iris-versicolor' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
            " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
            " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
            " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
            " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
            " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
            " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
            " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
            " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
            " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
            " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
            " 'Iris-virginica' 'Iris-virginica'] 135 (135,)\n",
            "-------------------y test-----------------------\n",
            "['Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
            " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
            " 'Iris-versicolor' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
            " 'Iris-virginica' 'Iris-virginica'] 15 (15,)\n",
            "-------train-----------------train-----------------train-----------------train-----------------train-----------------train-----------------train-----------------train-----------------train-----------------train----------\n",
            "[  0   1   2   4   5   6   7   8   9  10  11  12  13  14  17  18  19  20\n",
            "  21  22  23  24  25  26  27  28  30  31  33  34  35  36  37  38  39  40\n",
            "  41  42  43  44  45  46  47  48  49  50  51  54  55  56  57  58  59  60\n",
            "  61  62  63  65  66  67  68  69  70  71  72  73  74  75  77  78  79  80\n",
            "  81  82  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99\n",
            " 101 102 103 104 105 106 107 108 110 111 112 113 114 116 117 119 120 121\n",
            " 122 123 124 125 126 127 128 129 130 131 132 133 134 136 137 138 139 140\n",
            " 141 142 143 144 145 146 147 148 149]\n",
            "--------------------X train----------------------\n",
            "[[5.1 3.5 1.4 0.2]\n",
            " [4.9 3.  1.4 0.2]\n",
            " [4.7 3.2 1.3 0.2]\n",
            " [5.  3.6 1.4 0.2]\n",
            " [5.4 3.9 1.7 0.4]\n",
            " [4.6 3.4 1.4 0.3]\n",
            " [5.  3.4 1.5 0.2]\n",
            " [4.4 2.9 1.4 0.2]\n",
            " [4.9 3.1 1.5 0.1]\n",
            " [5.4 3.7 1.5 0.2]\n",
            " [4.8 3.4 1.6 0.2]\n",
            " [4.8 3.  1.4 0.1]\n",
            " [4.3 3.  1.1 0.1]\n",
            " [5.8 4.  1.2 0.2]\n",
            " [5.1 3.5 1.4 0.3]\n",
            " [5.7 3.8 1.7 0.3]\n",
            " [5.1 3.8 1.5 0.3]\n",
            " [5.4 3.4 1.7 0.2]\n",
            " [5.1 3.7 1.5 0.4]\n",
            " [4.6 3.6 1.  0.2]\n",
            " [5.1 3.3 1.7 0.5]\n",
            " [4.8 3.4 1.9 0.2]\n",
            " [5.  3.  1.6 0.2]\n",
            " [5.  3.4 1.6 0.4]\n",
            " [5.2 3.5 1.5 0.2]\n",
            " [5.2 3.4 1.4 0.2]\n",
            " [4.8 3.1 1.6 0.2]\n",
            " [5.4 3.4 1.5 0.4]\n",
            " [5.5 4.2 1.4 0.2]\n",
            " [4.9 3.1 1.5 0.1]\n",
            " [5.  3.2 1.2 0.2]\n",
            " [5.5 3.5 1.3 0.2]\n",
            " [4.9 3.1 1.5 0.1]\n",
            " [4.4 3.  1.3 0.2]\n",
            " [5.1 3.4 1.5 0.2]\n",
            " [5.  3.5 1.3 0.3]\n",
            " [4.5 2.3 1.3 0.3]\n",
            " [4.4 3.2 1.3 0.2]\n",
            " [5.  3.5 1.6 0.6]\n",
            " [5.1 3.8 1.9 0.4]\n",
            " [4.8 3.  1.4 0.3]\n",
            " [5.1 3.8 1.6 0.2]\n",
            " [4.6 3.2 1.4 0.2]\n",
            " [5.3 3.7 1.5 0.2]\n",
            " [5.  3.3 1.4 0.2]\n",
            " [7.  3.2 4.7 1.4]\n",
            " [6.4 3.2 4.5 1.5]\n",
            " [6.5 2.8 4.6 1.5]\n",
            " [5.7 2.8 4.5 1.3]\n",
            " [6.3 3.3 4.7 1.6]\n",
            " [4.9 2.4 3.3 1. ]\n",
            " [6.6 2.9 4.6 1.3]\n",
            " [5.2 2.7 3.9 1.4]\n",
            " [5.  2.  3.5 1. ]\n",
            " [5.9 3.  4.2 1.5]\n",
            " [6.  2.2 4.  1. ]\n",
            " [6.1 2.9 4.7 1.4]\n",
            " [6.7 3.1 4.4 1.4]\n",
            " [5.6 3.  4.5 1.5]\n",
            " [5.8 2.7 4.1 1. ]\n",
            " [6.2 2.2 4.5 1.5]\n",
            " [5.6 2.5 3.9 1.1]\n",
            " [5.9 3.2 4.8 1.8]\n",
            " [6.1 2.8 4.  1.3]\n",
            " [6.3 2.5 4.9 1.5]\n",
            " [6.1 2.8 4.7 1.2]\n",
            " [6.4 2.9 4.3 1.3]\n",
            " [6.6 3.  4.4 1.4]\n",
            " [6.7 3.  5.  1.7]\n",
            " [6.  2.9 4.5 1.5]\n",
            " [5.7 2.6 3.5 1. ]\n",
            " [5.5 2.4 3.8 1.1]\n",
            " [5.5 2.4 3.7 1. ]\n",
            " [5.8 2.7 3.9 1.2]\n",
            " [5.4 3.  4.5 1.5]\n",
            " [6.  3.4 4.5 1.6]\n",
            " [6.7 3.1 4.7 1.5]\n",
            " [6.3 2.3 4.4 1.3]\n",
            " [5.6 3.  4.1 1.3]\n",
            " [5.5 2.5 4.  1.3]\n",
            " [5.5 2.6 4.4 1.2]\n",
            " [6.1 3.  4.6 1.4]\n",
            " [5.8 2.6 4.  1.2]\n",
            " [5.  2.3 3.3 1. ]\n",
            " [5.6 2.7 4.2 1.3]\n",
            " [5.7 3.  4.2 1.2]\n",
            " [5.7 2.9 4.2 1.3]\n",
            " [6.2 2.9 4.3 1.3]\n",
            " [5.1 2.5 3.  1.1]\n",
            " [5.7 2.8 4.1 1.3]\n",
            " [5.8 2.7 5.1 1.9]\n",
            " [7.1 3.  5.9 2.1]\n",
            " [6.3 2.9 5.6 1.8]\n",
            " [6.5 3.  5.8 2.2]\n",
            " [7.6 3.  6.6 2.1]\n",
            " [4.9 2.5 4.5 1.7]\n",
            " [7.3 2.9 6.3 1.8]\n",
            " [6.7 2.5 5.8 1.8]\n",
            " [6.5 3.2 5.1 2. ]\n",
            " [6.4 2.7 5.3 1.9]\n",
            " [6.8 3.  5.5 2.1]\n",
            " [5.7 2.5 5.  2. ]\n",
            " [5.8 2.8 5.1 2.4]\n",
            " [6.5 3.  5.5 1.8]\n",
            " [7.7 3.8 6.7 2.2]\n",
            " [6.  2.2 5.  1.5]\n",
            " [6.9 3.2 5.7 2.3]\n",
            " [5.6 2.8 4.9 2. ]\n",
            " [7.7 2.8 6.7 2. ]\n",
            " [6.3 2.7 4.9 1.8]\n",
            " [6.7 3.3 5.7 2.1]\n",
            " [7.2 3.2 6.  1.8]\n",
            " [6.2 2.8 4.8 1.8]\n",
            " [6.1 3.  4.9 1.8]\n",
            " [6.4 2.8 5.6 2.1]\n",
            " [7.2 3.  5.8 1.6]\n",
            " [7.4 2.8 6.1 1.9]\n",
            " [7.9 3.8 6.4 2. ]\n",
            " [6.4 2.8 5.6 2.2]\n",
            " [6.3 2.8 5.1 1.5]\n",
            " [6.1 2.6 5.6 1.4]\n",
            " [6.3 3.4 5.6 2.4]\n",
            " [6.4 3.1 5.5 1.8]\n",
            " [6.  3.  4.8 1.8]\n",
            " [6.9 3.1 5.4 2.1]\n",
            " [6.7 3.1 5.6 2.4]\n",
            " [6.9 3.1 5.1 2.3]\n",
            " [5.8 2.7 5.1 1.9]\n",
            " [6.8 3.2 5.9 2.3]\n",
            " [6.7 3.3 5.7 2.5]\n",
            " [6.7 3.  5.2 2.3]\n",
            " [6.3 2.5 5.  1.9]\n",
            " [6.5 3.  5.2 2. ]\n",
            " [6.2 3.4 5.4 2.3]\n",
            " [5.9 3.  5.1 1.8]] 135 (135, 4)\n",
            "-------------------X test-----------------------\n",
            "[[4.6 3.1 1.5 0.2]\n",
            " [5.7 4.4 1.5 0.4]\n",
            " [5.4 3.9 1.3 0.4]\n",
            " [4.7 3.2 1.6 0.2]\n",
            " [5.2 4.1 1.5 0.1]\n",
            " [6.9 3.1 4.9 1.5]\n",
            " [5.5 2.3 4.  1.3]\n",
            " [5.6 2.9 3.6 1.3]\n",
            " [6.8 2.8 4.8 1.4]\n",
            " [6.  2.7 5.1 1.6]\n",
            " [6.3 3.3 6.  2.5]\n",
            " [7.2 3.6 6.1 2.5]\n",
            " [6.4 3.2 5.3 2.3]\n",
            " [7.7 2.6 6.9 2.3]\n",
            " [7.7 3.  6.1 2.3]] 15 (15, 4)\n",
            "--------------------y train----------------------\n",
            "['Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
            " 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
            " 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
            " 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
            " 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
            " 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
            " 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
            " 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
            " 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
            " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
            " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
            " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
            " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
            " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
            " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
            " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
            " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
            " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
            " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
            " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
            " 'Iris-versicolor' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
            " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
            " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
            " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
            " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
            " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
            " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
            " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
            " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
            " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
            " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
            " 'Iris-virginica' 'Iris-virginica'] 135 (135,)\n",
            "-------------------y test-----------------------\n",
            "['Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
            " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
            " 'Iris-versicolor' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
            " 'Iris-virginica' 'Iris-virginica'] 15 (15,)\n",
            "-------train-----------------train-----------------train-----------------train-----------------train-----------------train-----------------train-----------------train-----------------train-----------------train----------\n",
            "[  0   1   2   3   4   5   6   8   9  10  11  12  13  14  15  16  17  18\n",
            "  19  20  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38\n",
            "  39  40  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58\n",
            "  59  60  62  63  64  65  66  68  69  70  71  72  73  74  75  76  77  78\n",
            "  79  80  81  83  84  85  87  88  89  90  92  93  94  95  96  97  98  99\n",
            " 100 101 102 103 104 105 106 107 109 110 111 112 113 114 115 116 117 118\n",
            " 119 120 121 122 124 125 126 127 128 129 130 131 132 134 135 136 137 138\n",
            " 139 140 141 142 144 145 146 147 149]\n",
            "--------------------X train----------------------\n",
            "[[5.1 3.5 1.4 0.2]\n",
            " [4.9 3.  1.4 0.2]\n",
            " [4.7 3.2 1.3 0.2]\n",
            " [4.6 3.1 1.5 0.2]\n",
            " [5.  3.6 1.4 0.2]\n",
            " [5.4 3.9 1.7 0.4]\n",
            " [4.6 3.4 1.4 0.3]\n",
            " [4.4 2.9 1.4 0.2]\n",
            " [4.9 3.1 1.5 0.1]\n",
            " [5.4 3.7 1.5 0.2]\n",
            " [4.8 3.4 1.6 0.2]\n",
            " [4.8 3.  1.4 0.1]\n",
            " [4.3 3.  1.1 0.1]\n",
            " [5.8 4.  1.2 0.2]\n",
            " [5.7 4.4 1.5 0.4]\n",
            " [5.4 3.9 1.3 0.4]\n",
            " [5.1 3.5 1.4 0.3]\n",
            " [5.7 3.8 1.7 0.3]\n",
            " [5.1 3.8 1.5 0.3]\n",
            " [5.4 3.4 1.7 0.2]\n",
            " [5.1 3.3 1.7 0.5]\n",
            " [4.8 3.4 1.9 0.2]\n",
            " [5.  3.  1.6 0.2]\n",
            " [5.  3.4 1.6 0.4]\n",
            " [5.2 3.5 1.5 0.2]\n",
            " [5.2 3.4 1.4 0.2]\n",
            " [4.7 3.2 1.6 0.2]\n",
            " [4.8 3.1 1.6 0.2]\n",
            " [5.4 3.4 1.5 0.4]\n",
            " [5.2 4.1 1.5 0.1]\n",
            " [5.5 4.2 1.4 0.2]\n",
            " [4.9 3.1 1.5 0.1]\n",
            " [5.  3.2 1.2 0.2]\n",
            " [5.5 3.5 1.3 0.2]\n",
            " [4.9 3.1 1.5 0.1]\n",
            " [4.4 3.  1.3 0.2]\n",
            " [5.1 3.4 1.5 0.2]\n",
            " [5.  3.5 1.3 0.3]\n",
            " [5.  3.5 1.6 0.6]\n",
            " [5.1 3.8 1.9 0.4]\n",
            " [4.8 3.  1.4 0.3]\n",
            " [5.1 3.8 1.6 0.2]\n",
            " [4.6 3.2 1.4 0.2]\n",
            " [5.3 3.7 1.5 0.2]\n",
            " [5.  3.3 1.4 0.2]\n",
            " [7.  3.2 4.7 1.4]\n",
            " [6.4 3.2 4.5 1.5]\n",
            " [6.9 3.1 4.9 1.5]\n",
            " [5.5 2.3 4.  1.3]\n",
            " [6.5 2.8 4.6 1.5]\n",
            " [5.7 2.8 4.5 1.3]\n",
            " [6.3 3.3 4.7 1.6]\n",
            " [4.9 2.4 3.3 1. ]\n",
            " [6.6 2.9 4.6 1.3]\n",
            " [5.2 2.7 3.9 1.4]\n",
            " [5.  2.  3.5 1. ]\n",
            " [6.  2.2 4.  1. ]\n",
            " [6.1 2.9 4.7 1.4]\n",
            " [5.6 2.9 3.6 1.3]\n",
            " [6.7 3.1 4.4 1.4]\n",
            " [5.6 3.  4.5 1.5]\n",
            " [6.2 2.2 4.5 1.5]\n",
            " [5.6 2.5 3.9 1.1]\n",
            " [5.9 3.2 4.8 1.8]\n",
            " [6.1 2.8 4.  1.3]\n",
            " [6.3 2.5 4.9 1.5]\n",
            " [6.1 2.8 4.7 1.2]\n",
            " [6.4 2.9 4.3 1.3]\n",
            " [6.6 3.  4.4 1.4]\n",
            " [6.8 2.8 4.8 1.4]\n",
            " [6.7 3.  5.  1.7]\n",
            " [6.  2.9 4.5 1.5]\n",
            " [5.7 2.6 3.5 1. ]\n",
            " [5.5 2.4 3.8 1.1]\n",
            " [5.5 2.4 3.7 1. ]\n",
            " [6.  2.7 5.1 1.6]\n",
            " [5.4 3.  4.5 1.5]\n",
            " [6.  3.4 4.5 1.6]\n",
            " [6.3 2.3 4.4 1.3]\n",
            " [5.6 3.  4.1 1.3]\n",
            " [5.5 2.5 4.  1.3]\n",
            " [5.5 2.6 4.4 1.2]\n",
            " [5.8 2.6 4.  1.2]\n",
            " [5.  2.3 3.3 1. ]\n",
            " [5.6 2.7 4.2 1.3]\n",
            " [5.7 3.  4.2 1.2]\n",
            " [5.7 2.9 4.2 1.3]\n",
            " [6.2 2.9 4.3 1.3]\n",
            " [5.1 2.5 3.  1.1]\n",
            " [5.7 2.8 4.1 1.3]\n",
            " [6.3 3.3 6.  2.5]\n",
            " [5.8 2.7 5.1 1.9]\n",
            " [7.1 3.  5.9 2.1]\n",
            " [6.3 2.9 5.6 1.8]\n",
            " [6.5 3.  5.8 2.2]\n",
            " [7.6 3.  6.6 2.1]\n",
            " [4.9 2.5 4.5 1.7]\n",
            " [7.3 2.9 6.3 1.8]\n",
            " [7.2 3.6 6.1 2.5]\n",
            " [6.5 3.2 5.1 2. ]\n",
            " [6.4 2.7 5.3 1.9]\n",
            " [6.8 3.  5.5 2.1]\n",
            " [5.7 2.5 5.  2. ]\n",
            " [5.8 2.8 5.1 2.4]\n",
            " [6.4 3.2 5.3 2.3]\n",
            " [6.5 3.  5.5 1.8]\n",
            " [7.7 3.8 6.7 2.2]\n",
            " [7.7 2.6 6.9 2.3]\n",
            " [6.  2.2 5.  1.5]\n",
            " [6.9 3.2 5.7 2.3]\n",
            " [5.6 2.8 4.9 2. ]\n",
            " [7.7 2.8 6.7 2. ]\n",
            " [6.7 3.3 5.7 2.1]\n",
            " [7.2 3.2 6.  1.8]\n",
            " [6.2 2.8 4.8 1.8]\n",
            " [6.1 3.  4.9 1.8]\n",
            " [6.4 2.8 5.6 2.1]\n",
            " [7.2 3.  5.8 1.6]\n",
            " [7.4 2.8 6.1 1.9]\n",
            " [7.9 3.8 6.4 2. ]\n",
            " [6.4 2.8 5.6 2.2]\n",
            " [6.1 2.6 5.6 1.4]\n",
            " [7.7 3.  6.1 2.3]\n",
            " [6.3 3.4 5.6 2.4]\n",
            " [6.4 3.1 5.5 1.8]\n",
            " [6.  3.  4.8 1.8]\n",
            " [6.9 3.1 5.4 2.1]\n",
            " [6.7 3.1 5.6 2.4]\n",
            " [6.9 3.1 5.1 2.3]\n",
            " [5.8 2.7 5.1 1.9]\n",
            " [6.7 3.3 5.7 2.5]\n",
            " [6.7 3.  5.2 2.3]\n",
            " [6.3 2.5 5.  1.9]\n",
            " [6.5 3.  5.2 2. ]\n",
            " [5.9 3.  5.1 1.8]] 135 (135, 4)\n",
            "-------------------X test-----------------------\n",
            "[[5.  3.4 1.5 0.2]\n",
            " [5.1 3.7 1.5 0.4]\n",
            " [4.6 3.6 1.  0.2]\n",
            " [4.5 2.3 1.3 0.3]\n",
            " [4.4 3.2 1.3 0.2]\n",
            " [5.9 3.  4.2 1.5]\n",
            " [5.8 2.7 4.1 1. ]\n",
            " [5.8 2.7 3.9 1.2]\n",
            " [6.7 3.1 4.7 1.5]\n",
            " [6.1 3.  4.6 1.4]\n",
            " [6.7 2.5 5.8 1.8]\n",
            " [6.3 2.7 4.9 1.8]\n",
            " [6.3 2.8 5.1 1.5]\n",
            " [6.8 3.2 5.9 2.3]\n",
            " [6.2 3.4 5.4 2.3]] 15 (15, 4)\n",
            "--------------------y train----------------------\n",
            "['Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
            " 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
            " 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
            " 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
            " 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
            " 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
            " 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
            " 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
            " 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
            " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
            " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
            " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
            " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
            " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
            " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
            " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
            " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
            " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
            " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
            " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
            " 'Iris-versicolor' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
            " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
            " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
            " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
            " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
            " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
            " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
            " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
            " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
            " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
            " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
            " 'Iris-virginica' 'Iris-virginica'] 135 (135,)\n",
            "-------------------y test-----------------------\n",
            "['Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
            " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
            " 'Iris-versicolor' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
            " 'Iris-virginica' 'Iris-virginica'] 15 (15,)\n",
            "-------train-----------------train-----------------train-----------------train-----------------train-----------------train-----------------train-----------------train-----------------train-----------------train----------\n",
            "[  0   3   4   5   6   7   8   9  10  11  12  13  15  16  18  19  20  21\n",
            "  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39\n",
            "  40  41  42  43  44  45  47  48  49  50  51  52  53  54  55  57  58  59\n",
            "  60  61  62  63  64  65  67  68  69  70  71  72  73  74  75  76  78  79\n",
            "  80  81  82  83  84  85  86  87  88  89  90  91  92  93  94  96  98  99\n",
            " 100 101 102 104 105 106 107 108 109 110 112 113 114 115 116 117 118 119\n",
            " 120 121 122 123 124 125 126 127 128 129 130 132 133 134 135 136 137 138\n",
            " 140 141 142 143 144 145 146 148 149]\n",
            "--------------------X train----------------------\n",
            "[[5.1 3.5 1.4 0.2]\n",
            " [4.6 3.1 1.5 0.2]\n",
            " [5.  3.6 1.4 0.2]\n",
            " [5.4 3.9 1.7 0.4]\n",
            " [4.6 3.4 1.4 0.3]\n",
            " [5.  3.4 1.5 0.2]\n",
            " [4.4 2.9 1.4 0.2]\n",
            " [4.9 3.1 1.5 0.1]\n",
            " [5.4 3.7 1.5 0.2]\n",
            " [4.8 3.4 1.6 0.2]\n",
            " [4.8 3.  1.4 0.1]\n",
            " [4.3 3.  1.1 0.1]\n",
            " [5.7 4.4 1.5 0.4]\n",
            " [5.4 3.9 1.3 0.4]\n",
            " [5.7 3.8 1.7 0.3]\n",
            " [5.1 3.8 1.5 0.3]\n",
            " [5.4 3.4 1.7 0.2]\n",
            " [5.1 3.7 1.5 0.4]\n",
            " [4.6 3.6 1.  0.2]\n",
            " [5.1 3.3 1.7 0.5]\n",
            " [4.8 3.4 1.9 0.2]\n",
            " [5.  3.  1.6 0.2]\n",
            " [5.  3.4 1.6 0.4]\n",
            " [5.2 3.5 1.5 0.2]\n",
            " [5.2 3.4 1.4 0.2]\n",
            " [4.7 3.2 1.6 0.2]\n",
            " [4.8 3.1 1.6 0.2]\n",
            " [5.4 3.4 1.5 0.4]\n",
            " [5.2 4.1 1.5 0.1]\n",
            " [5.5 4.2 1.4 0.2]\n",
            " [4.9 3.1 1.5 0.1]\n",
            " [5.  3.2 1.2 0.2]\n",
            " [5.5 3.5 1.3 0.2]\n",
            " [4.9 3.1 1.5 0.1]\n",
            " [4.4 3.  1.3 0.2]\n",
            " [5.1 3.4 1.5 0.2]\n",
            " [5.  3.5 1.3 0.3]\n",
            " [4.5 2.3 1.3 0.3]\n",
            " [4.4 3.2 1.3 0.2]\n",
            " [5.  3.5 1.6 0.6]\n",
            " [5.1 3.8 1.9 0.4]\n",
            " [4.8 3.  1.4 0.3]\n",
            " [4.6 3.2 1.4 0.2]\n",
            " [5.3 3.7 1.5 0.2]\n",
            " [5.  3.3 1.4 0.2]\n",
            " [7.  3.2 4.7 1.4]\n",
            " [6.4 3.2 4.5 1.5]\n",
            " [6.9 3.1 4.9 1.5]\n",
            " [5.5 2.3 4.  1.3]\n",
            " [6.5 2.8 4.6 1.5]\n",
            " [5.7 2.8 4.5 1.3]\n",
            " [4.9 2.4 3.3 1. ]\n",
            " [6.6 2.9 4.6 1.3]\n",
            " [5.2 2.7 3.9 1.4]\n",
            " [5.  2.  3.5 1. ]\n",
            " [5.9 3.  4.2 1.5]\n",
            " [6.  2.2 4.  1. ]\n",
            " [6.1 2.9 4.7 1.4]\n",
            " [5.6 2.9 3.6 1.3]\n",
            " [6.7 3.1 4.4 1.4]\n",
            " [5.8 2.7 4.1 1. ]\n",
            " [6.2 2.2 4.5 1.5]\n",
            " [5.6 2.5 3.9 1.1]\n",
            " [5.9 3.2 4.8 1.8]\n",
            " [6.1 2.8 4.  1.3]\n",
            " [6.3 2.5 4.9 1.5]\n",
            " [6.1 2.8 4.7 1.2]\n",
            " [6.4 2.9 4.3 1.3]\n",
            " [6.6 3.  4.4 1.4]\n",
            " [6.8 2.8 4.8 1.4]\n",
            " [6.  2.9 4.5 1.5]\n",
            " [5.7 2.6 3.5 1. ]\n",
            " [5.5 2.4 3.8 1.1]\n",
            " [5.5 2.4 3.7 1. ]\n",
            " [5.8 2.7 3.9 1.2]\n",
            " [6.  2.7 5.1 1.6]\n",
            " [5.4 3.  4.5 1.5]\n",
            " [6.  3.4 4.5 1.6]\n",
            " [6.7 3.1 4.7 1.5]\n",
            " [6.3 2.3 4.4 1.3]\n",
            " [5.6 3.  4.1 1.3]\n",
            " [5.5 2.5 4.  1.3]\n",
            " [5.5 2.6 4.4 1.2]\n",
            " [6.1 3.  4.6 1.4]\n",
            " [5.8 2.6 4.  1.2]\n",
            " [5.  2.3 3.3 1. ]\n",
            " [5.6 2.7 4.2 1.3]\n",
            " [5.7 2.9 4.2 1.3]\n",
            " [5.1 2.5 3.  1.1]\n",
            " [5.7 2.8 4.1 1.3]\n",
            " [6.3 3.3 6.  2.5]\n",
            " [5.8 2.7 5.1 1.9]\n",
            " [7.1 3.  5.9 2.1]\n",
            " [6.5 3.  5.8 2.2]\n",
            " [7.6 3.  6.6 2.1]\n",
            " [4.9 2.5 4.5 1.7]\n",
            " [7.3 2.9 6.3 1.8]\n",
            " [6.7 2.5 5.8 1.8]\n",
            " [7.2 3.6 6.1 2.5]\n",
            " [6.5 3.2 5.1 2. ]\n",
            " [6.8 3.  5.5 2.1]\n",
            " [5.7 2.5 5.  2. ]\n",
            " [5.8 2.8 5.1 2.4]\n",
            " [6.4 3.2 5.3 2.3]\n",
            " [6.5 3.  5.5 1.8]\n",
            " [7.7 3.8 6.7 2.2]\n",
            " [7.7 2.6 6.9 2.3]\n",
            " [6.  2.2 5.  1.5]\n",
            " [6.9 3.2 5.7 2.3]\n",
            " [5.6 2.8 4.9 2. ]\n",
            " [7.7 2.8 6.7 2. ]\n",
            " [6.3 2.7 4.9 1.8]\n",
            " [6.7 3.3 5.7 2.1]\n",
            " [7.2 3.2 6.  1.8]\n",
            " [6.2 2.8 4.8 1.8]\n",
            " [6.1 3.  4.9 1.8]\n",
            " [6.4 2.8 5.6 2.1]\n",
            " [7.2 3.  5.8 1.6]\n",
            " [7.4 2.8 6.1 1.9]\n",
            " [6.4 2.8 5.6 2.2]\n",
            " [6.3 2.8 5.1 1.5]\n",
            " [6.1 2.6 5.6 1.4]\n",
            " [7.7 3.  6.1 2.3]\n",
            " [6.3 3.4 5.6 2.4]\n",
            " [6.4 3.1 5.5 1.8]\n",
            " [6.  3.  4.8 1.8]\n",
            " [6.7 3.1 5.6 2.4]\n",
            " [6.9 3.1 5.1 2.3]\n",
            " [5.8 2.7 5.1 1.9]\n",
            " [6.8 3.2 5.9 2.3]\n",
            " [6.7 3.3 5.7 2.5]\n",
            " [6.7 3.  5.2 2.3]\n",
            " [6.3 2.5 5.  1.9]\n",
            " [6.2 3.4 5.4 2.3]\n",
            " [5.9 3.  5.1 1.8]] 135 (135, 4)\n",
            "-------------------X test-----------------------\n",
            "[[4.9 3.  1.4 0.2]\n",
            " [4.7 3.2 1.3 0.2]\n",
            " [5.8 4.  1.2 0.2]\n",
            " [5.1 3.5 1.4 0.3]\n",
            " [5.1 3.8 1.6 0.2]\n",
            " [6.3 3.3 4.7 1.6]\n",
            " [5.6 3.  4.5 1.5]\n",
            " [6.7 3.  5.  1.7]\n",
            " [5.7 3.  4.2 1.2]\n",
            " [6.2 2.9 4.3 1.3]\n",
            " [6.3 2.9 5.6 1.8]\n",
            " [6.4 2.7 5.3 1.9]\n",
            " [7.9 3.8 6.4 2. ]\n",
            " [6.9 3.1 5.4 2.1]\n",
            " [6.5 3.  5.2 2. ]] 15 (15, 4)\n",
            "--------------------y train----------------------\n",
            "['Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
            " 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
            " 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
            " 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
            " 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
            " 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
            " 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
            " 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
            " 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
            " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
            " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
            " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
            " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
            " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
            " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
            " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
            " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
            " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
            " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
            " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
            " 'Iris-versicolor' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
            " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
            " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
            " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
            " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
            " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
            " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
            " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
            " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
            " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
            " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
            " 'Iris-virginica' 'Iris-virginica'] 135 (135,)\n",
            "-------------------y test-----------------------\n",
            "['Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
            " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
            " 'Iris-versicolor' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
            " 'Iris-virginica' 'Iris-virginica'] 15 (15,)\n",
            "-------train-----------------train-----------------train-----------------train-----------------train-----------------train-----------------train-----------------train-----------------train-----------------train----------\n",
            "[  0   1   2   3   5   6   7   8  10  11  12  13  14  15  16  17  18  19\n",
            "  20  21  22  23  24  25  26  27  28  29  31  32  33  34  36  37  38  40\n",
            "  41  42  43  44  45  46  47  48  49  50  51  52  53  55  56  57  58  59\n",
            "  60  61  62  64  65  66  67  68  69  70  71  72  73  75  76  77  79  80\n",
            "  81  82  83  84  85  86  87  88  89  90  91  92  93  95  96  97  98  99\n",
            " 100 101 103 104 106 107 108 109 110 111 112 113 115 116 117 118 119 120\n",
            " 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138\n",
            " 139 141 142 143 144 145 147 148 149]\n",
            "--------------------X train----------------------\n",
            "[[5.1 3.5 1.4 0.2]\n",
            " [4.9 3.  1.4 0.2]\n",
            " [4.7 3.2 1.3 0.2]\n",
            " [4.6 3.1 1.5 0.2]\n",
            " [5.4 3.9 1.7 0.4]\n",
            " [4.6 3.4 1.4 0.3]\n",
            " [5.  3.4 1.5 0.2]\n",
            " [4.4 2.9 1.4 0.2]\n",
            " [5.4 3.7 1.5 0.2]\n",
            " [4.8 3.4 1.6 0.2]\n",
            " [4.8 3.  1.4 0.1]\n",
            " [4.3 3.  1.1 0.1]\n",
            " [5.8 4.  1.2 0.2]\n",
            " [5.7 4.4 1.5 0.4]\n",
            " [5.4 3.9 1.3 0.4]\n",
            " [5.1 3.5 1.4 0.3]\n",
            " [5.7 3.8 1.7 0.3]\n",
            " [5.1 3.8 1.5 0.3]\n",
            " [5.4 3.4 1.7 0.2]\n",
            " [5.1 3.7 1.5 0.4]\n",
            " [4.6 3.6 1.  0.2]\n",
            " [5.1 3.3 1.7 0.5]\n",
            " [4.8 3.4 1.9 0.2]\n",
            " [5.  3.  1.6 0.2]\n",
            " [5.  3.4 1.6 0.4]\n",
            " [5.2 3.5 1.5 0.2]\n",
            " [5.2 3.4 1.4 0.2]\n",
            " [4.7 3.2 1.6 0.2]\n",
            " [5.4 3.4 1.5 0.4]\n",
            " [5.2 4.1 1.5 0.1]\n",
            " [5.5 4.2 1.4 0.2]\n",
            " [4.9 3.1 1.5 0.1]\n",
            " [5.5 3.5 1.3 0.2]\n",
            " [4.9 3.1 1.5 0.1]\n",
            " [4.4 3.  1.3 0.2]\n",
            " [5.  3.5 1.3 0.3]\n",
            " [4.5 2.3 1.3 0.3]\n",
            " [4.4 3.2 1.3 0.2]\n",
            " [5.  3.5 1.6 0.6]\n",
            " [5.1 3.8 1.9 0.4]\n",
            " [4.8 3.  1.4 0.3]\n",
            " [5.1 3.8 1.6 0.2]\n",
            " [4.6 3.2 1.4 0.2]\n",
            " [5.3 3.7 1.5 0.2]\n",
            " [5.  3.3 1.4 0.2]\n",
            " [7.  3.2 4.7 1.4]\n",
            " [6.4 3.2 4.5 1.5]\n",
            " [6.9 3.1 4.9 1.5]\n",
            " [5.5 2.3 4.  1.3]\n",
            " [5.7 2.8 4.5 1.3]\n",
            " [6.3 3.3 4.7 1.6]\n",
            " [4.9 2.4 3.3 1. ]\n",
            " [6.6 2.9 4.6 1.3]\n",
            " [5.2 2.7 3.9 1.4]\n",
            " [5.  2.  3.5 1. ]\n",
            " [5.9 3.  4.2 1.5]\n",
            " [6.  2.2 4.  1. ]\n",
            " [5.6 2.9 3.6 1.3]\n",
            " [6.7 3.1 4.4 1.4]\n",
            " [5.6 3.  4.5 1.5]\n",
            " [5.8 2.7 4.1 1. ]\n",
            " [6.2 2.2 4.5 1.5]\n",
            " [5.6 2.5 3.9 1.1]\n",
            " [5.9 3.2 4.8 1.8]\n",
            " [6.1 2.8 4.  1.3]\n",
            " [6.3 2.5 4.9 1.5]\n",
            " [6.1 2.8 4.7 1.2]\n",
            " [6.6 3.  4.4 1.4]\n",
            " [6.8 2.8 4.8 1.4]\n",
            " [6.7 3.  5.  1.7]\n",
            " [5.7 2.6 3.5 1. ]\n",
            " [5.5 2.4 3.8 1.1]\n",
            " [5.5 2.4 3.7 1. ]\n",
            " [5.8 2.7 3.9 1.2]\n",
            " [6.  2.7 5.1 1.6]\n",
            " [5.4 3.  4.5 1.5]\n",
            " [6.  3.4 4.5 1.6]\n",
            " [6.7 3.1 4.7 1.5]\n",
            " [6.3 2.3 4.4 1.3]\n",
            " [5.6 3.  4.1 1.3]\n",
            " [5.5 2.5 4.  1.3]\n",
            " [5.5 2.6 4.4 1.2]\n",
            " [6.1 3.  4.6 1.4]\n",
            " [5.8 2.6 4.  1.2]\n",
            " [5.  2.3 3.3 1. ]\n",
            " [5.7 3.  4.2 1.2]\n",
            " [5.7 2.9 4.2 1.3]\n",
            " [6.2 2.9 4.3 1.3]\n",
            " [5.1 2.5 3.  1.1]\n",
            " [5.7 2.8 4.1 1.3]\n",
            " [6.3 3.3 6.  2.5]\n",
            " [5.8 2.7 5.1 1.9]\n",
            " [6.3 2.9 5.6 1.8]\n",
            " [6.5 3.  5.8 2.2]\n",
            " [4.9 2.5 4.5 1.7]\n",
            " [7.3 2.9 6.3 1.8]\n",
            " [6.7 2.5 5.8 1.8]\n",
            " [7.2 3.6 6.1 2.5]\n",
            " [6.5 3.2 5.1 2. ]\n",
            " [6.4 2.7 5.3 1.9]\n",
            " [6.8 3.  5.5 2.1]\n",
            " [5.7 2.5 5.  2. ]\n",
            " [6.4 3.2 5.3 2.3]\n",
            " [6.5 3.  5.5 1.8]\n",
            " [7.7 3.8 6.7 2.2]\n",
            " [7.7 2.6 6.9 2.3]\n",
            " [6.  2.2 5.  1.5]\n",
            " [6.9 3.2 5.7 2.3]\n",
            " [5.6 2.8 4.9 2. ]\n",
            " [7.7 2.8 6.7 2. ]\n",
            " [6.3 2.7 4.9 1.8]\n",
            " [6.7 3.3 5.7 2.1]\n",
            " [7.2 3.2 6.  1.8]\n",
            " [6.2 2.8 4.8 1.8]\n",
            " [6.1 3.  4.9 1.8]\n",
            " [6.4 2.8 5.6 2.1]\n",
            " [7.2 3.  5.8 1.6]\n",
            " [7.4 2.8 6.1 1.9]\n",
            " [7.9 3.8 6.4 2. ]\n",
            " [6.4 2.8 5.6 2.2]\n",
            " [6.3 2.8 5.1 1.5]\n",
            " [6.1 2.6 5.6 1.4]\n",
            " [7.7 3.  6.1 2.3]\n",
            " [6.3 3.4 5.6 2.4]\n",
            " [6.4 3.1 5.5 1.8]\n",
            " [6.  3.  4.8 1.8]\n",
            " [6.9 3.1 5.4 2.1]\n",
            " [6.9 3.1 5.1 2.3]\n",
            " [5.8 2.7 5.1 1.9]\n",
            " [6.8 3.2 5.9 2.3]\n",
            " [6.7 3.3 5.7 2.5]\n",
            " [6.7 3.  5.2 2.3]\n",
            " [6.5 3.  5.2 2. ]\n",
            " [6.2 3.4 5.4 2.3]\n",
            " [5.9 3.  5.1 1.8]] 135 (135, 4)\n",
            "-------------------X test-----------------------\n",
            "[[5.  3.6 1.4 0.2]\n",
            " [4.9 3.1 1.5 0.1]\n",
            " [4.8 3.1 1.6 0.2]\n",
            " [5.  3.2 1.2 0.2]\n",
            " [5.1 3.4 1.5 0.2]\n",
            " [6.5 2.8 4.6 1.5]\n",
            " [6.1 2.9 4.7 1.4]\n",
            " [6.4 2.9 4.3 1.3]\n",
            " [6.  2.9 4.5 1.5]\n",
            " [5.6 2.7 4.2 1.3]\n",
            " [7.1 3.  5.9 2.1]\n",
            " [7.6 3.  6.6 2.1]\n",
            " [5.8 2.8 5.1 2.4]\n",
            " [6.7 3.1 5.6 2.4]\n",
            " [6.3 2.5 5.  1.9]] 15 (15, 4)\n",
            "--------------------y train----------------------\n",
            "['Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
            " 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
            " 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
            " 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
            " 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
            " 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
            " 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
            " 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
            " 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
            " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
            " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
            " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
            " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
            " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
            " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
            " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
            " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
            " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
            " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
            " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
            " 'Iris-versicolor' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
            " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
            " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
            " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
            " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
            " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
            " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
            " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
            " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
            " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
            " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
            " 'Iris-virginica' 'Iris-virginica'] 135 (135,)\n",
            "-------------------y test-----------------------\n",
            "['Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
            " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
            " 'Iris-versicolor' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
            " 'Iris-virginica' 'Iris-virginica'] 15 (15,)\n",
            "-------train-----------------train-----------------train-----------------train-----------------train-----------------train-----------------train-----------------train-----------------train-----------------train----------\n",
            "[  0   1   2   3   4   6   7   8   9  10  11  12  14  15  16  17  18  19\n",
            "  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  38  39  40\n",
            "  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  58  59\n",
            "  60  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77\n",
            "  78  79  80  81  82  83  84  86  87  88  89  90  91  92  93  94  95  97\n",
            " 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 117 118\n",
            " 119 121 122 123 124 125 126 128 130 131 133 134 135 136 137 138 139 140\n",
            " 141 142 143 144 145 146 147 148 149]\n",
            "--------------------X train----------------------\n",
            "[[5.1 3.5 1.4 0.2]\n",
            " [4.9 3.  1.4 0.2]\n",
            " [4.7 3.2 1.3 0.2]\n",
            " [4.6 3.1 1.5 0.2]\n",
            " [5.  3.6 1.4 0.2]\n",
            " [4.6 3.4 1.4 0.3]\n",
            " [5.  3.4 1.5 0.2]\n",
            " [4.4 2.9 1.4 0.2]\n",
            " [4.9 3.1 1.5 0.1]\n",
            " [5.4 3.7 1.5 0.2]\n",
            " [4.8 3.4 1.6 0.2]\n",
            " [4.8 3.  1.4 0.1]\n",
            " [5.8 4.  1.2 0.2]\n",
            " [5.7 4.4 1.5 0.4]\n",
            " [5.4 3.9 1.3 0.4]\n",
            " [5.1 3.5 1.4 0.3]\n",
            " [5.7 3.8 1.7 0.3]\n",
            " [5.1 3.8 1.5 0.3]\n",
            " [5.1 3.7 1.5 0.4]\n",
            " [4.6 3.6 1.  0.2]\n",
            " [5.1 3.3 1.7 0.5]\n",
            " [4.8 3.4 1.9 0.2]\n",
            " [5.  3.  1.6 0.2]\n",
            " [5.  3.4 1.6 0.4]\n",
            " [5.2 3.5 1.5 0.2]\n",
            " [5.2 3.4 1.4 0.2]\n",
            " [4.7 3.2 1.6 0.2]\n",
            " [4.8 3.1 1.6 0.2]\n",
            " [5.4 3.4 1.5 0.4]\n",
            " [5.2 4.1 1.5 0.1]\n",
            " [5.5 4.2 1.4 0.2]\n",
            " [4.9 3.1 1.5 0.1]\n",
            " [5.  3.2 1.2 0.2]\n",
            " [4.4 3.  1.3 0.2]\n",
            " [5.1 3.4 1.5 0.2]\n",
            " [5.  3.5 1.3 0.3]\n",
            " [4.5 2.3 1.3 0.3]\n",
            " [4.4 3.2 1.3 0.2]\n",
            " [5.  3.5 1.6 0.6]\n",
            " [5.1 3.8 1.9 0.4]\n",
            " [4.8 3.  1.4 0.3]\n",
            " [5.1 3.8 1.6 0.2]\n",
            " [4.6 3.2 1.4 0.2]\n",
            " [5.3 3.7 1.5 0.2]\n",
            " [5.  3.3 1.4 0.2]\n",
            " [7.  3.2 4.7 1.4]\n",
            " [6.4 3.2 4.5 1.5]\n",
            " [6.9 3.1 4.9 1.5]\n",
            " [5.5 2.3 4.  1.3]\n",
            " [6.5 2.8 4.6 1.5]\n",
            " [5.7 2.8 4.5 1.3]\n",
            " [6.3 3.3 4.7 1.6]\n",
            " [6.6 2.9 4.6 1.3]\n",
            " [5.2 2.7 3.9 1.4]\n",
            " [5.  2.  3.5 1. ]\n",
            " [5.9 3.  4.2 1.5]\n",
            " [6.  2.2 4.  1. ]\n",
            " [6.1 2.9 4.7 1.4]\n",
            " [5.6 2.9 3.6 1.3]\n",
            " [6.7 3.1 4.4 1.4]\n",
            " [5.6 3.  4.5 1.5]\n",
            " [5.8 2.7 4.1 1. ]\n",
            " [6.2 2.2 4.5 1.5]\n",
            " [5.6 2.5 3.9 1.1]\n",
            " [5.9 3.2 4.8 1.8]\n",
            " [6.1 2.8 4.  1.3]\n",
            " [6.3 2.5 4.9 1.5]\n",
            " [6.1 2.8 4.7 1.2]\n",
            " [6.4 2.9 4.3 1.3]\n",
            " [6.6 3.  4.4 1.4]\n",
            " [6.8 2.8 4.8 1.4]\n",
            " [6.7 3.  5.  1.7]\n",
            " [6.  2.9 4.5 1.5]\n",
            " [5.7 2.6 3.5 1. ]\n",
            " [5.5 2.4 3.8 1.1]\n",
            " [5.5 2.4 3.7 1. ]\n",
            " [5.8 2.7 3.9 1.2]\n",
            " [6.  2.7 5.1 1.6]\n",
            " [5.4 3.  4.5 1.5]\n",
            " [6.7 3.1 4.7 1.5]\n",
            " [6.3 2.3 4.4 1.3]\n",
            " [5.6 3.  4.1 1.3]\n",
            " [5.5 2.5 4.  1.3]\n",
            " [5.5 2.6 4.4 1.2]\n",
            " [6.1 3.  4.6 1.4]\n",
            " [5.8 2.6 4.  1.2]\n",
            " [5.  2.3 3.3 1. ]\n",
            " [5.6 2.7 4.2 1.3]\n",
            " [5.7 3.  4.2 1.2]\n",
            " [6.2 2.9 4.3 1.3]\n",
            " [6.3 3.3 6.  2.5]\n",
            " [5.8 2.7 5.1 1.9]\n",
            " [7.1 3.  5.9 2.1]\n",
            " [6.3 2.9 5.6 1.8]\n",
            " [6.5 3.  5.8 2.2]\n",
            " [7.6 3.  6.6 2.1]\n",
            " [4.9 2.5 4.5 1.7]\n",
            " [7.3 2.9 6.3 1.8]\n",
            " [6.7 2.5 5.8 1.8]\n",
            " [7.2 3.6 6.1 2.5]\n",
            " [6.5 3.2 5.1 2. ]\n",
            " [6.4 2.7 5.3 1.9]\n",
            " [6.8 3.  5.5 2.1]\n",
            " [5.7 2.5 5.  2. ]\n",
            " [5.8 2.8 5.1 2.4]\n",
            " [6.4 3.2 5.3 2.3]\n",
            " [7.7 3.8 6.7 2.2]\n",
            " [7.7 2.6 6.9 2.3]\n",
            " [6.  2.2 5.  1.5]\n",
            " [5.6 2.8 4.9 2. ]\n",
            " [7.7 2.8 6.7 2. ]\n",
            " [6.3 2.7 4.9 1.8]\n",
            " [6.7 3.3 5.7 2.1]\n",
            " [7.2 3.2 6.  1.8]\n",
            " [6.2 2.8 4.8 1.8]\n",
            " [6.4 2.8 5.6 2.1]\n",
            " [7.4 2.8 6.1 1.9]\n",
            " [7.9 3.8 6.4 2. ]\n",
            " [6.3 2.8 5.1 1.5]\n",
            " [6.1 2.6 5.6 1.4]\n",
            " [7.7 3.  6.1 2.3]\n",
            " [6.3 3.4 5.6 2.4]\n",
            " [6.4 3.1 5.5 1.8]\n",
            " [6.  3.  4.8 1.8]\n",
            " [6.9 3.1 5.4 2.1]\n",
            " [6.7 3.1 5.6 2.4]\n",
            " [6.9 3.1 5.1 2.3]\n",
            " [5.8 2.7 5.1 1.9]\n",
            " [6.8 3.2 5.9 2.3]\n",
            " [6.7 3.3 5.7 2.5]\n",
            " [6.7 3.  5.2 2.3]\n",
            " [6.3 2.5 5.  1.9]\n",
            " [6.5 3.  5.2 2. ]\n",
            " [6.2 3.4 5.4 2.3]\n",
            " [5.9 3.  5.1 1.8]] 135 (135, 4)\n",
            "-------------------X test-----------------------\n",
            "[[5.4 3.9 1.7 0.4]\n",
            " [4.3 3.  1.1 0.1]\n",
            " [5.4 3.4 1.7 0.2]\n",
            " [5.5 3.5 1.3 0.2]\n",
            " [4.9 3.1 1.5 0.1]\n",
            " [4.9 2.4 3.3 1. ]\n",
            " [6.  3.4 4.5 1.6]\n",
            " [5.7 2.9 4.2 1.3]\n",
            " [5.1 2.5 3.  1.1]\n",
            " [5.7 2.8 4.1 1.3]\n",
            " [6.5 3.  5.5 1.8]\n",
            " [6.9 3.2 5.7 2.3]\n",
            " [6.1 3.  4.9 1.8]\n",
            " [7.2 3.  5.8 1.6]\n",
            " [6.4 2.8 5.6 2.2]] 15 (15, 4)\n",
            "--------------------y train----------------------\n",
            "['Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
            " 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
            " 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
            " 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
            " 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
            " 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
            " 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
            " 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
            " 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
            " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
            " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
            " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
            " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
            " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
            " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
            " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
            " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
            " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
            " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
            " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
            " 'Iris-versicolor' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
            " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
            " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
            " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
            " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
            " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
            " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
            " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
            " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
            " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
            " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
            " 'Iris-virginica' 'Iris-virginica'] 135 (135,)\n",
            "-------------------y test-----------------------\n",
            "['Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
            " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
            " 'Iris-versicolor' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
            " 'Iris-virginica' 'Iris-virginica'] 15 (15,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gynEfjQf5kbI",
        "outputId": "47e1ee27-7097-41bb-968a-5f19ca8c4746"
      },
      "source": [
        "accuracy=[]\n",
        "  \n",
        "#모델 저장\n",
        "Model_dir='./model/'\n",
        "if not os.path.exists(Model_dir):\n",
        "  os.mkdir(Model_dir)\n",
        "\n",
        "modelpath='./model/{epoch:02d}-{val_loss:.4f}.hdf5'\n",
        "\n",
        "\n",
        "for train, test in skf.split(X,y_le): #y_le는 라벨인코딩한 값임.. 따라서 이후 for 문 안에서 원핫인코딩을 해주어야 함. \n",
        "  \n",
        "  model=Sequential()\n",
        "  model.add(Dense(24, input_dim=4, activation='relu'))\n",
        "  model.add(Dense(10, activation='relu'))\n",
        "  model.add(Dense(3, activation='softmax')) #3= y_encoded nunique. \n",
        "  model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        " \n",
        "  #모델 업데이트 및 저장\n",
        "  checkpointer = ModelCheckpoint(filepath=modelpath, monitor='val_loss', verbose=1, save_best_only=True)\n",
        "  \n",
        "  #학습 자동 중단 설정\n",
        "  early_stopping_callback=EarlyStopping(monitor='val_loss', patience=100)\n",
        "\n",
        "  y_le_en=tf.keras.utils.to_categorical(y_le) #라벨인코딩한것을 원핫인코딩으로 바꾸는 부분. \n",
        "  model.fit(X[train],y_le_en[train], validation_data=(X[test], y_le_en[test]), epochs=100, batch_size=5, verbose=1, callbacks=[early_stopping_callback, checkpointer])\n",
        "\n",
        "  k_accuracy = '%.4f'%(model.evaluate(X[test], y_le_en[test])[1])\n",
        "  accuracy.append(k_accuracy)\n",
        "\n",
        "\n",
        "print('\\n %.f fold accuracy: '%n_fold, accuracy)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "27/27 [==============================] - 1s 15ms/step - loss: 1.5473 - accuracy: 0.3831 - val_loss: 1.2181 - val_accuracy: 0.3333\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 1.21811, saving model to ./model/01-1.2181.hdf5\n",
            "Epoch 2/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 1.1484 - accuracy: 0.3473 - val_loss: 1.0593 - val_accuracy: 0.3333\n",
            "\n",
            "Epoch 00002: val_loss improved from 1.21811 to 1.05933, saving model to ./model/02-1.0593.hdf5\n",
            "Epoch 3/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 1.0403 - accuracy: 0.3011 - val_loss: 0.9884 - val_accuracy: 0.2667\n",
            "\n",
            "Epoch 00003: val_loss improved from 1.05933 to 0.98842, saving model to ./model/03-0.9884.hdf5\n",
            "Epoch 4/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.9903 - accuracy: 0.3838 - val_loss: 0.9405 - val_accuracy: 0.6667\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.98842 to 0.94046, saving model to ./model/04-0.9405.hdf5\n",
            "Epoch 5/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.9438 - accuracy: 0.6305 - val_loss: 0.8901 - val_accuracy: 0.7333\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.94046 to 0.89008, saving model to ./model/05-0.8901.hdf5\n",
            "Epoch 6/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.8919 - accuracy: 0.7877 - val_loss: 0.8386 - val_accuracy: 0.7333\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.89008 to 0.83862, saving model to ./model/06-0.8386.hdf5\n",
            "Epoch 7/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.8163 - accuracy: 0.7310 - val_loss: 0.7786 - val_accuracy: 0.6667\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.83862 to 0.77859, saving model to ./model/07-0.7786.hdf5\n",
            "Epoch 8/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.7860 - accuracy: 0.7563 - val_loss: 0.7161 - val_accuracy: 0.7333\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.77859 to 0.71609, saving model to ./model/08-0.7161.hdf5\n",
            "Epoch 9/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.7214 - accuracy: 0.8418 - val_loss: 0.6557 - val_accuracy: 0.7333\n",
            "\n",
            "Epoch 00009: val_loss improved from 0.71609 to 0.65567, saving model to ./model/09-0.6557.hdf5\n",
            "Epoch 10/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.6501 - accuracy: 0.8308 - val_loss: 0.6008 - val_accuracy: 0.7333\n",
            "\n",
            "Epoch 00010: val_loss improved from 0.65567 to 0.60083, saving model to ./model/10-0.6008.hdf5\n",
            "Epoch 11/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.5917 - accuracy: 0.9048 - val_loss: 0.5550 - val_accuracy: 0.7333\n",
            "\n",
            "Epoch 00011: val_loss improved from 0.60083 to 0.55501, saving model to ./model/11-0.5550.hdf5\n",
            "Epoch 12/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.5744 - accuracy: 0.9442 - val_loss: 0.5154 - val_accuracy: 0.8000\n",
            "\n",
            "Epoch 00012: val_loss improved from 0.55501 to 0.51536, saving model to ./model/12-0.5154.hdf5\n",
            "Epoch 13/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.5065 - accuracy: 0.8578 - val_loss: 0.4815 - val_accuracy: 0.8000\n",
            "\n",
            "Epoch 00013: val_loss improved from 0.51536 to 0.48153, saving model to ./model/13-0.4815.hdf5\n",
            "Epoch 14/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.4994 - accuracy: 0.9425 - val_loss: 0.4589 - val_accuracy: 0.7333\n",
            "\n",
            "Epoch 00014: val_loss improved from 0.48153 to 0.45895, saving model to ./model/14-0.4589.hdf5\n",
            "Epoch 15/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.4673 - accuracy: 0.9328 - val_loss: 0.4316 - val_accuracy: 0.8000\n",
            "\n",
            "Epoch 00015: val_loss improved from 0.45895 to 0.43163, saving model to ./model/15-0.4316.hdf5\n",
            "Epoch 16/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.4571 - accuracy: 0.9593 - val_loss: 0.4112 - val_accuracy: 0.8000\n",
            "\n",
            "Epoch 00016: val_loss improved from 0.43163 to 0.41120, saving model to ./model/16-0.4112.hdf5\n",
            "Epoch 17/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.4059 - accuracy: 0.9210 - val_loss: 0.3935 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00017: val_loss improved from 0.41120 to 0.39350, saving model to ./model/17-0.3935.hdf5\n",
            "Epoch 18/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.3897 - accuracy: 0.9355 - val_loss: 0.3869 - val_accuracy: 0.8000\n",
            "\n",
            "Epoch 00018: val_loss improved from 0.39350 to 0.38690, saving model to ./model/18-0.3869.hdf5\n",
            "Epoch 19/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.3849 - accuracy: 0.8914 - val_loss: 0.3635 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00019: val_loss improved from 0.38690 to 0.36348, saving model to ./model/19-0.3635.hdf5\n",
            "Epoch 20/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.3699 - accuracy: 0.9536 - val_loss: 0.3499 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00020: val_loss improved from 0.36348 to 0.34992, saving model to ./model/20-0.3499.hdf5\n",
            "Epoch 21/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.3616 - accuracy: 0.9424 - val_loss: 0.3492 - val_accuracy: 0.8000\n",
            "\n",
            "Epoch 00021: val_loss improved from 0.34992 to 0.34924, saving model to ./model/21-0.3492.hdf5\n",
            "Epoch 22/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.3360 - accuracy: 0.9041 - val_loss: 0.3258 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00022: val_loss improved from 0.34924 to 0.32578, saving model to ./model/22-0.3258.hdf5\n",
            "Epoch 23/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.3416 - accuracy: 0.9431 - val_loss: 0.3204 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00023: val_loss improved from 0.32578 to 0.32044, saving model to ./model/23-0.3204.hdf5\n",
            "Epoch 24/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.3194 - accuracy: 0.9760 - val_loss: 0.3046 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00024: val_loss improved from 0.32044 to 0.30456, saving model to ./model/24-0.3046.hdf5\n",
            "Epoch 25/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.3118 - accuracy: 0.9470 - val_loss: 0.2929 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00025: val_loss improved from 0.30456 to 0.29287, saving model to ./model/25-0.2929.hdf5\n",
            "Epoch 26/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.3464 - accuracy: 0.9202 - val_loss: 0.2903 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00026: val_loss improved from 0.29287 to 0.29026, saving model to ./model/26-0.2903.hdf5\n",
            "Epoch 27/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.2860 - accuracy: 0.9586 - val_loss: 0.2759 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00027: val_loss improved from 0.29026 to 0.27586, saving model to ./model/27-0.2759.hdf5\n",
            "Epoch 28/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.3122 - accuracy: 0.9480 - val_loss: 0.2782 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.27586\n",
            "Epoch 29/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.2850 - accuracy: 0.9537 - val_loss: 0.2546 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00029: val_loss improved from 0.27586 to 0.25460, saving model to ./model/29-0.2546.hdf5\n",
            "Epoch 30/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.2385 - accuracy: 0.9841 - val_loss: 0.2557 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.25460\n",
            "Epoch 31/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.2433 - accuracy: 0.9677 - val_loss: 0.2469 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00031: val_loss improved from 0.25460 to 0.24691, saving model to ./model/31-0.2469.hdf5\n",
            "Epoch 32/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.2758 - accuracy: 0.9367 - val_loss: 0.2288 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00032: val_loss improved from 0.24691 to 0.22879, saving model to ./model/32-0.2288.hdf5\n",
            "Epoch 33/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.2204 - accuracy: 0.9811 - val_loss: 0.2314 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.22879\n",
            "Epoch 34/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.2183 - accuracy: 0.9924 - val_loss: 0.2138 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00034: val_loss improved from 0.22879 to 0.21384, saving model to ./model/34-0.2138.hdf5\n",
            "Epoch 35/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.2558 - accuracy: 0.9357 - val_loss: 0.2332 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.21384\n",
            "Epoch 36/100\n",
            "27/27 [==============================] - 0s 5ms/step - loss: 0.2552 - accuracy: 0.9447 - val_loss: 0.2001 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00036: val_loss improved from 0.21384 to 0.20005, saving model to ./model/36-0.2001.hdf5\n",
            "Epoch 37/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.2034 - accuracy: 0.9638 - val_loss: 0.1906 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00037: val_loss improved from 0.20005 to 0.19062, saving model to ./model/37-0.1906.hdf5\n",
            "Epoch 38/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1778 - accuracy: 0.9777 - val_loss: 0.1811 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00038: val_loss improved from 0.19062 to 0.18112, saving model to ./model/38-0.1811.hdf5\n",
            "Epoch 39/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.2075 - accuracy: 0.9649 - val_loss: 0.1914 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.18112\n",
            "Epoch 40/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1712 - accuracy: 0.9845 - val_loss: 0.1675 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00040: val_loss improved from 0.18112 to 0.16748, saving model to ./model/40-0.1675.hdf5\n",
            "Epoch 41/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1653 - accuracy: 0.9586 - val_loss: 0.1611 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00041: val_loss improved from 0.16748 to 0.16113, saving model to ./model/41-0.1611.hdf5\n",
            "Epoch 42/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1921 - accuracy: 0.9780 - val_loss: 0.1582 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00042: val_loss improved from 0.16113 to 0.15815, saving model to ./model/42-0.1582.hdf5\n",
            "Epoch 43/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1669 - accuracy: 0.9820 - val_loss: 0.1514 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00043: val_loss improved from 0.15815 to 0.15136, saving model to ./model/43-0.1514.hdf5\n",
            "Epoch 44/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1917 - accuracy: 0.9689 - val_loss: 0.1483 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00044: val_loss improved from 0.15136 to 0.14827, saving model to ./model/44-0.1483.hdf5\n",
            "Epoch 45/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1399 - accuracy: 0.9665 - val_loss: 0.1386 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00045: val_loss improved from 0.14827 to 0.13857, saving model to ./model/45-0.1386.hdf5\n",
            "Epoch 46/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1526 - accuracy: 0.9927 - val_loss: 0.1491 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.13857\n",
            "Epoch 47/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1466 - accuracy: 0.9798 - val_loss: 0.1336 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00047: val_loss improved from 0.13857 to 0.13363, saving model to ./model/47-0.1336.hdf5\n",
            "Epoch 48/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1315 - accuracy: 0.9867 - val_loss: 0.1471 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.13363\n",
            "Epoch 49/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1533 - accuracy: 0.9571 - val_loss: 0.1544 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.13363\n",
            "Epoch 50/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1818 - accuracy: 0.9688 - val_loss: 0.1204 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00050: val_loss improved from 0.13363 to 0.12042, saving model to ./model/50-0.1204.hdf5\n",
            "Epoch 51/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1456 - accuracy: 0.9739 - val_loss: 0.1288 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.12042\n",
            "Epoch 52/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1233 - accuracy: 0.9750 - val_loss: 0.1221 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.12042\n",
            "Epoch 53/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1277 - accuracy: 0.9846 - val_loss: 0.1085 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00053: val_loss improved from 0.12042 to 0.10850, saving model to ./model/53-0.1085.hdf5\n",
            "Epoch 54/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1100 - accuracy: 0.9861 - val_loss: 0.1067 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00054: val_loss improved from 0.10850 to 0.10674, saving model to ./model/54-0.1067.hdf5\n",
            "Epoch 55/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1039 - accuracy: 0.9971 - val_loss: 0.1217 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.10674\n",
            "Epoch 56/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1191 - accuracy: 0.9690 - val_loss: 0.1205 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.10674\n",
            "Epoch 57/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1444 - accuracy: 0.9606 - val_loss: 0.1102 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.10674\n",
            "Epoch 58/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1194 - accuracy: 0.9714 - val_loss: 0.1018 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00058: val_loss improved from 0.10674 to 0.10183, saving model to ./model/58-0.1018.hdf5\n",
            "Epoch 59/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1193 - accuracy: 0.9775 - val_loss: 0.0900 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00059: val_loss improved from 0.10183 to 0.08997, saving model to ./model/59-0.0900.hdf5\n",
            "Epoch 60/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0991 - accuracy: 0.9887 - val_loss: 0.0892 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00060: val_loss improved from 0.08997 to 0.08916, saving model to ./model/60-0.0892.hdf5\n",
            "Epoch 61/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0895 - accuracy: 0.9926 - val_loss: 0.0899 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.08916\n",
            "Epoch 62/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1303 - accuracy: 0.9504 - val_loss: 0.0964 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.08916\n",
            "Epoch 63/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1335 - accuracy: 0.9564 - val_loss: 0.1153 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.08916\n",
            "Epoch 64/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1110 - accuracy: 0.9740 - val_loss: 0.0867 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00064: val_loss improved from 0.08916 to 0.08675, saving model to ./model/64-0.0867.hdf5\n",
            "Epoch 65/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0921 - accuracy: 0.9907 - val_loss: 0.0779 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00065: val_loss improved from 0.08675 to 0.07790, saving model to ./model/65-0.0779.hdf5\n",
            "Epoch 66/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1009 - accuracy: 0.9731 - val_loss: 0.0788 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.07790\n",
            "Epoch 67/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0898 - accuracy: 0.9746 - val_loss: 0.0854 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.07790\n",
            "Epoch 68/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0761 - accuracy: 0.9883 - val_loss: 0.0741 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00068: val_loss improved from 0.07790 to 0.07415, saving model to ./model/68-0.0741.hdf5\n",
            "Epoch 69/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0839 - accuracy: 0.9861 - val_loss: 0.0708 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00069: val_loss improved from 0.07415 to 0.07083, saving model to ./model/69-0.0708.hdf5\n",
            "Epoch 70/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.0850 - accuracy: 0.9941 - val_loss: 0.0686 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00070: val_loss improved from 0.07083 to 0.06859, saving model to ./model/70-0.0686.hdf5\n",
            "Epoch 71/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.0936 - accuracy: 0.9815 - val_loss: 0.0670 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00071: val_loss improved from 0.06859 to 0.06705, saving model to ./model/71-0.0670.hdf5\n",
            "Epoch 72/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.0892 - accuracy: 0.9878 - val_loss: 0.0766 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.06705\n",
            "Epoch 73/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0713 - accuracy: 0.9975 - val_loss: 0.0807 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.06705\n",
            "Epoch 74/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0708 - accuracy: 0.9766 - val_loss: 0.0631 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00074: val_loss improved from 0.06705 to 0.06310, saving model to ./model/74-0.0631.hdf5\n",
            "Epoch 75/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.0994 - accuracy: 0.9933 - val_loss: 0.0618 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00075: val_loss improved from 0.06310 to 0.06184, saving model to ./model/75-0.0618.hdf5\n",
            "Epoch 76/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.0767 - accuracy: 0.9788 - val_loss: 0.0611 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00076: val_loss improved from 0.06184 to 0.06108, saving model to ./model/76-0.0611.hdf5\n",
            "Epoch 77/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.0749 - accuracy: 0.9842 - val_loss: 0.0597 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00077: val_loss improved from 0.06108 to 0.05966, saving model to ./model/77-0.0597.hdf5\n",
            "Epoch 78/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0758 - accuracy: 0.9698 - val_loss: 0.0791 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.05966\n",
            "Epoch 79/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0684 - accuracy: 0.9875 - val_loss: 0.0646 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.05966\n",
            "Epoch 80/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1304 - accuracy: 0.9173 - val_loss: 0.0594 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00080: val_loss improved from 0.05966 to 0.05938, saving model to ./model/80-0.0594.hdf5\n",
            "Epoch 81/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.0628 - accuracy: 0.9945 - val_loss: 0.0585 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00081: val_loss improved from 0.05938 to 0.05853, saving model to ./model/81-0.0585.hdf5\n",
            "Epoch 82/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.0757 - accuracy: 0.9774 - val_loss: 0.0658 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.05853\n",
            "Epoch 83/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0999 - accuracy: 0.9783 - val_loss: 0.0653 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.05853\n",
            "Epoch 84/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0746 - accuracy: 0.9837 - val_loss: 0.0534 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00084: val_loss improved from 0.05853 to 0.05345, saving model to ./model/84-0.0534.hdf5\n",
            "Epoch 85/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.0860 - accuracy: 0.9870 - val_loss: 0.0979 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.05345\n",
            "Epoch 86/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0563 - accuracy: 0.9834 - val_loss: 0.0517 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00086: val_loss improved from 0.05345 to 0.05171, saving model to ./model/86-0.0517.hdf5\n",
            "Epoch 87/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1004 - accuracy: 0.9634 - val_loss: 0.0956 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.05171\n",
            "Epoch 88/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.0695 - accuracy: 0.9715 - val_loss: 0.0497 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00088: val_loss improved from 0.05171 to 0.04966, saving model to ./model/88-0.0497.hdf5\n",
            "Epoch 89/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.0676 - accuracy: 0.9738 - val_loss: 0.0540 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.04966\n",
            "Epoch 90/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.0512 - accuracy: 0.9877 - val_loss: 0.0598 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.04966\n",
            "Epoch 91/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1411 - accuracy: 0.9424 - val_loss: 0.0901 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.04966\n",
            "Epoch 92/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1562 - accuracy: 0.9553 - val_loss: 0.0524 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.04966\n",
            "Epoch 93/100\n",
            "27/27 [==============================] - 0s 5ms/step - loss: 0.0780 - accuracy: 0.9688 - val_loss: 0.0475 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00093: val_loss improved from 0.04966 to 0.04750, saving model to ./model/93-0.0475.hdf5\n",
            "Epoch 94/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0680 - accuracy: 0.9856 - val_loss: 0.0582 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.04750\n",
            "Epoch 95/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1035 - accuracy: 0.9763 - val_loss: 0.0541 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.04750\n",
            "Epoch 96/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1033 - accuracy: 0.9787 - val_loss: 0.0464 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00096: val_loss improved from 0.04750 to 0.04640, saving model to ./model/96-0.0464.hdf5\n",
            "Epoch 97/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0821 - accuracy: 0.9601 - val_loss: 0.0443 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00097: val_loss improved from 0.04640 to 0.04427, saving model to ./model/97-0.0443.hdf5\n",
            "Epoch 98/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0710 - accuracy: 0.9842 - val_loss: 0.0520 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.04427\n",
            "Epoch 99/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0544 - accuracy: 0.9825 - val_loss: 0.0465 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.04427\n",
            "Epoch 100/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1104 - accuracy: 0.9643 - val_loss: 0.0559 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.04427\n",
            "1/1 [==============================] - 0s 307ms/step - loss: 0.0559 - accuracy: 1.0000\n",
            "Epoch 1/100\n",
            "27/27 [==============================] - 1s 15ms/step - loss: 1.3102 - accuracy: 0.3734 - val_loss: 1.1689 - val_accuracy: 0.3333\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 1.16889, saving model to ./model/01-1.1689.hdf5\n",
            "Epoch 2/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 1.1191 - accuracy: 0.3856 - val_loss: 1.0623 - val_accuracy: 0.3333\n",
            "\n",
            "Epoch 00002: val_loss improved from 1.16889 to 1.06225, saving model to ./model/02-1.0623.hdf5\n",
            "Epoch 3/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 1.0009 - accuracy: 0.4909 - val_loss: 0.9699 - val_accuracy: 0.8000\n",
            "\n",
            "Epoch 00003: val_loss improved from 1.06225 to 0.96994, saving model to ./model/03-0.9699.hdf5\n",
            "Epoch 4/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.9398 - accuracy: 0.8827 - val_loss: 0.8968 - val_accuracy: 0.8667\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.96994 to 0.89683, saving model to ./model/04-0.8968.hdf5\n",
            "Epoch 5/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.8749 - accuracy: 0.9417 - val_loss: 0.8263 - val_accuracy: 0.8667\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.89683 to 0.82631, saving model to ./model/05-0.8263.hdf5\n",
            "Epoch 6/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.7810 - accuracy: 0.9326 - val_loss: 0.7465 - val_accuracy: 0.8667\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.82631 to 0.74653, saving model to ./model/06-0.7465.hdf5\n",
            "Epoch 7/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.7139 - accuracy: 0.9275 - val_loss: 0.6643 - val_accuracy: 0.8667\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.74653 to 0.66433, saving model to ./model/07-0.6643.hdf5\n",
            "Epoch 8/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.6153 - accuracy: 0.9449 - val_loss: 0.5886 - val_accuracy: 0.8667\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.66433 to 0.58856, saving model to ./model/08-0.5886.hdf5\n",
            "Epoch 9/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.5359 - accuracy: 0.9791 - val_loss: 0.5191 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00009: val_loss improved from 0.58856 to 0.51908, saving model to ./model/09-0.5191.hdf5\n",
            "Epoch 10/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.4986 - accuracy: 0.9340 - val_loss: 0.4674 - val_accuracy: 0.8667\n",
            "\n",
            "Epoch 00010: val_loss improved from 0.51908 to 0.46744, saving model to ./model/10-0.4674.hdf5\n",
            "Epoch 11/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.4502 - accuracy: 0.8858 - val_loss: 0.4178 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00011: val_loss improved from 0.46744 to 0.41778, saving model to ./model/11-0.4178.hdf5\n",
            "Epoch 12/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.4106 - accuracy: 0.9707 - val_loss: 0.3737 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00012: val_loss improved from 0.41778 to 0.37372, saving model to ./model/12-0.3737.hdf5\n",
            "Epoch 13/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.3962 - accuracy: 0.9686 - val_loss: 0.3426 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00013: val_loss improved from 0.37372 to 0.34259, saving model to ./model/13-0.3426.hdf5\n",
            "Epoch 14/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.3033 - accuracy: 0.9708 - val_loss: 0.3137 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00014: val_loss improved from 0.34259 to 0.31367, saving model to ./model/14-0.3137.hdf5\n",
            "Epoch 15/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.2839 - accuracy: 0.9824 - val_loss: 0.2866 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00015: val_loss improved from 0.31367 to 0.28660, saving model to ./model/15-0.2866.hdf5\n",
            "Epoch 16/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.2558 - accuracy: 0.9770 - val_loss: 0.2659 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00016: val_loss improved from 0.28660 to 0.26590, saving model to ./model/16-0.2659.hdf5\n",
            "Epoch 17/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.2572 - accuracy: 0.9847 - val_loss: 0.2470 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00017: val_loss improved from 0.26590 to 0.24699, saving model to ./model/17-0.2470.hdf5\n",
            "Epoch 18/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.2529 - accuracy: 0.9923 - val_loss: 0.2349 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00018: val_loss improved from 0.24699 to 0.23490, saving model to ./model/18-0.2349.hdf5\n",
            "Epoch 19/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.2326 - accuracy: 0.9240 - val_loss: 0.2236 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00019: val_loss improved from 0.23490 to 0.22358, saving model to ./model/19-0.2236.hdf5\n",
            "Epoch 20/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1943 - accuracy: 0.9899 - val_loss: 0.2031 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00020: val_loss improved from 0.22358 to 0.20308, saving model to ./model/20-0.2031.hdf5\n",
            "Epoch 21/100\n",
            "27/27 [==============================] - 0s 5ms/step - loss: 0.1663 - accuracy: 0.9881 - val_loss: 0.1967 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00021: val_loss improved from 0.20308 to 0.19673, saving model to ./model/21-0.1967.hdf5\n",
            "Epoch 22/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1917 - accuracy: 0.9380 - val_loss: 0.2228 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.19673\n",
            "Epoch 23/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.2052 - accuracy: 0.9520 - val_loss: 0.1752 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00023: val_loss improved from 0.19673 to 0.17516, saving model to ./model/23-0.1752.hdf5\n",
            "Epoch 24/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1590 - accuracy: 0.9892 - val_loss: 0.1642 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00024: val_loss improved from 0.17516 to 0.16417, saving model to ./model/24-0.1642.hdf5\n",
            "Epoch 25/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1424 - accuracy: 0.9926 - val_loss: 0.1570 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00025: val_loss improved from 0.16417 to 0.15702, saving model to ./model/25-0.1570.hdf5\n",
            "Epoch 26/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1557 - accuracy: 0.9719 - val_loss: 0.1594 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.15702\n",
            "Epoch 27/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1539 - accuracy: 0.9493 - val_loss: 0.1676 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.15702\n",
            "Epoch 28/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1278 - accuracy: 0.9833 - val_loss: 0.1608 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.15702\n",
            "Epoch 29/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1301 - accuracy: 0.9722 - val_loss: 0.1410 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00029: val_loss improved from 0.15702 to 0.14103, saving model to ./model/29-0.1410.hdf5\n",
            "Epoch 30/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1523 - accuracy: 0.9441 - val_loss: 0.1290 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00030: val_loss improved from 0.14103 to 0.12904, saving model to ./model/30-0.1290.hdf5\n",
            "Epoch 31/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1493 - accuracy: 0.9880 - val_loss: 0.1230 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00031: val_loss improved from 0.12904 to 0.12304, saving model to ./model/31-0.1230.hdf5\n",
            "Epoch 32/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1229 - accuracy: 0.9636 - val_loss: 0.1198 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00032: val_loss improved from 0.12304 to 0.11982, saving model to ./model/32-0.1198.hdf5\n",
            "Epoch 33/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0932 - accuracy: 0.9920 - val_loss: 0.1169 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00033: val_loss improved from 0.11982 to 0.11693, saving model to ./model/33-0.1169.hdf5\n",
            "Epoch 34/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1365 - accuracy: 0.9795 - val_loss: 0.1218 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.11693\n",
            "Epoch 35/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1170 - accuracy: 0.9690 - val_loss: 0.1325 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.11693\n",
            "Epoch 36/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0843 - accuracy: 0.9788 - val_loss: 0.1145 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00036: val_loss improved from 0.11693 to 0.11447, saving model to ./model/36-0.1145.hdf5\n",
            "Epoch 37/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1298 - accuracy: 0.9559 - val_loss: 0.1140 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00037: val_loss improved from 0.11447 to 0.11396, saving model to ./model/37-0.1140.hdf5\n",
            "Epoch 38/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1292 - accuracy: 0.9564 - val_loss: 0.1155 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.11396\n",
            "Epoch 39/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1086 - accuracy: 0.9732 - val_loss: 0.1035 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00039: val_loss improved from 0.11396 to 0.10351, saving model to ./model/39-0.1035.hdf5\n",
            "Epoch 40/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.0834 - accuracy: 0.9866 - val_loss: 0.0972 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00040: val_loss improved from 0.10351 to 0.09722, saving model to ./model/40-0.0972.hdf5\n",
            "Epoch 41/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.0764 - accuracy: 0.9824 - val_loss: 0.1140 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.09722\n",
            "Epoch 42/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1117 - accuracy: 0.9732 - val_loss: 0.1019 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.09722\n",
            "Epoch 43/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1108 - accuracy: 0.9681 - val_loss: 0.1389 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.09722\n",
            "Epoch 44/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1182 - accuracy: 0.9508 - val_loss: 0.0880 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00044: val_loss improved from 0.09722 to 0.08804, saving model to ./model/44-0.0880.hdf5\n",
            "Epoch 45/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1308 - accuracy: 0.9496 - val_loss: 0.1010 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.08804\n",
            "Epoch 46/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1003 - accuracy: 0.9567 - val_loss: 0.0926 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.08804\n",
            "Epoch 47/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0935 - accuracy: 0.9774 - val_loss: 0.0844 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00047: val_loss improved from 0.08804 to 0.08438, saving model to ./model/47-0.0844.hdf5\n",
            "Epoch 48/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.0621 - accuracy: 0.9852 - val_loss: 0.0885 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.08438\n",
            "Epoch 49/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0921 - accuracy: 0.9749 - val_loss: 0.0878 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.08438\n",
            "Epoch 50/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0861 - accuracy: 0.9914 - val_loss: 0.0949 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.08438\n",
            "Epoch 51/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0647 - accuracy: 0.9922 - val_loss: 0.0763 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00051: val_loss improved from 0.08438 to 0.07625, saving model to ./model/51-0.0763.hdf5\n",
            "Epoch 52/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0987 - accuracy: 0.9701 - val_loss: 0.0745 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00052: val_loss improved from 0.07625 to 0.07446, saving model to ./model/52-0.0745.hdf5\n",
            "Epoch 53/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1072 - accuracy: 0.9666 - val_loss: 0.0794 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.07446\n",
            "Epoch 54/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0620 - accuracy: 0.9890 - val_loss: 0.0715 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00054: val_loss improved from 0.07446 to 0.07152, saving model to ./model/54-0.0715.hdf5\n",
            "Epoch 55/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0622 - accuracy: 0.9865 - val_loss: 0.0898 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.07152\n",
            "Epoch 56/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0789 - accuracy: 0.9875 - val_loss: 0.0815 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.07152\n",
            "Epoch 57/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0984 - accuracy: 0.9521 - val_loss: 0.1144 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.07152\n",
            "Epoch 58/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1318 - accuracy: 0.9567 - val_loss: 0.0845 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.07152\n",
            "Epoch 59/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0851 - accuracy: 0.9737 - val_loss: 0.0843 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.07152\n",
            "Epoch 60/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0752 - accuracy: 0.9828 - val_loss: 0.0707 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00060: val_loss improved from 0.07152 to 0.07068, saving model to ./model/60-0.0707.hdf5\n",
            "Epoch 61/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.0861 - accuracy: 0.9872 - val_loss: 0.0649 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00061: val_loss improved from 0.07068 to 0.06493, saving model to ./model/61-0.0649.hdf5\n",
            "Epoch 62/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0602 - accuracy: 0.9908 - val_loss: 0.1445 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.06493\n",
            "Epoch 63/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0664 - accuracy: 0.9729 - val_loss: 0.0629 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00063: val_loss improved from 0.06493 to 0.06285, saving model to ./model/63-0.0629.hdf5\n",
            "Epoch 64/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0804 - accuracy: 0.9615 - val_loss: 0.1370 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.06285\n",
            "Epoch 65/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0681 - accuracy: 0.9661 - val_loss: 0.0679 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.06285\n",
            "Epoch 66/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0864 - accuracy: 0.9636 - val_loss: 0.0769 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.06285\n",
            "Epoch 67/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0431 - accuracy: 0.9841 - val_loss: 0.0717 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.06285\n",
            "Epoch 68/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0691 - accuracy: 0.9871 - val_loss: 0.0711 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.06285\n",
            "Epoch 69/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1203 - accuracy: 0.9607 - val_loss: 0.0580 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00069: val_loss improved from 0.06285 to 0.05802, saving model to ./model/69-0.0580.hdf5\n",
            "Epoch 70/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.0884 - accuracy: 0.9509 - val_loss: 0.0610 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.05802\n",
            "Epoch 71/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0947 - accuracy: 0.9642 - val_loss: 0.1310 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.05802\n",
            "Epoch 72/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.0615 - accuracy: 0.9879 - val_loss: 0.0611 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.05802\n",
            "Epoch 73/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.0811 - accuracy: 0.9823 - val_loss: 0.0575 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00073: val_loss improved from 0.05802 to 0.05749, saving model to ./model/73-0.0575.hdf5\n",
            "Epoch 74/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0632 - accuracy: 0.9674 - val_loss: 0.0669 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.05749\n",
            "Epoch 75/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0650 - accuracy: 0.9873 - val_loss: 0.0674 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.05749\n",
            "Epoch 76/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0571 - accuracy: 0.9796 - val_loss: 0.0828 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.05749\n",
            "Epoch 77/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0777 - accuracy: 0.9669 - val_loss: 0.0690 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.05749\n",
            "Epoch 78/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0639 - accuracy: 0.9915 - val_loss: 0.0636 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.05749\n",
            "Epoch 79/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.0391 - accuracy: 0.9934 - val_loss: 0.0652 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.05749\n",
            "Epoch 80/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.0741 - accuracy: 0.9779 - val_loss: 0.1249 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.05749\n",
            "Epoch 81/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0921 - accuracy: 0.9540 - val_loss: 0.0649 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.05749\n",
            "Epoch 82/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0745 - accuracy: 0.9778 - val_loss: 0.0921 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.05749\n",
            "Epoch 83/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.0710 - accuracy: 0.9668 - val_loss: 0.0881 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.05749\n",
            "Epoch 84/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0397 - accuracy: 0.9908 - val_loss: 0.0531 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00084: val_loss improved from 0.05749 to 0.05313, saving model to ./model/84-0.0531.hdf5\n",
            "Epoch 85/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0616 - accuracy: 0.9705 - val_loss: 0.0524 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00085: val_loss improved from 0.05313 to 0.05245, saving model to ./model/85-0.0524.hdf5\n",
            "Epoch 86/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0694 - accuracy: 0.9817 - val_loss: 0.1011 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.05245\n",
            "Epoch 87/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1026 - accuracy: 0.9501 - val_loss: 0.0694 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.05245\n",
            "Epoch 88/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0639 - accuracy: 0.9859 - val_loss: 0.0628 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.05245\n",
            "Epoch 89/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0821 - accuracy: 0.9635 - val_loss: 0.0485 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00089: val_loss improved from 0.05245 to 0.04851, saving model to ./model/89-0.0485.hdf5\n",
            "Epoch 90/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0774 - accuracy: 0.9687 - val_loss: 0.0704 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.04851\n",
            "Epoch 91/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0702 - accuracy: 0.9856 - val_loss: 0.0689 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.04851\n",
            "Epoch 92/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1178 - accuracy: 0.9602 - val_loss: 0.0784 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.04851\n",
            "Epoch 93/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0664 - accuracy: 0.9715 - val_loss: 0.0642 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.04851\n",
            "Epoch 94/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0861 - accuracy: 0.9754 - val_loss: 0.0825 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.04851\n",
            "Epoch 95/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.0592 - accuracy: 0.9894 - val_loss: 0.0786 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.04851\n",
            "Epoch 96/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1151 - accuracy: 0.9653 - val_loss: 0.0640 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.04851\n",
            "Epoch 97/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1122 - accuracy: 0.9532 - val_loss: 0.1306 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.04851\n",
            "Epoch 98/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0529 - accuracy: 0.9812 - val_loss: 0.0454 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00098: val_loss improved from 0.04851 to 0.04544, saving model to ./model/98-0.0454.hdf5\n",
            "Epoch 99/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0729 - accuracy: 0.9670 - val_loss: 0.0573 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.04544\n",
            "Epoch 100/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.0535 - accuracy: 0.9897 - val_loss: 0.0911 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.04544\n",
            "1/1 [==============================] - 0s 302ms/step - loss: 0.0911 - accuracy: 0.9333\n",
            "Epoch 1/100\n",
            "27/27 [==============================] - 1s 15ms/step - loss: 1.4710 - accuracy: 0.0000e+00 - val_loss: 1.2164 - val_accuracy: 0.0000e+00\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 1.21637, saving model to ./model/01-1.2164.hdf5\n",
            "Epoch 2/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 1.1952 - accuracy: 0.0667 - val_loss: 1.0584 - val_accuracy: 0.6667\n",
            "\n",
            "Epoch 00002: val_loss improved from 1.21637 to 1.05841, saving model to ./model/02-1.0584.hdf5\n",
            "Epoch 3/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 1.0605 - accuracy: 0.6076 - val_loss: 0.9657 - val_accuracy: 0.6667\n",
            "\n",
            "Epoch 00003: val_loss improved from 1.05841 to 0.96566, saving model to ./model/03-0.9657.hdf5\n",
            "Epoch 4/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.9439 - accuracy: 0.6730 - val_loss: 0.8907 - val_accuracy: 0.6667\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.96566 to 0.89068, saving model to ./model/04-0.8907.hdf5\n",
            "Epoch 5/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.8865 - accuracy: 0.6381 - val_loss: 0.8304 - val_accuracy: 0.6667\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.89068 to 0.83040, saving model to ./model/05-0.8304.hdf5\n",
            "Epoch 6/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.8488 - accuracy: 0.6259 - val_loss: 0.7755 - val_accuracy: 0.6667\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.83040 to 0.77546, saving model to ./model/06-0.7755.hdf5\n",
            "Epoch 7/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.7170 - accuracy: 0.7274 - val_loss: 0.7228 - val_accuracy: 0.6667\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.77546 to 0.72279, saving model to ./model/07-0.7228.hdf5\n",
            "Epoch 8/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.6711 - accuracy: 0.7112 - val_loss: 0.6772 - val_accuracy: 0.6667\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.72279 to 0.67720, saving model to ./model/08-0.6772.hdf5\n",
            "Epoch 9/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.6527 - accuracy: 0.6945 - val_loss: 0.6349 - val_accuracy: 0.6667\n",
            "\n",
            "Epoch 00009: val_loss improved from 0.67720 to 0.63491, saving model to ./model/09-0.6349.hdf5\n",
            "Epoch 10/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.6132 - accuracy: 0.6966 - val_loss: 0.5974 - val_accuracy: 0.7333\n",
            "\n",
            "Epoch 00010: val_loss improved from 0.63491 to 0.59743, saving model to ./model/10-0.5974.hdf5\n",
            "Epoch 11/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.5404 - accuracy: 0.7450 - val_loss: 0.5609 - val_accuracy: 0.8667\n",
            "\n",
            "Epoch 00011: val_loss improved from 0.59743 to 0.56091, saving model to ./model/11-0.5609.hdf5\n",
            "Epoch 12/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.5291 - accuracy: 0.8248 - val_loss: 0.5246 - val_accuracy: 0.8667\n",
            "\n",
            "Epoch 00012: val_loss improved from 0.56091 to 0.52461, saving model to ./model/12-0.5246.hdf5\n",
            "Epoch 13/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.5248 - accuracy: 0.7552 - val_loss: 0.4868 - val_accuracy: 0.8667\n",
            "\n",
            "Epoch 00013: val_loss improved from 0.52461 to 0.48684, saving model to ./model/13-0.4868.hdf5\n",
            "Epoch 14/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.5048 - accuracy: 0.8372 - val_loss: 0.4534 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00014: val_loss improved from 0.48684 to 0.45339, saving model to ./model/14-0.4534.hdf5\n",
            "Epoch 15/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.4618 - accuracy: 0.8954 - val_loss: 0.4175 - val_accuracy: 0.8667\n",
            "\n",
            "Epoch 00015: val_loss improved from 0.45339 to 0.41751, saving model to ./model/15-0.4175.hdf5\n",
            "Epoch 16/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.3733 - accuracy: 0.9183 - val_loss: 0.3968 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00016: val_loss improved from 0.41751 to 0.39680, saving model to ./model/16-0.3968.hdf5\n",
            "Epoch 17/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.3704 - accuracy: 0.9677 - val_loss: 0.3611 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00017: val_loss improved from 0.39680 to 0.36112, saving model to ./model/17-0.3611.hdf5\n",
            "Epoch 18/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.3854 - accuracy: 0.9347 - val_loss: 0.3365 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00018: val_loss improved from 0.36112 to 0.33653, saving model to ./model/18-0.3365.hdf5\n",
            "Epoch 19/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.3270 - accuracy: 0.9575 - val_loss: 0.3147 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00019: val_loss improved from 0.33653 to 0.31469, saving model to ./model/19-0.3147.hdf5\n",
            "Epoch 20/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.3345 - accuracy: 0.9258 - val_loss: 0.2973 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00020: val_loss improved from 0.31469 to 0.29731, saving model to ./model/20-0.2973.hdf5\n",
            "Epoch 21/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.2998 - accuracy: 0.9603 - val_loss: 0.2770 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00021: val_loss improved from 0.29731 to 0.27696, saving model to ./model/21-0.2770.hdf5\n",
            "Epoch 22/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.3128 - accuracy: 0.9624 - val_loss: 0.2569 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00022: val_loss improved from 0.27696 to 0.25691, saving model to ./model/22-0.2569.hdf5\n",
            "Epoch 23/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.2537 - accuracy: 0.9520 - val_loss: 0.2505 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00023: val_loss improved from 0.25691 to 0.25046, saving model to ./model/23-0.2505.hdf5\n",
            "Epoch 24/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.2310 - accuracy: 0.9771 - val_loss: 0.2299 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00024: val_loss improved from 0.25046 to 0.22991, saving model to ./model/24-0.2299.hdf5\n",
            "Epoch 25/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.2653 - accuracy: 0.9281 - val_loss: 0.2180 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00025: val_loss improved from 0.22991 to 0.21801, saving model to ./model/25-0.2180.hdf5\n",
            "Epoch 26/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.2057 - accuracy: 0.9668 - val_loss: 0.2039 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00026: val_loss improved from 0.21801 to 0.20392, saving model to ./model/26-0.2039.hdf5\n",
            "Epoch 27/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.2128 - accuracy: 0.9731 - val_loss: 0.1916 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00027: val_loss improved from 0.20392 to 0.19156, saving model to ./model/27-0.1916.hdf5\n",
            "Epoch 28/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.2232 - accuracy: 0.9710 - val_loss: 0.1837 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00028: val_loss improved from 0.19156 to 0.18368, saving model to ./model/28-0.1837.hdf5\n",
            "Epoch 29/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1838 - accuracy: 0.9860 - val_loss: 0.1723 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00029: val_loss improved from 0.18368 to 0.17231, saving model to ./model/29-0.1723.hdf5\n",
            "Epoch 30/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1939 - accuracy: 0.9716 - val_loss: 0.1694 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00030: val_loss improved from 0.17231 to 0.16944, saving model to ./model/30-0.1694.hdf5\n",
            "Epoch 31/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1758 - accuracy: 0.9673 - val_loss: 0.1568 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00031: val_loss improved from 0.16944 to 0.15677, saving model to ./model/31-0.1568.hdf5\n",
            "Epoch 32/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1720 - accuracy: 0.9872 - val_loss: 0.1473 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00032: val_loss improved from 0.15677 to 0.14733, saving model to ./model/32-0.1473.hdf5\n",
            "Epoch 33/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1747 - accuracy: 0.9571 - val_loss: 0.1405 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00033: val_loss improved from 0.14733 to 0.14054, saving model to ./model/33-0.1405.hdf5\n",
            "Epoch 34/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1685 - accuracy: 0.9710 - val_loss: 0.1667 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.14054\n",
            "Epoch 35/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1664 - accuracy: 0.9532 - val_loss: 0.1330 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00035: val_loss improved from 0.14054 to 0.13301, saving model to ./model/35-0.1330.hdf5\n",
            "Epoch 36/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1286 - accuracy: 0.9908 - val_loss: 0.1439 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.13301\n",
            "Epoch 37/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1179 - accuracy: 0.9879 - val_loss: 0.1160 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00037: val_loss improved from 0.13301 to 0.11596, saving model to ./model/37-0.1160.hdf5\n",
            "Epoch 38/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.2214 - accuracy: 0.8861 - val_loss: 0.1151 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00038: val_loss improved from 0.11596 to 0.11508, saving model to ./model/38-0.1151.hdf5\n",
            "Epoch 39/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1718 - accuracy: 0.9628 - val_loss: 0.1076 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00039: val_loss improved from 0.11508 to 0.10760, saving model to ./model/39-0.1076.hdf5\n",
            "Epoch 40/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1264 - accuracy: 0.9749 - val_loss: 0.1027 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00040: val_loss improved from 0.10760 to 0.10274, saving model to ./model/40-0.1027.hdf5\n",
            "Epoch 41/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1692 - accuracy: 0.9445 - val_loss: 0.1061 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.10274\n",
            "Epoch 42/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1213 - accuracy: 0.9691 - val_loss: 0.1260 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.10274\n",
            "Epoch 43/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1487 - accuracy: 0.9283 - val_loss: 0.0909 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00043: val_loss improved from 0.10274 to 0.09085, saving model to ./model/43-0.0909.hdf5\n",
            "Epoch 44/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1188 - accuracy: 0.9861 - val_loss: 0.0987 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.09085\n",
            "Epoch 45/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1065 - accuracy: 0.9880 - val_loss: 0.0878 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00045: val_loss improved from 0.09085 to 0.08779, saving model to ./model/45-0.0878.hdf5\n",
            "Epoch 46/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0985 - accuracy: 0.9784 - val_loss: 0.0828 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00046: val_loss improved from 0.08779 to 0.08277, saving model to ./model/46-0.0828.hdf5\n",
            "Epoch 47/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1296 - accuracy: 0.9603 - val_loss: 0.0903 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.08277\n",
            "Epoch 48/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1249 - accuracy: 0.9630 - val_loss: 0.0787 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00048: val_loss improved from 0.08277 to 0.07865, saving model to ./model/48-0.0787.hdf5\n",
            "Epoch 49/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1374 - accuracy: 0.9629 - val_loss: 0.0760 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00049: val_loss improved from 0.07865 to 0.07598, saving model to ./model/49-0.0760.hdf5\n",
            "Epoch 50/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.0986 - accuracy: 0.9763 - val_loss: 0.0761 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.07598\n",
            "Epoch 51/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0964 - accuracy: 0.9869 - val_loss: 0.0692 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00051: val_loss improved from 0.07598 to 0.06922, saving model to ./model/51-0.0692.hdf5\n",
            "Epoch 52/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1087 - accuracy: 0.9672 - val_loss: 0.0687 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00052: val_loss improved from 0.06922 to 0.06873, saving model to ./model/52-0.0687.hdf5\n",
            "Epoch 53/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1126 - accuracy: 0.9677 - val_loss: 0.0866 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.06873\n",
            "Epoch 54/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1064 - accuracy: 0.9544 - val_loss: 0.0641 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00054: val_loss improved from 0.06873 to 0.06413, saving model to ./model/54-0.0641.hdf5\n",
            "Epoch 55/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1246 - accuracy: 0.9589 - val_loss: 0.0663 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.06413\n",
            "Epoch 56/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1134 - accuracy: 0.9610 - val_loss: 0.0661 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.06413\n",
            "Epoch 57/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1197 - accuracy: 0.9637 - val_loss: 0.0571 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00057: val_loss improved from 0.06413 to 0.05708, saving model to ./model/57-0.0571.hdf5\n",
            "Epoch 58/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1062 - accuracy: 0.9645 - val_loss: 0.0553 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00058: val_loss improved from 0.05708 to 0.05531, saving model to ./model/58-0.0553.hdf5\n",
            "Epoch 59/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.0897 - accuracy: 0.9630 - val_loss: 0.0558 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.05531\n",
            "Epoch 60/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0919 - accuracy: 0.9846 - val_loss: 0.0518 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00060: val_loss improved from 0.05531 to 0.05184, saving model to ./model/60-0.0518.hdf5\n",
            "Epoch 61/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0845 - accuracy: 0.9802 - val_loss: 0.0645 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.05184\n",
            "Epoch 62/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0867 - accuracy: 0.9867 - val_loss: 0.0554 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.05184\n",
            "Epoch 63/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0799 - accuracy: 0.9793 - val_loss: 0.0520 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.05184\n",
            "Epoch 64/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1105 - accuracy: 0.9654 - val_loss: 0.0503 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00064: val_loss improved from 0.05184 to 0.05027, saving model to ./model/64-0.0503.hdf5\n",
            "Epoch 65/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1109 - accuracy: 0.9703 - val_loss: 0.0476 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00065: val_loss improved from 0.05027 to 0.04757, saving model to ./model/65-0.0476.hdf5\n",
            "Epoch 66/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0765 - accuracy: 0.9555 - val_loss: 0.0460 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00066: val_loss improved from 0.04757 to 0.04600, saving model to ./model/66-0.0460.hdf5\n",
            "Epoch 67/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.0808 - accuracy: 0.9723 - val_loss: 0.0432 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00067: val_loss improved from 0.04600 to 0.04323, saving model to ./model/67-0.0432.hdf5\n",
            "Epoch 68/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.0811 - accuracy: 0.9833 - val_loss: 0.0647 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.04323\n",
            "Epoch 69/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.0663 - accuracy: 0.9830 - val_loss: 0.0431 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00069: val_loss improved from 0.04323 to 0.04306, saving model to ./model/69-0.0431.hdf5\n",
            "Epoch 70/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.0628 - accuracy: 0.9891 - val_loss: 0.0404 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00070: val_loss improved from 0.04306 to 0.04043, saving model to ./model/70-0.0404.hdf5\n",
            "Epoch 71/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0833 - accuracy: 0.9859 - val_loss: 0.0503 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.04043\n",
            "Epoch 72/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0857 - accuracy: 0.9574 - val_loss: 0.0388 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00072: val_loss improved from 0.04043 to 0.03878, saving model to ./model/72-0.0388.hdf5\n",
            "Epoch 73/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.0706 - accuracy: 0.9691 - val_loss: 0.0444 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.03878\n",
            "Epoch 74/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0816 - accuracy: 0.9812 - val_loss: 0.0367 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00074: val_loss improved from 0.03878 to 0.03673, saving model to ./model/74-0.0367.hdf5\n",
            "Epoch 75/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0826 - accuracy: 0.9792 - val_loss: 0.0676 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.03673\n",
            "Epoch 76/100\n",
            "27/27 [==============================] - 0s 5ms/step - loss: 0.0888 - accuracy: 0.9753 - val_loss: 0.0353 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00076: val_loss improved from 0.03673 to 0.03532, saving model to ./model/76-0.0353.hdf5\n",
            "Epoch 77/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0995 - accuracy: 0.9555 - val_loss: 0.0346 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00077: val_loss improved from 0.03532 to 0.03464, saving model to ./model/77-0.0346.hdf5\n",
            "Epoch 78/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0682 - accuracy: 0.9734 - val_loss: 0.0361 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.03464\n",
            "Epoch 79/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.0637 - accuracy: 0.9913 - val_loss: 0.0341 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00079: val_loss improved from 0.03464 to 0.03406, saving model to ./model/79-0.0341.hdf5\n",
            "Epoch 80/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0761 - accuracy: 0.9728 - val_loss: 0.0360 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.03406\n",
            "Epoch 81/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0763 - accuracy: 0.9964 - val_loss: 0.0316 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00081: val_loss improved from 0.03406 to 0.03165, saving model to ./model/81-0.0316.hdf5\n",
            "Epoch 82/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0991 - accuracy: 0.9458 - val_loss: 0.0312 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00082: val_loss improved from 0.03165 to 0.03117, saving model to ./model/82-0.0312.hdf5\n",
            "Epoch 83/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0892 - accuracy: 0.9601 - val_loss: 0.0321 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.03117\n",
            "Epoch 84/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.0744 - accuracy: 0.9861 - val_loss: 0.0391 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.03117\n",
            "Epoch 85/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1292 - accuracy: 0.9510 - val_loss: 0.0295 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00085: val_loss improved from 0.03117 to 0.02953, saving model to ./model/85-0.0295.hdf5\n",
            "Epoch 86/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.0601 - accuracy: 0.9838 - val_loss: 0.0427 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.02953\n",
            "Epoch 87/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0880 - accuracy: 0.9540 - val_loss: 0.0309 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.02953\n",
            "Epoch 88/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0612 - accuracy: 0.9737 - val_loss: 0.0309 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.02953\n",
            "Epoch 89/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0796 - accuracy: 0.9538 - val_loss: 0.0323 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.02953\n",
            "Epoch 90/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0892 - accuracy: 0.9704 - val_loss: 0.0271 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00090: val_loss improved from 0.02953 to 0.02715, saving model to ./model/90-0.0271.hdf5\n",
            "Epoch 91/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0624 - accuracy: 0.9841 - val_loss: 0.0290 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.02715\n",
            "Epoch 92/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.0896 - accuracy: 0.9627 - val_loss: 0.0260 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00092: val_loss improved from 0.02715 to 0.02604, saving model to ./model/92-0.0260.hdf5\n",
            "Epoch 93/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0633 - accuracy: 0.9755 - val_loss: 0.0356 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.02604\n",
            "Epoch 94/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0510 - accuracy: 0.9756 - val_loss: 0.0287 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.02604\n",
            "Epoch 95/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0699 - accuracy: 0.9720 - val_loss: 0.0283 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.02604\n",
            "Epoch 96/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.0788 - accuracy: 0.9666 - val_loss: 0.0244 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00096: val_loss improved from 0.02604 to 0.02439, saving model to ./model/96-0.0244.hdf5\n",
            "Epoch 97/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0512 - accuracy: 0.9743 - val_loss: 0.0273 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.02439\n",
            "Epoch 98/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.0562 - accuracy: 0.9740 - val_loss: 0.0264 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.02439\n",
            "Epoch 99/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0513 - accuracy: 0.9910 - val_loss: 0.0271 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.02439\n",
            "Epoch 100/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.0828 - accuracy: 0.9747 - val_loss: 0.0260 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.02439\n",
            "1/1 [==============================] - 0s 300ms/step - loss: 0.0260 - accuracy: 1.0000\n",
            "Epoch 1/100\n",
            "27/27 [==============================] - 1s 15ms/step - loss: 1.0350 - accuracy: 0.3694 - val_loss: 1.0135 - val_accuracy: 0.4000\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 1.01349, saving model to ./model/01-1.0135.hdf5\n",
            "Epoch 2/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.9852 - accuracy: 0.5066 - val_loss: 0.9552 - val_accuracy: 0.4667\n",
            "\n",
            "Epoch 00002: val_loss improved from 1.01349 to 0.95517, saving model to ./model/02-0.9552.hdf5\n",
            "Epoch 3/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.9217 - accuracy: 0.3896 - val_loss: 0.9133 - val_accuracy: 0.3333\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.95517 to 0.91333, saving model to ./model/03-0.9133.hdf5\n",
            "Epoch 4/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.9048 - accuracy: 0.4272 - val_loss: 0.8618 - val_accuracy: 0.6000\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.91333 to 0.86176, saving model to ./model/04-0.8618.hdf5\n",
            "Epoch 5/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.8155 - accuracy: 0.6782 - val_loss: 0.7957 - val_accuracy: 0.6000\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.86176 to 0.79573, saving model to ./model/05-0.7957.hdf5\n",
            "Epoch 6/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.7548 - accuracy: 0.7189 - val_loss: 0.7351 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.79573 to 0.73513, saving model to ./model/06-0.7351.hdf5\n",
            "Epoch 7/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.7062 - accuracy: 0.7297 - val_loss: 0.6761 - val_accuracy: 0.8000\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.73513 to 0.67611, saving model to ./model/07-0.6761.hdf5\n",
            "Epoch 8/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.6491 - accuracy: 0.8373 - val_loss: 0.6072 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.67611 to 0.60722, saving model to ./model/08-0.6072.hdf5\n",
            "Epoch 9/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.5321 - accuracy: 0.8036 - val_loss: 0.5670 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00009: val_loss improved from 0.60722 to 0.56696, saving model to ./model/09-0.5670.hdf5\n",
            "Epoch 10/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.5146 - accuracy: 0.9574 - val_loss: 0.4916 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00010: val_loss improved from 0.56696 to 0.49156, saving model to ./model/10-0.4916.hdf5\n",
            "Epoch 11/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.4786 - accuracy: 0.9022 - val_loss: 0.4443 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00011: val_loss improved from 0.49156 to 0.44434, saving model to ./model/11-0.4443.hdf5\n",
            "Epoch 12/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.4456 - accuracy: 0.9159 - val_loss: 0.4080 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00012: val_loss improved from 0.44434 to 0.40796, saving model to ./model/12-0.4080.hdf5\n",
            "Epoch 13/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.3857 - accuracy: 0.9376 - val_loss: 0.3814 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00013: val_loss improved from 0.40796 to 0.38138, saving model to ./model/13-0.3814.hdf5\n",
            "Epoch 14/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.3551 - accuracy: 0.9330 - val_loss: 0.3477 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00014: val_loss improved from 0.38138 to 0.34768, saving model to ./model/14-0.3477.hdf5\n",
            "Epoch 15/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.3361 - accuracy: 0.9695 - val_loss: 0.3159 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00015: val_loss improved from 0.34768 to 0.31585, saving model to ./model/15-0.3159.hdf5\n",
            "Epoch 16/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.2965 - accuracy: 0.9933 - val_loss: 0.2777 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00016: val_loss improved from 0.31585 to 0.27769, saving model to ./model/16-0.2777.hdf5\n",
            "Epoch 17/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.2877 - accuracy: 0.9426 - val_loss: 0.2725 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00017: val_loss improved from 0.27769 to 0.27249, saving model to ./model/17-0.2725.hdf5\n",
            "Epoch 18/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.2526 - accuracy: 0.9759 - val_loss: 0.2493 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00018: val_loss improved from 0.27249 to 0.24933, saving model to ./model/18-0.2493.hdf5\n",
            "Epoch 19/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.2619 - accuracy: 0.9486 - val_loss: 0.2337 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00019: val_loss improved from 0.24933 to 0.23369, saving model to ./model/19-0.2337.hdf5\n",
            "Epoch 20/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.2287 - accuracy: 0.9657 - val_loss: 0.2241 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00020: val_loss improved from 0.23369 to 0.22410, saving model to ./model/20-0.2241.hdf5\n",
            "Epoch 21/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.2334 - accuracy: 0.9603 - val_loss: 0.2005 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00021: val_loss improved from 0.22410 to 0.20046, saving model to ./model/21-0.2005.hdf5\n",
            "Epoch 22/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.2118 - accuracy: 0.9793 - val_loss: 0.2010 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.20046\n",
            "Epoch 23/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1947 - accuracy: 0.9903 - val_loss: 0.1990 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00023: val_loss improved from 0.20046 to 0.19899, saving model to ./model/23-0.1990.hdf5\n",
            "Epoch 24/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.2019 - accuracy: 0.9798 - val_loss: 0.1898 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00024: val_loss improved from 0.19899 to 0.18982, saving model to ./model/24-0.1898.hdf5\n",
            "Epoch 25/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.2030 - accuracy: 0.9818 - val_loss: 0.1878 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00025: val_loss improved from 0.18982 to 0.18778, saving model to ./model/25-0.1878.hdf5\n",
            "Epoch 26/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1522 - accuracy: 0.9936 - val_loss: 0.1540 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00026: val_loss improved from 0.18778 to 0.15400, saving model to ./model/26-0.1540.hdf5\n",
            "Epoch 27/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1703 - accuracy: 0.9921 - val_loss: 0.1852 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.15400\n",
            "Epoch 28/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1739 - accuracy: 0.9454 - val_loss: 0.1416 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00028: val_loss improved from 0.15400 to 0.14158, saving model to ./model/28-0.1416.hdf5\n",
            "Epoch 29/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1208 - accuracy: 0.9918 - val_loss: 0.1373 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00029: val_loss improved from 0.14158 to 0.13733, saving model to ./model/29-0.1373.hdf5\n",
            "Epoch 30/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1462 - accuracy: 0.9615 - val_loss: 0.1572 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.13733\n",
            "Epoch 31/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1228 - accuracy: 0.9700 - val_loss: 0.1342 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00031: val_loss improved from 0.13733 to 0.13422, saving model to ./model/31-0.1342.hdf5\n",
            "Epoch 32/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1458 - accuracy: 0.9855 - val_loss: 0.1380 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.13422\n",
            "Epoch 33/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1397 - accuracy: 0.9709 - val_loss: 0.1218 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00033: val_loss improved from 0.13422 to 0.12178, saving model to ./model/33-0.1218.hdf5\n",
            "Epoch 34/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1203 - accuracy: 0.9649 - val_loss: 0.1059 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00034: val_loss improved from 0.12178 to 0.10587, saving model to ./model/34-0.1059.hdf5\n",
            "Epoch 35/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1385 - accuracy: 0.9528 - val_loss: 0.1110 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.10587\n",
            "Epoch 36/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1095 - accuracy: 0.9897 - val_loss: 0.1301 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.10587\n",
            "Epoch 37/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1183 - accuracy: 0.9569 - val_loss: 0.1206 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.10587\n",
            "Epoch 38/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1121 - accuracy: 0.9744 - val_loss: 0.0795 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00038: val_loss improved from 0.10587 to 0.07948, saving model to ./model/38-0.0795.hdf5\n",
            "Epoch 39/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1459 - accuracy: 0.9517 - val_loss: 0.1345 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.07948\n",
            "Epoch 40/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1136 - accuracy: 0.9649 - val_loss: 0.0969 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.07948\n",
            "Epoch 41/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1097 - accuracy: 0.9809 - val_loss: 0.0918 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.07948\n",
            "Epoch 42/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1362 - accuracy: 0.9546 - val_loss: 0.0865 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.07948\n",
            "Epoch 43/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0886 - accuracy: 0.9788 - val_loss: 0.1497 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.07948\n",
            "Epoch 44/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1254 - accuracy: 0.9349 - val_loss: 0.0901 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.07948\n",
            "Epoch 45/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0877 - accuracy: 0.9757 - val_loss: 0.0921 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.07948\n",
            "Epoch 46/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0920 - accuracy: 0.9768 - val_loss: 0.1508 - val_accuracy: 0.8667\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.07948\n",
            "Epoch 47/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1181 - accuracy: 0.9493 - val_loss: 0.0915 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.07948\n",
            "Epoch 48/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0936 - accuracy: 0.9742 - val_loss: 0.1075 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.07948\n",
            "Epoch 49/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1116 - accuracy: 0.9717 - val_loss: 0.0679 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00049: val_loss improved from 0.07948 to 0.06785, saving model to ./model/49-0.0679.hdf5\n",
            "Epoch 50/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1066 - accuracy: 0.9659 - val_loss: 0.0771 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.06785\n",
            "Epoch 51/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.0963 - accuracy: 0.9777 - val_loss: 0.0911 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.06785\n",
            "Epoch 52/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0934 - accuracy: 0.9717 - val_loss: 0.0616 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00052: val_loss improved from 0.06785 to 0.06162, saving model to ./model/52-0.0616.hdf5\n",
            "Epoch 53/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.0929 - accuracy: 0.9692 - val_loss: 0.0803 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.06162\n",
            "Epoch 54/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0988 - accuracy: 0.9432 - val_loss: 0.0559 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00054: val_loss improved from 0.06162 to 0.05589, saving model to ./model/54-0.0559.hdf5\n",
            "Epoch 55/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0926 - accuracy: 0.9683 - val_loss: 0.1081 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.05589\n",
            "Epoch 56/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0634 - accuracy: 0.9830 - val_loss: 0.0669 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.05589\n",
            "Epoch 57/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0744 - accuracy: 0.9809 - val_loss: 0.1093 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.05589\n",
            "Epoch 58/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0813 - accuracy: 0.9562 - val_loss: 0.0555 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00058: val_loss improved from 0.05589 to 0.05555, saving model to ./model/58-0.0555.hdf5\n",
            "Epoch 59/100\n",
            "27/27 [==============================] - 0s 5ms/step - loss: 0.0948 - accuracy: 0.9720 - val_loss: 0.0490 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00059: val_loss improved from 0.05555 to 0.04902, saving model to ./model/59-0.0490.hdf5\n",
            "Epoch 60/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0835 - accuracy: 0.9697 - val_loss: 0.0570 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.04902\n",
            "Epoch 61/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1075 - accuracy: 0.9432 - val_loss: 0.0603 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.04902\n",
            "Epoch 62/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0733 - accuracy: 0.9909 - val_loss: 0.1291 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.04902\n",
            "Epoch 63/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0861 - accuracy: 0.9633 - val_loss: 0.0572 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.04902\n",
            "Epoch 64/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.0740 - accuracy: 0.9794 - val_loss: 0.0741 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.04902\n",
            "Epoch 65/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0762 - accuracy: 0.9800 - val_loss: 0.0594 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.04902\n",
            "Epoch 66/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0731 - accuracy: 0.9887 - val_loss: 0.1043 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.04902\n",
            "Epoch 67/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.0668 - accuracy: 0.9756 - val_loss: 0.0802 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.04902\n",
            "Epoch 68/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0855 - accuracy: 0.9722 - val_loss: 0.0402 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00068: val_loss improved from 0.04902 to 0.04015, saving model to ./model/68-0.0402.hdf5\n",
            "Epoch 69/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0968 - accuracy: 0.9518 - val_loss: 0.0732 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.04015\n",
            "Epoch 70/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0923 - accuracy: 0.9641 - val_loss: 0.0638 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.04015\n",
            "Epoch 71/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0830 - accuracy: 0.9628 - val_loss: 0.0471 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.04015\n",
            "Epoch 72/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0619 - accuracy: 0.9786 - val_loss: 0.0453 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.04015\n",
            "Epoch 73/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0888 - accuracy: 0.9893 - val_loss: 0.1159 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.04015\n",
            "Epoch 74/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0662 - accuracy: 0.9723 - val_loss: 0.0453 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.04015\n",
            "Epoch 75/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0866 - accuracy: 0.9780 - val_loss: 0.0483 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.04015\n",
            "Epoch 76/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0810 - accuracy: 0.9757 - val_loss: 0.0767 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.04015\n",
            "Epoch 77/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0889 - accuracy: 0.9660 - val_loss: 0.0486 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.04015\n",
            "Epoch 78/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0733 - accuracy: 0.9621 - val_loss: 0.0521 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.04015\n",
            "Epoch 79/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1048 - accuracy: 0.9775 - val_loss: 0.0795 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.04015\n",
            "Epoch 80/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1352 - accuracy: 0.9330 - val_loss: 0.0662 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.04015\n",
            "Epoch 81/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0593 - accuracy: 0.9801 - val_loss: 0.0299 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00081: val_loss improved from 0.04015 to 0.02992, saving model to ./model/81-0.0299.hdf5\n",
            "Epoch 82/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1177 - accuracy: 0.9624 - val_loss: 0.0615 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.02992\n",
            "Epoch 83/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0905 - accuracy: 0.9628 - val_loss: 0.0503 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.02992\n",
            "Epoch 84/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0480 - accuracy: 0.9903 - val_loss: 0.0489 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.02992\n",
            "Epoch 85/100\n",
            "27/27 [==============================] - 0s 6ms/step - loss: 0.1116 - accuracy: 0.9816 - val_loss: 0.0742 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.02992\n",
            "Epoch 86/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1172 - accuracy: 0.9523 - val_loss: 0.0394 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.02992\n",
            "Epoch 87/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0760 - accuracy: 0.9721 - val_loss: 0.0360 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.02992\n",
            "Epoch 88/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0856 - accuracy: 0.9722 - val_loss: 0.0685 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.02992\n",
            "Epoch 89/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0553 - accuracy: 0.9890 - val_loss: 0.0600 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.02992\n",
            "Epoch 90/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0605 - accuracy: 0.9907 - val_loss: 0.0842 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.02992\n",
            "Epoch 91/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0710 - accuracy: 0.9852 - val_loss: 0.0593 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.02992\n",
            "Epoch 92/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0544 - accuracy: 0.9860 - val_loss: 0.0738 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.02992\n",
            "Epoch 93/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0776 - accuracy: 0.9679 - val_loss: 0.0387 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.02992\n",
            "Epoch 94/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.0513 - accuracy: 0.9951 - val_loss: 0.0616 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.02992\n",
            "Epoch 95/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0607 - accuracy: 0.9794 - val_loss: 0.0448 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.02992\n",
            "Epoch 96/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0646 - accuracy: 0.9889 - val_loss: 0.0655 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.02992\n",
            "Epoch 97/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0940 - accuracy: 0.9542 - val_loss: 0.0304 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.02992\n",
            "Epoch 98/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1028 - accuracy: 0.9553 - val_loss: 0.0326 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.02992\n",
            "Epoch 99/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1042 - accuracy: 0.9569 - val_loss: 0.0486 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.02992\n",
            "Epoch 100/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0588 - accuracy: 0.9799 - val_loss: 0.0500 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.02992\n",
            "1/1 [==============================] - 0s 324ms/step - loss: 0.0500 - accuracy: 1.0000\n",
            "Epoch 1/100\n",
            "27/27 [==============================] - 1s 14ms/step - loss: 2.9937 - accuracy: 0.3389 - val_loss: 1.4894 - val_accuracy: 0.3333\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 1.48944, saving model to ./model/01-1.4894.hdf5\n",
            "Epoch 2/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 1.3085 - accuracy: 0.3179 - val_loss: 0.9744 - val_accuracy: 0.4000\n",
            "\n",
            "Epoch 00002: val_loss improved from 1.48944 to 0.97443, saving model to ./model/02-0.9744.hdf5\n",
            "Epoch 3/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.9740 - accuracy: 0.3191 - val_loss: 0.8938 - val_accuracy: 0.3333\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.97443 to 0.89380, saving model to ./model/03-0.8938.hdf5\n",
            "Epoch 4/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.8438 - accuracy: 0.4234 - val_loss: 0.8441 - val_accuracy: 0.6000\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.89380 to 0.84412, saving model to ./model/04-0.8441.hdf5\n",
            "Epoch 5/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.8672 - accuracy: 0.5138 - val_loss: 0.8043 - val_accuracy: 0.8000\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.84412 to 0.80427, saving model to ./model/05-0.8043.hdf5\n",
            "Epoch 6/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.8080 - accuracy: 0.7642 - val_loss: 0.7703 - val_accuracy: 0.8667\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.80427 to 0.77030, saving model to ./model/06-0.7703.hdf5\n",
            "Epoch 7/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.7770 - accuracy: 0.7421 - val_loss: 0.7402 - val_accuracy: 0.8667\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.77030 to 0.74020, saving model to ./model/07-0.7402.hdf5\n",
            "Epoch 8/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.7375 - accuracy: 0.7577 - val_loss: 0.7099 - val_accuracy: 0.8667\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.74020 to 0.70986, saving model to ./model/08-0.7099.hdf5\n",
            "Epoch 9/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.6931 - accuracy: 0.8272 - val_loss: 0.6838 - val_accuracy: 0.8667\n",
            "\n",
            "Epoch 00009: val_loss improved from 0.70986 to 0.68376, saving model to ./model/09-0.6838.hdf5\n",
            "Epoch 10/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.6643 - accuracy: 0.8315 - val_loss: 0.6559 - val_accuracy: 0.8667\n",
            "\n",
            "Epoch 00010: val_loss improved from 0.68376 to 0.65592, saving model to ./model/10-0.6559.hdf5\n",
            "Epoch 11/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.6596 - accuracy: 0.9056 - val_loss: 0.6304 - val_accuracy: 0.8667\n",
            "\n",
            "Epoch 00011: val_loss improved from 0.65592 to 0.63037, saving model to ./model/11-0.6304.hdf5\n",
            "Epoch 12/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.5891 - accuracy: 0.8740 - val_loss: 0.6017 - val_accuracy: 0.8667\n",
            "\n",
            "Epoch 00012: val_loss improved from 0.63037 to 0.60168, saving model to ./model/12-0.6017.hdf5\n",
            "Epoch 13/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.6053 - accuracy: 0.9765 - val_loss: 0.5759 - val_accuracy: 0.8667\n",
            "\n",
            "Epoch 00013: val_loss improved from 0.60168 to 0.57591, saving model to ./model/13-0.5759.hdf5\n",
            "Epoch 14/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.5730 - accuracy: 0.9755 - val_loss: 0.5477 - val_accuracy: 0.8667\n",
            "\n",
            "Epoch 00014: val_loss improved from 0.57591 to 0.54770, saving model to ./model/14-0.5477.hdf5\n",
            "Epoch 15/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.5317 - accuracy: 0.9826 - val_loss: 0.5195 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00015: val_loss improved from 0.54770 to 0.51949, saving model to ./model/15-0.5195.hdf5\n",
            "Epoch 16/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.5067 - accuracy: 0.9272 - val_loss: 0.4886 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00016: val_loss improved from 0.51949 to 0.48858, saving model to ./model/16-0.4886.hdf5\n",
            "Epoch 17/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.4528 - accuracy: 0.9392 - val_loss: 0.4569 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00017: val_loss improved from 0.48858 to 0.45687, saving model to ./model/17-0.4569.hdf5\n",
            "Epoch 18/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.4615 - accuracy: 0.9814 - val_loss: 0.4242 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00018: val_loss improved from 0.45687 to 0.42422, saving model to ./model/18-0.4242.hdf5\n",
            "Epoch 19/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.4241 - accuracy: 0.9548 - val_loss: 0.3944 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00019: val_loss improved from 0.42422 to 0.39436, saving model to ./model/19-0.3944.hdf5\n",
            "Epoch 20/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.3802 - accuracy: 0.9843 - val_loss: 0.3676 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00020: val_loss improved from 0.39436 to 0.36762, saving model to ./model/20-0.3676.hdf5\n",
            "Epoch 21/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.3709 - accuracy: 0.9599 - val_loss: 0.3454 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00021: val_loss improved from 0.36762 to 0.34538, saving model to ./model/21-0.3454.hdf5\n",
            "Epoch 22/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.3456 - accuracy: 0.9567 - val_loss: 0.3160 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00022: val_loss improved from 0.34538 to 0.31597, saving model to ./model/22-0.3160.hdf5\n",
            "Epoch 23/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.2948 - accuracy: 0.9805 - val_loss: 0.2925 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00023: val_loss improved from 0.31597 to 0.29254, saving model to ./model/23-0.2925.hdf5\n",
            "Epoch 24/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.2931 - accuracy: 0.9561 - val_loss: 0.3029 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.29254\n",
            "Epoch 25/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.2580 - accuracy: 0.9457 - val_loss: 0.2666 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00025: val_loss improved from 0.29254 to 0.26657, saving model to ./model/25-0.2666.hdf5\n",
            "Epoch 26/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.2736 - accuracy: 0.9326 - val_loss: 0.2555 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00026: val_loss improved from 0.26657 to 0.25552, saving model to ./model/26-0.2555.hdf5\n",
            "Epoch 27/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.2087 - accuracy: 0.9705 - val_loss: 0.2449 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00027: val_loss improved from 0.25552 to 0.24487, saving model to ./model/27-0.2449.hdf5\n",
            "Epoch 28/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.2159 - accuracy: 0.9615 - val_loss: 0.2260 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00028: val_loss improved from 0.24487 to 0.22595, saving model to ./model/28-0.2260.hdf5\n",
            "Epoch 29/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.2351 - accuracy: 0.9536 - val_loss: 0.2296 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.22595\n",
            "Epoch 30/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1915 - accuracy: 0.9611 - val_loss: 0.2157 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00030: val_loss improved from 0.22595 to 0.21571, saving model to ./model/30-0.2157.hdf5\n",
            "Epoch 31/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1736 - accuracy: 0.9796 - val_loss: 0.2077 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00031: val_loss improved from 0.21571 to 0.20771, saving model to ./model/31-0.2077.hdf5\n",
            "Epoch 32/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1769 - accuracy: 0.9663 - val_loss: 0.1972 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00032: val_loss improved from 0.20771 to 0.19725, saving model to ./model/32-0.1972.hdf5\n",
            "Epoch 33/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1901 - accuracy: 0.9659 - val_loss: 0.1958 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00033: val_loss improved from 0.19725 to 0.19584, saving model to ./model/33-0.1958.hdf5\n",
            "Epoch 34/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1601 - accuracy: 0.9733 - val_loss: 0.1803 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00034: val_loss improved from 0.19584 to 0.18031, saving model to ./model/34-0.1803.hdf5\n",
            "Epoch 35/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1643 - accuracy: 0.9452 - val_loss: 0.1821 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.18031\n",
            "Epoch 36/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1141 - accuracy: 0.9905 - val_loss: 0.1740 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00036: val_loss improved from 0.18031 to 0.17405, saving model to ./model/36-0.1740.hdf5\n",
            "Epoch 37/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1274 - accuracy: 0.9874 - val_loss: 0.1797 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.17405\n",
            "Epoch 38/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1515 - accuracy: 0.9758 - val_loss: 0.1669 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00038: val_loss improved from 0.17405 to 0.16689, saving model to ./model/38-0.1669.hdf5\n",
            "Epoch 39/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1239 - accuracy: 0.9902 - val_loss: 0.1708 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.16689\n",
            "Epoch 40/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1540 - accuracy: 0.9586 - val_loss: 0.1569 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00040: val_loss improved from 0.16689 to 0.15689, saving model to ./model/40-0.1569.hdf5\n",
            "Epoch 41/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1786 - accuracy: 0.9399 - val_loss: 0.1603 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.15689\n",
            "Epoch 42/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1047 - accuracy: 0.9818 - val_loss: 0.1571 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.15689\n",
            "Epoch 43/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1205 - accuracy: 0.9589 - val_loss: 0.1599 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.15689\n",
            "Epoch 44/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1208 - accuracy: 0.9805 - val_loss: 0.1691 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.15689\n",
            "Epoch 45/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1449 - accuracy: 0.9464 - val_loss: 0.1690 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.15689\n",
            "Epoch 46/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1129 - accuracy: 0.9612 - val_loss: 0.1551 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00046: val_loss improved from 0.15689 to 0.15506, saving model to ./model/46-0.1551.hdf5\n",
            "Epoch 47/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1203 - accuracy: 0.9697 - val_loss: 0.1586 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.15506\n",
            "Epoch 48/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0992 - accuracy: 0.9761 - val_loss: 0.1937 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.15506\n",
            "Epoch 49/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1587 - accuracy: 0.9433 - val_loss: 0.1681 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.15506\n",
            "Epoch 50/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1236 - accuracy: 0.9720 - val_loss: 0.1660 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.15506\n",
            "Epoch 51/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1153 - accuracy: 0.9622 - val_loss: 0.1401 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00051: val_loss improved from 0.15506 to 0.14005, saving model to ./model/51-0.1401.hdf5\n",
            "Epoch 52/100\n",
            "27/27 [==============================] - 0s 5ms/step - loss: 0.0806 - accuracy: 0.9922 - val_loss: 0.1846 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.14005\n",
            "Epoch 53/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0814 - accuracy: 0.9719 - val_loss: 0.1432 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.14005\n",
            "Epoch 54/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1137 - accuracy: 0.9765 - val_loss: 0.1766 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.14005\n",
            "Epoch 55/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.0894 - accuracy: 0.9607 - val_loss: 0.1491 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.14005\n",
            "Epoch 56/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.0830 - accuracy: 0.9921 - val_loss: 0.1274 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00056: val_loss improved from 0.14005 to 0.12735, saving model to ./model/56-0.1274.hdf5\n",
            "Epoch 57/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0860 - accuracy: 0.9733 - val_loss: 0.1617 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.12735\n",
            "Epoch 58/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0755 - accuracy: 0.9766 - val_loss: 0.1370 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.12735\n",
            "Epoch 59/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0986 - accuracy: 0.9782 - val_loss: 0.1957 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.12735\n",
            "Epoch 60/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0932 - accuracy: 0.9642 - val_loss: 0.1562 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.12735\n",
            "Epoch 61/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0968 - accuracy: 0.9674 - val_loss: 0.1205 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00061: val_loss improved from 0.12735 to 0.12054, saving model to ./model/61-0.1205.hdf5\n",
            "Epoch 62/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1330 - accuracy: 0.9495 - val_loss: 0.1491 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.12054\n",
            "Epoch 63/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0998 - accuracy: 0.9574 - val_loss: 0.1282 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.12054\n",
            "Epoch 64/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0833 - accuracy: 0.9760 - val_loss: 0.1370 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.12054\n",
            "Epoch 65/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0926 - accuracy: 0.9859 - val_loss: 0.1250 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.12054\n",
            "Epoch 66/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1091 - accuracy: 0.9586 - val_loss: 0.1512 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.12054\n",
            "Epoch 67/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0877 - accuracy: 0.9749 - val_loss: 0.1525 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.12054\n",
            "Epoch 68/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0743 - accuracy: 0.9903 - val_loss: 0.1170 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00068: val_loss improved from 0.12054 to 0.11699, saving model to ./model/68-0.1170.hdf5\n",
            "Epoch 69/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0906 - accuracy: 0.9488 - val_loss: 0.1247 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.11699\n",
            "Epoch 70/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0830 - accuracy: 0.9716 - val_loss: 0.1807 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.11699\n",
            "Epoch 71/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0796 - accuracy: 0.9634 - val_loss: 0.1143 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00071: val_loss improved from 0.11699 to 0.11426, saving model to ./model/71-0.1143.hdf5\n",
            "Epoch 72/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0970 - accuracy: 0.9623 - val_loss: 0.1353 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.11426\n",
            "Epoch 73/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0781 - accuracy: 0.9907 - val_loss: 0.1128 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00073: val_loss improved from 0.11426 to 0.11281, saving model to ./model/73-0.1128.hdf5\n",
            "Epoch 74/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1109 - accuracy: 0.9255 - val_loss: 0.1437 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.11281\n",
            "Epoch 75/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0529 - accuracy: 0.9866 - val_loss: 0.1149 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.11281\n",
            "Epoch 76/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0936 - accuracy: 0.9614 - val_loss: 0.1543 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.11281\n",
            "Epoch 77/100\n",
            "27/27 [==============================] - 0s 6ms/step - loss: 0.0787 - accuracy: 0.9771 - val_loss: 0.1266 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.11281\n",
            "Epoch 78/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0568 - accuracy: 0.9733 - val_loss: 0.1269 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.11281\n",
            "Epoch 79/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1174 - accuracy: 0.9623 - val_loss: 0.1649 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.11281\n",
            "Epoch 80/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0672 - accuracy: 0.9902 - val_loss: 0.1085 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00080: val_loss improved from 0.11281 to 0.10854, saving model to ./model/80-0.1085.hdf5\n",
            "Epoch 81/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0737 - accuracy: 0.9659 - val_loss: 0.1577 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.10854\n",
            "Epoch 82/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0639 - accuracy: 0.9873 - val_loss: 0.1189 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.10854\n",
            "Epoch 83/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0897 - accuracy: 0.9687 - val_loss: 0.1499 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.10854\n",
            "Epoch 84/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.0762 - accuracy: 0.9822 - val_loss: 0.1438 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.10854\n",
            "Epoch 85/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.0603 - accuracy: 0.9685 - val_loss: 0.1379 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.10854\n",
            "Epoch 86/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0546 - accuracy: 0.9860 - val_loss: 0.1403 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.10854\n",
            "Epoch 87/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0696 - accuracy: 0.9894 - val_loss: 0.1473 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.10854\n",
            "Epoch 88/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0720 - accuracy: 0.9684 - val_loss: 0.1370 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.10854\n",
            "Epoch 89/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1014 - accuracy: 0.9688 - val_loss: 0.2006 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.10854\n",
            "Epoch 90/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0896 - accuracy: 0.9777 - val_loss: 0.1164 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.10854\n",
            "Epoch 91/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0422 - accuracy: 0.9803 - val_loss: 0.1174 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.10854\n",
            "Epoch 92/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0846 - accuracy: 0.9485 - val_loss: 0.1131 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.10854\n",
            "Epoch 93/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0622 - accuracy: 0.9774 - val_loss: 0.1405 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.10854\n",
            "Epoch 94/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0643 - accuracy: 0.9912 - val_loss: 0.1263 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.10854\n",
            "Epoch 95/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0691 - accuracy: 0.9879 - val_loss: 0.1221 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.10854\n",
            "Epoch 96/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0551 - accuracy: 0.9937 - val_loss: 0.1177 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.10854\n",
            "Epoch 97/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0357 - accuracy: 0.9946 - val_loss: 0.1022 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00097: val_loss improved from 0.10854 to 0.10217, saving model to ./model/97-0.1022.hdf5\n",
            "Epoch 98/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0629 - accuracy: 0.9799 - val_loss: 0.1182 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.10217\n",
            "Epoch 99/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0723 - accuracy: 0.9781 - val_loss: 0.2076 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.10217\n",
            "Epoch 100/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0611 - accuracy: 0.9736 - val_loss: 0.1135 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.10217\n",
            "1/1 [==============================] - 0s 301ms/step - loss: 0.1135 - accuracy: 0.9333\n",
            "Epoch 1/100\n",
            "27/27 [==============================] - 1s 13ms/step - loss: 1.1674 - accuracy: 0.3727 - val_loss: 1.0789 - val_accuracy: 0.4667\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 1.07889, saving model to ./model/01-1.0789.hdf5\n",
            "Epoch 2/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 1.0698 - accuracy: 0.3116 - val_loss: 0.9406 - val_accuracy: 0.6000\n",
            "\n",
            "Epoch 00002: val_loss improved from 1.07889 to 0.94064, saving model to ./model/02-0.9406.hdf5\n",
            "Epoch 3/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.9517 - accuracy: 0.6389 - val_loss: 0.8533 - val_accuracy: 0.6667\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.94064 to 0.85331, saving model to ./model/03-0.8533.hdf5\n",
            "Epoch 4/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.8741 - accuracy: 0.7574 - val_loss: 0.7646 - val_accuracy: 0.6667\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.85331 to 0.76456, saving model to ./model/04-0.7646.hdf5\n",
            "Epoch 5/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.7931 - accuracy: 0.7549 - val_loss: 0.6931 - val_accuracy: 0.8667\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.76456 to 0.69308, saving model to ./model/05-0.6931.hdf5\n",
            "Epoch 6/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.7017 - accuracy: 0.7785 - val_loss: 0.6340 - val_accuracy: 0.8667\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.69308 to 0.63403, saving model to ./model/06-0.6340.hdf5\n",
            "Epoch 7/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.6967 - accuracy: 0.8054 - val_loss: 0.5807 - val_accuracy: 0.8000\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.63403 to 0.58072, saving model to ./model/07-0.5807.hdf5\n",
            "Epoch 8/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.6193 - accuracy: 0.7806 - val_loss: 0.5401 - val_accuracy: 0.8667\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.58072 to 0.54012, saving model to ./model/08-0.5401.hdf5\n",
            "Epoch 9/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.5853 - accuracy: 0.8724 - val_loss: 0.5060 - val_accuracy: 0.6667\n",
            "\n",
            "Epoch 00009: val_loss improved from 0.54012 to 0.50598, saving model to ./model/09-0.5060.hdf5\n",
            "Epoch 10/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.5525 - accuracy: 0.7014 - val_loss: 0.4749 - val_accuracy: 0.7333\n",
            "\n",
            "Epoch 00010: val_loss improved from 0.50598 to 0.47494, saving model to ./model/10-0.4749.hdf5\n",
            "Epoch 11/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.4929 - accuracy: 0.8503 - val_loss: 0.4485 - val_accuracy: 0.8667\n",
            "\n",
            "Epoch 00011: val_loss improved from 0.47494 to 0.44849, saving model to ./model/11-0.4485.hdf5\n",
            "Epoch 12/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.4575 - accuracy: 0.7562 - val_loss: 0.4318 - val_accuracy: 0.7333\n",
            "\n",
            "Epoch 00012: val_loss improved from 0.44849 to 0.43177, saving model to ./model/12-0.4318.hdf5\n",
            "Epoch 13/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.4797 - accuracy: 0.8252 - val_loss: 0.4113 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00013: val_loss improved from 0.43177 to 0.41132, saving model to ./model/13-0.4113.hdf5\n",
            "Epoch 14/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.4343 - accuracy: 0.9002 - val_loss: 0.3955 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00014: val_loss improved from 0.41132 to 0.39545, saving model to ./model/14-0.3955.hdf5\n",
            "Epoch 15/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.4255 - accuracy: 0.9089 - val_loss: 0.3860 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00015: val_loss improved from 0.39545 to 0.38602, saving model to ./model/15-0.3860.hdf5\n",
            "Epoch 16/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.4172 - accuracy: 0.8830 - val_loss: 0.3696 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00016: val_loss improved from 0.38602 to 0.36959, saving model to ./model/16-0.3696.hdf5\n",
            "Epoch 17/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.4209 - accuracy: 0.9209 - val_loss: 0.3642 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00017: val_loss improved from 0.36959 to 0.36415, saving model to ./model/17-0.3642.hdf5\n",
            "Epoch 18/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.4137 - accuracy: 0.8888 - val_loss: 0.3478 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00018: val_loss improved from 0.36415 to 0.34782, saving model to ./model/18-0.3478.hdf5\n",
            "Epoch 19/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.3796 - accuracy: 0.9202 - val_loss: 0.3371 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00019: val_loss improved from 0.34782 to 0.33714, saving model to ./model/19-0.3371.hdf5\n",
            "Epoch 20/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.3878 - accuracy: 0.9241 - val_loss: 0.3362 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00020: val_loss improved from 0.33714 to 0.33620, saving model to ./model/20-0.3362.hdf5\n",
            "Epoch 21/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.3715 - accuracy: 0.8946 - val_loss: 0.3175 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00021: val_loss improved from 0.33620 to 0.31751, saving model to ./model/21-0.3175.hdf5\n",
            "Epoch 22/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.3773 - accuracy: 0.9681 - val_loss: 0.3134 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00022: val_loss improved from 0.31751 to 0.31337, saving model to ./model/22-0.3134.hdf5\n",
            "Epoch 23/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.3335 - accuracy: 0.9708 - val_loss: 0.3015 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00023: val_loss improved from 0.31337 to 0.30150, saving model to ./model/23-0.3015.hdf5\n",
            "Epoch 24/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.3690 - accuracy: 0.9602 - val_loss: 0.2958 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00024: val_loss improved from 0.30150 to 0.29578, saving model to ./model/24-0.2958.hdf5\n",
            "Epoch 25/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.3144 - accuracy: 0.9705 - val_loss: 0.2858 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00025: val_loss improved from 0.29578 to 0.28581, saving model to ./model/25-0.2858.hdf5\n",
            "Epoch 26/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.3039 - accuracy: 0.9525 - val_loss: 0.2731 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00026: val_loss improved from 0.28581 to 0.27314, saving model to ./model/26-0.2731.hdf5\n",
            "Epoch 27/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.2877 - accuracy: 0.9491 - val_loss: 0.2732 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.27314\n",
            "Epoch 28/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.2883 - accuracy: 0.9771 - val_loss: 0.2619 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00028: val_loss improved from 0.27314 to 0.26193, saving model to ./model/28-0.2619.hdf5\n",
            "Epoch 29/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.2985 - accuracy: 0.9640 - val_loss: 0.2614 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00029: val_loss improved from 0.26193 to 0.26143, saving model to ./model/29-0.2614.hdf5\n",
            "Epoch 30/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.2577 - accuracy: 0.9730 - val_loss: 0.2356 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00030: val_loss improved from 0.26143 to 0.23559, saving model to ./model/30-0.2356.hdf5\n",
            "Epoch 31/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.2578 - accuracy: 0.9571 - val_loss: 0.2293 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00031: val_loss improved from 0.23559 to 0.22930, saving model to ./model/31-0.2293.hdf5\n",
            "Epoch 32/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.2570 - accuracy: 0.9246 - val_loss: 0.2273 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00032: val_loss improved from 0.22930 to 0.22728, saving model to ./model/32-0.2273.hdf5\n",
            "Epoch 33/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.2271 - accuracy: 0.9684 - val_loss: 0.2209 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00033: val_loss improved from 0.22728 to 0.22094, saving model to ./model/33-0.2209.hdf5\n",
            "Epoch 34/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.2541 - accuracy: 0.9621 - val_loss: 0.2101 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00034: val_loss improved from 0.22094 to 0.21006, saving model to ./model/34-0.2101.hdf5\n",
            "Epoch 35/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.2485 - accuracy: 0.9803 - val_loss: 0.2222 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.21006\n",
            "Epoch 36/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.2254 - accuracy: 0.9770 - val_loss: 0.2065 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00036: val_loss improved from 0.21006 to 0.20648, saving model to ./model/36-0.2065.hdf5\n",
            "Epoch 37/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.2227 - accuracy: 0.9894 - val_loss: 0.1986 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00037: val_loss improved from 0.20648 to 0.19860, saving model to ./model/37-0.1986.hdf5\n",
            "Epoch 38/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.2256 - accuracy: 0.9622 - val_loss: 0.1753 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00038: val_loss improved from 0.19860 to 0.17528, saving model to ./model/38-0.1753.hdf5\n",
            "Epoch 39/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.2152 - accuracy: 0.9689 - val_loss: 0.1998 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.17528\n",
            "Epoch 40/100\n",
            "27/27 [==============================] - 0s 5ms/step - loss: 0.1973 - accuracy: 0.9811 - val_loss: 0.2061 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.17528\n",
            "Epoch 41/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.2098 - accuracy: 0.9618 - val_loss: 0.1692 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00041: val_loss improved from 0.17528 to 0.16919, saving model to ./model/41-0.1692.hdf5\n",
            "Epoch 42/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.2235 - accuracy: 0.9539 - val_loss: 0.1514 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00042: val_loss improved from 0.16919 to 0.15143, saving model to ./model/42-0.1514.hdf5\n",
            "Epoch 43/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1847 - accuracy: 0.9679 - val_loss: 0.1616 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.15143\n",
            "Epoch 44/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1839 - accuracy: 0.9792 - val_loss: 0.1740 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.15143\n",
            "Epoch 45/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1675 - accuracy: 0.9705 - val_loss: 0.1976 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.15143\n",
            "Epoch 46/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1803 - accuracy: 0.9491 - val_loss: 0.2075 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.15143\n",
            "Epoch 47/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1790 - accuracy: 0.9696 - val_loss: 0.1497 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00047: val_loss improved from 0.15143 to 0.14972, saving model to ./model/47-0.1497.hdf5\n",
            "Epoch 48/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1548 - accuracy: 0.9864 - val_loss: 0.1848 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.14972\n",
            "Epoch 49/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1567 - accuracy: 0.9736 - val_loss: 0.1460 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00049: val_loss improved from 0.14972 to 0.14598, saving model to ./model/49-0.1460.hdf5\n",
            "Epoch 50/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1589 - accuracy: 0.9855 - val_loss: 0.1663 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.14598\n",
            "Epoch 51/100\n",
            "27/27 [==============================] - 0s 5ms/step - loss: 0.1611 - accuracy: 0.9764 - val_loss: 0.1605 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.14598\n",
            "Epoch 52/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1306 - accuracy: 0.9786 - val_loss: 0.1591 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.14598\n",
            "Epoch 53/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1462 - accuracy: 0.9948 - val_loss: 0.1455 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00053: val_loss improved from 0.14598 to 0.14551, saving model to ./model/53-0.1455.hdf5\n",
            "Epoch 54/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1292 - accuracy: 0.9898 - val_loss: 0.1537 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.14551\n",
            "Epoch 55/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1134 - accuracy: 0.9904 - val_loss: 0.1664 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.14551\n",
            "Epoch 56/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0972 - accuracy: 0.9902 - val_loss: 0.1730 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.14551\n",
            "Epoch 57/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1174 - accuracy: 0.9586 - val_loss: 0.1625 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.14551\n",
            "Epoch 58/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1204 - accuracy: 0.9775 - val_loss: 0.1602 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.14551\n",
            "Epoch 59/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1013 - accuracy: 0.9899 - val_loss: 0.1951 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.14551\n",
            "Epoch 60/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1247 - accuracy: 0.9804 - val_loss: 0.1635 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.14551\n",
            "Epoch 61/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0928 - accuracy: 0.9872 - val_loss: 0.1352 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00061: val_loss improved from 0.14551 to 0.13524, saving model to ./model/61-0.1352.hdf5\n",
            "Epoch 62/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0898 - accuracy: 0.9983 - val_loss: 0.1494 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.13524\n",
            "Epoch 63/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1029 - accuracy: 0.9747 - val_loss: 0.1534 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.13524\n",
            "Epoch 64/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1068 - accuracy: 0.9874 - val_loss: 0.1139 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00064: val_loss improved from 0.13524 to 0.11390, saving model to ./model/64-0.1139.hdf5\n",
            "Epoch 65/100\n",
            "27/27 [==============================] - 0s 5ms/step - loss: 0.0863 - accuracy: 0.9898 - val_loss: 0.1808 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.11390\n",
            "Epoch 66/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0894 - accuracy: 0.9870 - val_loss: 0.1409 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.11390\n",
            "Epoch 67/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1242 - accuracy: 0.9633 - val_loss: 0.1747 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.11390\n",
            "Epoch 68/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1178 - accuracy: 0.9618 - val_loss: 0.1252 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.11390\n",
            "Epoch 69/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1070 - accuracy: 0.9912 - val_loss: 0.1454 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.11390\n",
            "Epoch 70/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1075 - accuracy: 0.9829 - val_loss: 0.1262 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.11390\n",
            "Epoch 71/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0922 - accuracy: 0.9733 - val_loss: 0.1172 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.11390\n",
            "Epoch 72/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0985 - accuracy: 0.9830 - val_loss: 0.1750 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.11390\n",
            "Epoch 73/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1238 - accuracy: 0.9602 - val_loss: 0.1527 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.11390\n",
            "Epoch 74/100\n",
            "27/27 [==============================] - 0s 5ms/step - loss: 0.0903 - accuracy: 0.9811 - val_loss: 0.1620 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.11390\n",
            "Epoch 75/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0582 - accuracy: 0.9931 - val_loss: 0.1364 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.11390\n",
            "Epoch 76/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0821 - accuracy: 0.9831 - val_loss: 0.1343 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.11390\n",
            "Epoch 77/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0658 - accuracy: 0.9911 - val_loss: 0.2103 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.11390\n",
            "Epoch 78/100\n",
            "27/27 [==============================] - 0s 5ms/step - loss: 0.1217 - accuracy: 0.9676 - val_loss: 0.1859 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.11390\n",
            "Epoch 79/100\n",
            "27/27 [==============================] - 0s 5ms/step - loss: 0.1064 - accuracy: 0.9414 - val_loss: 0.1794 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.11390\n",
            "Epoch 80/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0728 - accuracy: 0.9854 - val_loss: 0.1038 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00080: val_loss improved from 0.11390 to 0.10378, saving model to ./model/80-0.1038.hdf5\n",
            "Epoch 81/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1041 - accuracy: 0.9640 - val_loss: 0.1519 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.10378\n",
            "Epoch 82/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0732 - accuracy: 0.9932 - val_loss: 0.1457 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.10378\n",
            "Epoch 83/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0680 - accuracy: 0.9939 - val_loss: 0.1542 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.10378\n",
            "Epoch 84/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.0946 - accuracy: 0.9451 - val_loss: 0.1703 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.10378\n",
            "Epoch 85/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0773 - accuracy: 0.9814 - val_loss: 0.1380 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.10378\n",
            "Epoch 86/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0621 - accuracy: 0.9818 - val_loss: 0.1630 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.10378\n",
            "Epoch 87/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0463 - accuracy: 0.9915 - val_loss: 0.1605 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.10378\n",
            "Epoch 88/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0892 - accuracy: 0.9826 - val_loss: 0.1321 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.10378\n",
            "Epoch 89/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0750 - accuracy: 0.9824 - val_loss: 0.1212 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.10378\n",
            "Epoch 90/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0814 - accuracy: 0.9766 - val_loss: 0.1446 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.10378\n",
            "Epoch 91/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0683 - accuracy: 0.9807 - val_loss: 0.1673 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.10378\n",
            "Epoch 92/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0775 - accuracy: 0.9628 - val_loss: 0.1722 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.10378\n",
            "Epoch 93/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0822 - accuracy: 0.9664 - val_loss: 0.1717 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.10378\n",
            "Epoch 94/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0730 - accuracy: 0.9932 - val_loss: 0.0972 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00094: val_loss improved from 0.10378 to 0.09725, saving model to ./model/94-0.0972.hdf5\n",
            "Epoch 95/100\n",
            "27/27 [==============================] - 0s 5ms/step - loss: 0.0817 - accuracy: 0.9753 - val_loss: 0.1724 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.09725\n",
            "Epoch 96/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0502 - accuracy: 0.9920 - val_loss: 0.1529 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.09725\n",
            "Epoch 97/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0673 - accuracy: 0.9973 - val_loss: 0.1548 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.09725\n",
            "Epoch 98/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1164 - accuracy: 0.9195 - val_loss: 0.2365 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.09725\n",
            "Epoch 99/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0777 - accuracy: 0.9690 - val_loss: 0.1525 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.09725\n",
            "Epoch 100/100\n",
            "27/27 [==============================] - 0s 5ms/step - loss: 0.0623 - accuracy: 0.9826 - val_loss: 0.1176 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.09725\n",
            "1/1 [==============================] - 0s 259ms/step - loss: 0.1176 - accuracy: 0.9333\n",
            "Epoch 1/100\n",
            "27/27 [==============================] - 1s 14ms/step - loss: 2.2949 - accuracy: 0.3287 - val_loss: 1.7379 - val_accuracy: 0.2667\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 1.73793, saving model to ./model/01-1.7379.hdf5\n",
            "Epoch 2/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 1.5074 - accuracy: 0.3682 - val_loss: 1.2875 - val_accuracy: 0.3333\n",
            "\n",
            "Epoch 00002: val_loss improved from 1.73793 to 1.28749, saving model to ./model/02-1.2875.hdf5\n",
            "Epoch 3/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 1.1814 - accuracy: 0.3542 - val_loss: 1.0970 - val_accuracy: 0.2000\n",
            "\n",
            "Epoch 00003: val_loss improved from 1.28749 to 1.09700, saving model to ./model/03-1.0970.hdf5\n",
            "Epoch 4/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 1.0716 - accuracy: 0.2908 - val_loss: 0.9998 - val_accuracy: 0.6000\n",
            "\n",
            "Epoch 00004: val_loss improved from 1.09700 to 0.99980, saving model to ./model/04-0.9998.hdf5\n",
            "Epoch 5/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.9778 - accuracy: 0.6361 - val_loss: 0.9353 - val_accuracy: 0.5333\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.99980 to 0.93529, saving model to ./model/05-0.9353.hdf5\n",
            "Epoch 6/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.9099 - accuracy: 0.6732 - val_loss: 0.8749 - val_accuracy: 0.6000\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.93529 to 0.87490, saving model to ./model/06-0.8749.hdf5\n",
            "Epoch 7/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.8277 - accuracy: 0.7812 - val_loss: 0.8190 - val_accuracy: 0.7333\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.87490 to 0.81900, saving model to ./model/07-0.8190.hdf5\n",
            "Epoch 8/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.7811 - accuracy: 0.7106 - val_loss: 0.7718 - val_accuracy: 0.6667\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.81900 to 0.77177, saving model to ./model/08-0.7718.hdf5\n",
            "Epoch 9/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.7397 - accuracy: 0.7561 - val_loss: 0.7240 - val_accuracy: 0.7333\n",
            "\n",
            "Epoch 00009: val_loss improved from 0.77177 to 0.72396, saving model to ./model/09-0.7240.hdf5\n",
            "Epoch 10/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.6767 - accuracy: 0.7738 - val_loss: 0.6749 - val_accuracy: 0.8000\n",
            "\n",
            "Epoch 00010: val_loss improved from 0.72396 to 0.67492, saving model to ./model/10-0.6749.hdf5\n",
            "Epoch 11/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.6281 - accuracy: 0.8006 - val_loss: 0.6286 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00011: val_loss improved from 0.67492 to 0.62855, saving model to ./model/11-0.6286.hdf5\n",
            "Epoch 12/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.5997 - accuracy: 0.8667 - val_loss: 0.5900 - val_accuracy: 0.8667\n",
            "\n",
            "Epoch 00012: val_loss improved from 0.62855 to 0.58998, saving model to ./model/12-0.5900.hdf5\n",
            "Epoch 13/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.5421 - accuracy: 0.9196 - val_loss: 0.5568 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00013: val_loss improved from 0.58998 to 0.55680, saving model to ./model/13-0.5568.hdf5\n",
            "Epoch 14/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.5404 - accuracy: 0.9187 - val_loss: 0.5261 - val_accuracy: 0.8667\n",
            "\n",
            "Epoch 00014: val_loss improved from 0.55680 to 0.52607, saving model to ./model/14-0.5261.hdf5\n",
            "Epoch 15/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.5118 - accuracy: 0.8155 - val_loss: 0.4980 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00015: val_loss improved from 0.52607 to 0.49797, saving model to ./model/15-0.4980.hdf5\n",
            "Epoch 16/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.4727 - accuracy: 0.9697 - val_loss: 0.4732 - val_accuracy: 0.8667\n",
            "\n",
            "Epoch 00016: val_loss improved from 0.49797 to 0.47324, saving model to ./model/16-0.4732.hdf5\n",
            "Epoch 17/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.4428 - accuracy: 0.8870 - val_loss: 0.4510 - val_accuracy: 0.8667\n",
            "\n",
            "Epoch 00017: val_loss improved from 0.47324 to 0.45105, saving model to ./model/17-0.4510.hdf5\n",
            "Epoch 18/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.4082 - accuracy: 0.8409 - val_loss: 0.4187 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00018: val_loss improved from 0.45105 to 0.41872, saving model to ./model/18-0.4187.hdf5\n",
            "Epoch 19/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.3949 - accuracy: 0.9452 - val_loss: 0.3868 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00019: val_loss improved from 0.41872 to 0.38681, saving model to ./model/19-0.3868.hdf5\n",
            "Epoch 20/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.3719 - accuracy: 0.9168 - val_loss: 0.3582 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00020: val_loss improved from 0.38681 to 0.35820, saving model to ./model/20-0.3582.hdf5\n",
            "Epoch 21/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.3426 - accuracy: 0.9521 - val_loss: 0.3312 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00021: val_loss improved from 0.35820 to 0.33121, saving model to ./model/21-0.3312.hdf5\n",
            "Epoch 22/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.3027 - accuracy: 0.9643 - val_loss: 0.3107 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00022: val_loss improved from 0.33121 to 0.31067, saving model to ./model/22-0.3107.hdf5\n",
            "Epoch 23/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.2993 - accuracy: 0.9554 - val_loss: 0.2917 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00023: val_loss improved from 0.31067 to 0.29171, saving model to ./model/23-0.2917.hdf5\n",
            "Epoch 24/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.3017 - accuracy: 0.9519 - val_loss: 0.2805 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00024: val_loss improved from 0.29171 to 0.28047, saving model to ./model/24-0.2805.hdf5\n",
            "Epoch 25/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.2900 - accuracy: 0.9481 - val_loss: 0.2568 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00025: val_loss improved from 0.28047 to 0.25678, saving model to ./model/25-0.2568.hdf5\n",
            "Epoch 26/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.2594 - accuracy: 0.9655 - val_loss: 0.2403 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00026: val_loss improved from 0.25678 to 0.24028, saving model to ./model/26-0.2403.hdf5\n",
            "Epoch 27/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.2405 - accuracy: 0.9722 - val_loss: 0.2337 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00027: val_loss improved from 0.24028 to 0.23367, saving model to ./model/27-0.2337.hdf5\n",
            "Epoch 28/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.2336 - accuracy: 0.9907 - val_loss: 0.2179 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00028: val_loss improved from 0.23367 to 0.21793, saving model to ./model/28-0.2179.hdf5\n",
            "Epoch 29/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.2281 - accuracy: 0.9648 - val_loss: 0.2056 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00029: val_loss improved from 0.21793 to 0.20560, saving model to ./model/29-0.2056.hdf5\n",
            "Epoch 30/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.2007 - accuracy: 0.9543 - val_loss: 0.1994 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00030: val_loss improved from 0.20560 to 0.19936, saving model to ./model/30-0.1994.hdf5\n",
            "Epoch 31/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.2000 - accuracy: 0.9668 - val_loss: 0.1822 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00031: val_loss improved from 0.19936 to 0.18222, saving model to ./model/31-0.1822.hdf5\n",
            "Epoch 32/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1820 - accuracy: 0.9661 - val_loss: 0.1742 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00032: val_loss improved from 0.18222 to 0.17424, saving model to ./model/32-0.1742.hdf5\n",
            "Epoch 33/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1766 - accuracy: 0.9729 - val_loss: 0.1724 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00033: val_loss improved from 0.17424 to 0.17245, saving model to ./model/33-0.1724.hdf5\n",
            "Epoch 34/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1779 - accuracy: 0.9756 - val_loss: 0.1554 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00034: val_loss improved from 0.17245 to 0.15540, saving model to ./model/34-0.1554.hdf5\n",
            "Epoch 35/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1634 - accuracy: 0.9513 - val_loss: 0.1486 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00035: val_loss improved from 0.15540 to 0.14860, saving model to ./model/35-0.1486.hdf5\n",
            "Epoch 36/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1602 - accuracy: 0.9729 - val_loss: 0.1483 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00036: val_loss improved from 0.14860 to 0.14827, saving model to ./model/36-0.1483.hdf5\n",
            "Epoch 37/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1509 - accuracy: 0.9797 - val_loss: 0.1432 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00037: val_loss improved from 0.14827 to 0.14319, saving model to ./model/37-0.1432.hdf5\n",
            "Epoch 38/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1676 - accuracy: 0.9745 - val_loss: 0.1363 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00038: val_loss improved from 0.14319 to 0.13631, saving model to ./model/38-0.1363.hdf5\n",
            "Epoch 39/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1484 - accuracy: 0.9718 - val_loss: 0.1324 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00039: val_loss improved from 0.13631 to 0.13235, saving model to ./model/39-0.1324.hdf5\n",
            "Epoch 40/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1347 - accuracy: 0.9727 - val_loss: 0.1241 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00040: val_loss improved from 0.13235 to 0.12414, saving model to ./model/40-0.1241.hdf5\n",
            "Epoch 41/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1279 - accuracy: 0.9885 - val_loss: 0.1301 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.12414\n",
            "Epoch 42/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1288 - accuracy: 0.9886 - val_loss: 0.1108 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00042: val_loss improved from 0.12414 to 0.11082, saving model to ./model/42-0.1108.hdf5\n",
            "Epoch 43/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1247 - accuracy: 0.9695 - val_loss: 0.1117 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.11082\n",
            "Epoch 44/100\n",
            "27/27 [==============================] - 0s 5ms/step - loss: 0.1257 - accuracy: 0.9813 - val_loss: 0.1707 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.11082\n",
            "Epoch 45/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1165 - accuracy: 0.9486 - val_loss: 0.1019 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00045: val_loss improved from 0.11082 to 0.10188, saving model to ./model/45-0.1019.hdf5\n",
            "Epoch 46/100\n",
            "27/27 [==============================] - 0s 5ms/step - loss: 0.1252 - accuracy: 0.9777 - val_loss: 0.1359 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.10188\n",
            "Epoch 47/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1353 - accuracy: 0.9747 - val_loss: 0.0948 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00047: val_loss improved from 0.10188 to 0.09479, saving model to ./model/47-0.0948.hdf5\n",
            "Epoch 48/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0713 - accuracy: 0.9925 - val_loss: 0.1459 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.09479\n",
            "Epoch 49/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1236 - accuracy: 0.9539 - val_loss: 0.0979 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.09479\n",
            "Epoch 50/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1278 - accuracy: 0.9566 - val_loss: 0.1108 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.09479\n",
            "Epoch 51/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1279 - accuracy: 0.9708 - val_loss: 0.0983 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.09479\n",
            "Epoch 52/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0899 - accuracy: 0.9726 - val_loss: 0.0957 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.09479\n",
            "Epoch 53/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0950 - accuracy: 0.9729 - val_loss: 0.0811 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00053: val_loss improved from 0.09479 to 0.08113, saving model to ./model/53-0.0811.hdf5\n",
            "Epoch 54/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1320 - accuracy: 0.9600 - val_loss: 0.1364 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.08113\n",
            "Epoch 55/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1271 - accuracy: 0.9501 - val_loss: 0.0735 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00055: val_loss improved from 0.08113 to 0.07350, saving model to ./model/55-0.0735.hdf5\n",
            "Epoch 56/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1061 - accuracy: 0.9610 - val_loss: 0.0763 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.07350\n",
            "Epoch 57/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0934 - accuracy: 0.9596 - val_loss: 0.0855 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.07350\n",
            "Epoch 58/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0960 - accuracy: 0.9661 - val_loss: 0.0915 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.07350\n",
            "Epoch 59/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0912 - accuracy: 0.9881 - val_loss: 0.1157 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.07350\n",
            "Epoch 60/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1006 - accuracy: 0.9576 - val_loss: 0.0888 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.07350\n",
            "Epoch 61/100\n",
            "27/27 [==============================] - 0s 5ms/step - loss: 0.0863 - accuracy: 0.9881 - val_loss: 0.0785 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.07350\n",
            "Epoch 62/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1027 - accuracy: 0.9495 - val_loss: 0.1074 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.07350\n",
            "Epoch 63/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0878 - accuracy: 0.9894 - val_loss: 0.0753 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.07350\n",
            "Epoch 64/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0787 - accuracy: 0.9720 - val_loss: 0.1014 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.07350\n",
            "Epoch 65/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1112 - accuracy: 0.9598 - val_loss: 0.0931 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.07350\n",
            "Epoch 66/100\n",
            "27/27 [==============================] - 0s 5ms/step - loss: 0.0857 - accuracy: 0.9744 - val_loss: 0.1065 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.07350\n",
            "Epoch 67/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0663 - accuracy: 0.9926 - val_loss: 0.0815 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.07350\n",
            "Epoch 68/100\n",
            "27/27 [==============================] - 0s 6ms/step - loss: 0.0768 - accuracy: 0.9818 - val_loss: 0.0954 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.07350\n",
            "Epoch 69/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0629 - accuracy: 0.9882 - val_loss: 0.1155 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.07350\n",
            "Epoch 70/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1311 - accuracy: 0.9391 - val_loss: 0.0570 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00070: val_loss improved from 0.07350 to 0.05702, saving model to ./model/70-0.0570.hdf5\n",
            "Epoch 71/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0851 - accuracy: 0.9593 - val_loss: 0.0924 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.05702\n",
            "Epoch 72/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0869 - accuracy: 0.9711 - val_loss: 0.0697 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.05702\n",
            "Epoch 73/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1197 - accuracy: 0.9567 - val_loss: 0.1023 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.05702\n",
            "Epoch 74/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0910 - accuracy: 0.9792 - val_loss: 0.0825 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.05702\n",
            "Epoch 75/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0720 - accuracy: 0.9829 - val_loss: 0.0741 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.05702\n",
            "Epoch 76/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1208 - accuracy: 0.9412 - val_loss: 0.0620 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.05702\n",
            "Epoch 77/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0911 - accuracy: 0.9636 - val_loss: 0.0503 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00077: val_loss improved from 0.05702 to 0.05026, saving model to ./model/77-0.0503.hdf5\n",
            "Epoch 78/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0859 - accuracy: 0.9634 - val_loss: 0.1048 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.05026\n",
            "Epoch 79/100\n",
            "27/27 [==============================] - 0s 5ms/step - loss: 0.0731 - accuracy: 0.9767 - val_loss: 0.0826 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.05026\n",
            "Epoch 80/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0642 - accuracy: 0.9690 - val_loss: 0.0960 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.05026\n",
            "Epoch 81/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0923 - accuracy: 0.9724 - val_loss: 0.0892 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.05026\n",
            "Epoch 82/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0711 - accuracy: 0.9751 - val_loss: 0.1047 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.05026\n",
            "Epoch 83/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0528 - accuracy: 0.9870 - val_loss: 0.0820 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.05026\n",
            "Epoch 84/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0642 - accuracy: 0.9772 - val_loss: 0.1140 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.05026\n",
            "Epoch 85/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0902 - accuracy: 0.9562 - val_loss: 0.0872 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.05026\n",
            "Epoch 86/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1077 - accuracy: 0.9779 - val_loss: 0.0467 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00086: val_loss improved from 0.05026 to 0.04668, saving model to ./model/86-0.0467.hdf5\n",
            "Epoch 87/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0463 - accuracy: 0.9832 - val_loss: 0.1369 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.04668\n",
            "Epoch 88/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0903 - accuracy: 0.9429 - val_loss: 0.0659 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.04668\n",
            "Epoch 89/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0786 - accuracy: 0.9830 - val_loss: 0.0693 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.04668\n",
            "Epoch 90/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0785 - accuracy: 0.9766 - val_loss: 0.0553 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.04668\n",
            "Epoch 91/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0736 - accuracy: 0.9771 - val_loss: 0.0769 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.04668\n",
            "Epoch 92/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0772 - accuracy: 0.9806 - val_loss: 0.0854 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.04668\n",
            "Epoch 93/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0845 - accuracy: 0.9698 - val_loss: 0.0657 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.04668\n",
            "Epoch 94/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1009 - accuracy: 0.9615 - val_loss: 0.0619 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.04668\n",
            "Epoch 95/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0562 - accuracy: 0.9742 - val_loss: 0.0782 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.04668\n",
            "Epoch 96/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0678 - accuracy: 0.9815 - val_loss: 0.0480 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.04668\n",
            "Epoch 97/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0653 - accuracy: 0.9725 - val_loss: 0.1367 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.04668\n",
            "Epoch 98/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0854 - accuracy: 0.9545 - val_loss: 0.0790 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.04668\n",
            "Epoch 99/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0504 - accuracy: 0.9893 - val_loss: 0.0799 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.04668\n",
            "Epoch 100/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0746 - accuracy: 0.9705 - val_loss: 0.0529 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.04668\n",
            "1/1 [==============================] - 1s 750ms/step - loss: 0.0529 - accuracy: 1.0000\n",
            "Epoch 1/100\n",
            "27/27 [==============================] - 1s 17ms/step - loss: 1.5296 - accuracy: 0.3120 - val_loss: 1.2067 - val_accuracy: 0.4667\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 1.20671, saving model to ./model/01-1.2067.hdf5\n",
            "Epoch 2/100\n",
            "27/27 [==============================] - 0s 5ms/step - loss: 1.1496 - accuracy: 0.6005 - val_loss: 0.9149 - val_accuracy: 0.6667\n",
            "\n",
            "Epoch 00002: val_loss improved from 1.20671 to 0.91485, saving model to ./model/02-0.9149.hdf5\n",
            "Epoch 3/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.8608 - accuracy: 0.7306 - val_loss: 0.7820 - val_accuracy: 0.8667\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.91485 to 0.78195, saving model to ./model/03-0.7820.hdf5\n",
            "Epoch 4/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.7427 - accuracy: 0.8549 - val_loss: 0.6996 - val_accuracy: 0.7333\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.78195 to 0.69959, saving model to ./model/04-0.6996.hdf5\n",
            "Epoch 5/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.6487 - accuracy: 0.8796 - val_loss: 0.6385 - val_accuracy: 0.8000\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.69959 to 0.63846, saving model to ./model/05-0.6385.hdf5\n",
            "Epoch 6/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.6369 - accuracy: 0.9541 - val_loss: 0.5901 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.63846 to 0.59013, saving model to ./model/06-0.5901.hdf5\n",
            "Epoch 7/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.5652 - accuracy: 0.9630 - val_loss: 0.5461 - val_accuracy: 0.8000\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.59013 to 0.54610, saving model to ./model/07-0.5461.hdf5\n",
            "Epoch 8/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.5294 - accuracy: 0.8442 - val_loss: 0.5115 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.54610 to 0.51149, saving model to ./model/08-0.5115.hdf5\n",
            "Epoch 9/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.4944 - accuracy: 0.9730 - val_loss: 0.4754 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00009: val_loss improved from 0.51149 to 0.47535, saving model to ./model/09-0.4754.hdf5\n",
            "Epoch 10/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.4404 - accuracy: 0.9442 - val_loss: 0.4488 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00010: val_loss improved from 0.47535 to 0.44885, saving model to ./model/10-0.4488.hdf5\n",
            "Epoch 11/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.4213 - accuracy: 0.9796 - val_loss: 0.4220 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00011: val_loss improved from 0.44885 to 0.42196, saving model to ./model/11-0.4220.hdf5\n",
            "Epoch 12/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.3694 - accuracy: 0.9503 - val_loss: 0.4023 - val_accuracy: 0.8667\n",
            "\n",
            "Epoch 00012: val_loss improved from 0.42196 to 0.40228, saving model to ./model/12-0.4023.hdf5\n",
            "Epoch 13/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.3658 - accuracy: 0.9747 - val_loss: 0.3827 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00013: val_loss improved from 0.40228 to 0.38271, saving model to ./model/13-0.3827.hdf5\n",
            "Epoch 14/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.3289 - accuracy: 0.9702 - val_loss: 0.3624 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00014: val_loss improved from 0.38271 to 0.36243, saving model to ./model/14-0.3624.hdf5\n",
            "Epoch 15/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.3344 - accuracy: 0.9837 - val_loss: 0.3455 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00015: val_loss improved from 0.36243 to 0.34551, saving model to ./model/15-0.3455.hdf5\n",
            "Epoch 16/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.2819 - accuracy: 0.9862 - val_loss: 0.3364 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00016: val_loss improved from 0.34551 to 0.33635, saving model to ./model/16-0.3364.hdf5\n",
            "Epoch 17/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.2717 - accuracy: 0.9637 - val_loss: 0.3243 - val_accuracy: 0.8667\n",
            "\n",
            "Epoch 00017: val_loss improved from 0.33635 to 0.32433, saving model to ./model/17-0.3243.hdf5\n",
            "Epoch 18/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.2579 - accuracy: 0.9030 - val_loss: 0.3081 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00018: val_loss improved from 0.32433 to 0.30811, saving model to ./model/18-0.3081.hdf5\n",
            "Epoch 19/100\n",
            "27/27 [==============================] - 0s 5ms/step - loss: 0.2610 - accuracy: 0.9620 - val_loss: 0.2831 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00019: val_loss improved from 0.30811 to 0.28307, saving model to ./model/19-0.2831.hdf5\n",
            "Epoch 20/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.2337 - accuracy: 0.9763 - val_loss: 0.2696 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00020: val_loss improved from 0.28307 to 0.26960, saving model to ./model/20-0.2696.hdf5\n",
            "Epoch 21/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.2444 - accuracy: 0.9439 - val_loss: 0.2583 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00021: val_loss improved from 0.26960 to 0.25828, saving model to ./model/21-0.2583.hdf5\n",
            "Epoch 22/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1941 - accuracy: 0.9799 - val_loss: 0.2464 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00022: val_loss improved from 0.25828 to 0.24639, saving model to ./model/22-0.2464.hdf5\n",
            "Epoch 23/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1672 - accuracy: 0.9779 - val_loss: 0.2360 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00023: val_loss improved from 0.24639 to 0.23604, saving model to ./model/23-0.2360.hdf5\n",
            "Epoch 24/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1704 - accuracy: 0.9586 - val_loss: 0.2302 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00024: val_loss improved from 0.23604 to 0.23024, saving model to ./model/24-0.2302.hdf5\n",
            "Epoch 25/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1924 - accuracy: 0.9791 - val_loss: 0.2161 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00025: val_loss improved from 0.23024 to 0.21610, saving model to ./model/25-0.2161.hdf5\n",
            "Epoch 26/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1742 - accuracy: 0.9710 - val_loss: 0.2008 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00026: val_loss improved from 0.21610 to 0.20079, saving model to ./model/26-0.2008.hdf5\n",
            "Epoch 27/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1595 - accuracy: 0.9519 - val_loss: 0.1921 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00027: val_loss improved from 0.20079 to 0.19213, saving model to ./model/27-0.1921.hdf5\n",
            "Epoch 28/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1622 - accuracy: 0.9741 - val_loss: 0.1866 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00028: val_loss improved from 0.19213 to 0.18656, saving model to ./model/28-0.1866.hdf5\n",
            "Epoch 29/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1255 - accuracy: 0.9813 - val_loss: 0.1793 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00029: val_loss improved from 0.18656 to 0.17930, saving model to ./model/29-0.1793.hdf5\n",
            "Epoch 30/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1286 - accuracy: 0.9715 - val_loss: 0.1689 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00030: val_loss improved from 0.17930 to 0.16894, saving model to ./model/30-0.1689.hdf5\n",
            "Epoch 31/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1099 - accuracy: 0.9798 - val_loss: 0.1681 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00031: val_loss improved from 0.16894 to 0.16808, saving model to ./model/31-0.1681.hdf5\n",
            "Epoch 32/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1508 - accuracy: 0.9713 - val_loss: 0.1748 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.16808\n",
            "Epoch 33/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1302 - accuracy: 0.9696 - val_loss: 0.1610 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00033: val_loss improved from 0.16808 to 0.16100, saving model to ./model/33-0.1610.hdf5\n",
            "Epoch 34/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1621 - accuracy: 0.9354 - val_loss: 0.1552 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00034: val_loss improved from 0.16100 to 0.15522, saving model to ./model/34-0.1552.hdf5\n",
            "Epoch 35/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1298 - accuracy: 0.9622 - val_loss: 0.1719 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.15522\n",
            "Epoch 36/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1160 - accuracy: 0.9550 - val_loss: 0.1412 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00036: val_loss improved from 0.15522 to 0.14123, saving model to ./model/36-0.1412.hdf5\n",
            "Epoch 37/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0965 - accuracy: 0.9722 - val_loss: 0.1335 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00037: val_loss improved from 0.14123 to 0.13355, saving model to ./model/37-0.1335.hdf5\n",
            "Epoch 38/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0937 - accuracy: 0.9879 - val_loss: 0.1772 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.13355\n",
            "Epoch 39/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1159 - accuracy: 0.9569 - val_loss: 0.1267 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00039: val_loss improved from 0.13355 to 0.12672, saving model to ./model/39-0.1267.hdf5\n",
            "Epoch 40/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1505 - accuracy: 0.9352 - val_loss: 0.1528 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.12672\n",
            "Epoch 41/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1534 - accuracy: 0.9645 - val_loss: 0.1251 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00041: val_loss improved from 0.12672 to 0.12513, saving model to ./model/41-0.1251.hdf5\n",
            "Epoch 42/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0975 - accuracy: 0.9605 - val_loss: 0.1326 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.12513\n",
            "Epoch 43/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1271 - accuracy: 0.9374 - val_loss: 0.1162 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00043: val_loss improved from 0.12513 to 0.11619, saving model to ./model/43-0.1162.hdf5\n",
            "Epoch 44/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1013 - accuracy: 0.9542 - val_loss: 0.1595 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.11619\n",
            "Epoch 45/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0883 - accuracy: 0.9670 - val_loss: 0.1120 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00045: val_loss improved from 0.11619 to 0.11204, saving model to ./model/45-0.1120.hdf5\n",
            "Epoch 46/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0550 - accuracy: 0.9902 - val_loss: 0.1135 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.11204\n",
            "Epoch 47/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0848 - accuracy: 0.9774 - val_loss: 0.1115 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00047: val_loss improved from 0.11204 to 0.11148, saving model to ./model/47-0.1115.hdf5\n",
            "Epoch 48/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.0930 - accuracy: 0.9639 - val_loss: 0.1105 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00048: val_loss improved from 0.11148 to 0.11052, saving model to ./model/48-0.1105.hdf5\n",
            "Epoch 49/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1229 - accuracy: 0.9672 - val_loss: 0.1039 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00049: val_loss improved from 0.11052 to 0.10392, saving model to ./model/49-0.1039.hdf5\n",
            "Epoch 50/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0800 - accuracy: 0.9675 - val_loss: 0.1071 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.10392\n",
            "Epoch 51/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0791 - accuracy: 0.9686 - val_loss: 0.1151 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.10392\n",
            "Epoch 52/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1005 - accuracy: 0.9693 - val_loss: 0.1057 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.10392\n",
            "Epoch 53/100\n",
            "27/27 [==============================] - 0s 5ms/step - loss: 0.0813 - accuracy: 0.9710 - val_loss: 0.0987 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00053: val_loss improved from 0.10392 to 0.09872, saving model to ./model/53-0.0987.hdf5\n",
            "Epoch 54/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0761 - accuracy: 0.9679 - val_loss: 0.0964 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00054: val_loss improved from 0.09872 to 0.09638, saving model to ./model/54-0.0964.hdf5\n",
            "Epoch 55/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1024 - accuracy: 0.9651 - val_loss: 0.0977 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.09638\n",
            "Epoch 56/100\n",
            "27/27 [==============================] - 0s 5ms/step - loss: 0.1010 - accuracy: 0.9574 - val_loss: 0.1213 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.09638\n",
            "Epoch 57/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1051 - accuracy: 0.9548 - val_loss: 0.0959 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00057: val_loss improved from 0.09638 to 0.09594, saving model to ./model/57-0.0959.hdf5\n",
            "Epoch 58/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0958 - accuracy: 0.9603 - val_loss: 0.0922 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00058: val_loss improved from 0.09594 to 0.09216, saving model to ./model/58-0.0922.hdf5\n",
            "Epoch 59/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1194 - accuracy: 0.9422 - val_loss: 0.0932 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.09216\n",
            "Epoch 60/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0798 - accuracy: 0.9727 - val_loss: 0.0898 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00060: val_loss improved from 0.09216 to 0.08979, saving model to ./model/60-0.0898.hdf5\n",
            "Epoch 61/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0892 - accuracy: 0.9835 - val_loss: 0.0885 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00061: val_loss improved from 0.08979 to 0.08855, saving model to ./model/61-0.0885.hdf5\n",
            "Epoch 62/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1101 - accuracy: 0.9414 - val_loss: 0.0876 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00062: val_loss improved from 0.08855 to 0.08764, saving model to ./model/62-0.0876.hdf5\n",
            "Epoch 63/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0513 - accuracy: 0.9870 - val_loss: 0.1044 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.08764\n",
            "Epoch 64/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0918 - accuracy: 0.9529 - val_loss: 0.0947 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.08764\n",
            "Epoch 65/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0771 - accuracy: 0.9751 - val_loss: 0.0857 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00065: val_loss improved from 0.08764 to 0.08566, saving model to ./model/65-0.0857.hdf5\n",
            "Epoch 66/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0540 - accuracy: 0.9901 - val_loss: 0.1108 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.08566\n",
            "Epoch 67/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0822 - accuracy: 0.9782 - val_loss: 0.0823 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00067: val_loss improved from 0.08566 to 0.08233, saving model to ./model/67-0.0823.hdf5\n",
            "Epoch 68/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0644 - accuracy: 0.9799 - val_loss: 0.0830 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.08233\n",
            "Epoch 69/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1131 - accuracy: 0.9580 - val_loss: 0.0909 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.08233\n",
            "Epoch 70/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0846 - accuracy: 0.9654 - val_loss: 0.0801 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00070: val_loss improved from 0.08233 to 0.08007, saving model to ./model/70-0.0801.hdf5\n",
            "Epoch 71/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0577 - accuracy: 0.9833 - val_loss: 0.0944 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.08007\n",
            "Epoch 72/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0872 - accuracy: 0.9727 - val_loss: 0.0800 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00072: val_loss improved from 0.08007 to 0.08000, saving model to ./model/72-0.0800.hdf5\n",
            "Epoch 73/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0830 - accuracy: 0.9804 - val_loss: 0.0948 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.08000\n",
            "Epoch 74/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0661 - accuracy: 0.9799 - val_loss: 0.0885 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.08000\n",
            "Epoch 75/100\n",
            "27/27 [==============================] - 0s 5ms/step - loss: 0.0761 - accuracy: 0.9752 - val_loss: 0.0836 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.08000\n",
            "Epoch 76/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0673 - accuracy: 0.9832 - val_loss: 0.0761 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00076: val_loss improved from 0.08000 to 0.07606, saving model to ./model/76-0.0761.hdf5\n",
            "Epoch 77/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1241 - accuracy: 0.9411 - val_loss: 0.0762 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.07606\n",
            "Epoch 78/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0841 - accuracy: 0.9765 - val_loss: 0.0801 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.07606\n",
            "Epoch 79/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0921 - accuracy: 0.9500 - val_loss: 0.0738 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00079: val_loss improved from 0.07606 to 0.07381, saving model to ./model/79-0.0738.hdf5\n",
            "Epoch 80/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.0489 - accuracy: 0.9785 - val_loss: 0.0831 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.07381\n",
            "Epoch 81/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0583 - accuracy: 0.9845 - val_loss: 0.0722 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00081: val_loss improved from 0.07381 to 0.07216, saving model to ./model/81-0.0722.hdf5\n",
            "Epoch 82/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1139 - accuracy: 0.9685 - val_loss: 0.0926 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.07216\n",
            "Epoch 83/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0654 - accuracy: 0.9728 - val_loss: 0.0771 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.07216\n",
            "Epoch 84/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0934 - accuracy: 0.9547 - val_loss: 0.0748 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.07216\n",
            "Epoch 85/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0718 - accuracy: 0.9708 - val_loss: 0.0890 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.07216\n",
            "Epoch 86/100\n",
            "27/27 [==============================] - 0s 5ms/step - loss: 0.0644 - accuracy: 0.9774 - val_loss: 0.0698 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00086: val_loss improved from 0.07216 to 0.06984, saving model to ./model/86-0.0698.hdf5\n",
            "Epoch 87/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0827 - accuracy: 0.9665 - val_loss: 0.0714 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.06984\n",
            "Epoch 88/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0976 - accuracy: 0.9496 - val_loss: 0.0831 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.06984\n",
            "Epoch 89/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0888 - accuracy: 0.9501 - val_loss: 0.1089 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.06984\n",
            "Epoch 90/100\n",
            "27/27 [==============================] - 0s 6ms/step - loss: 0.1162 - accuracy: 0.9676 - val_loss: 0.0739 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.06984\n",
            "Epoch 91/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0561 - accuracy: 0.9825 - val_loss: 0.0705 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.06984\n",
            "Epoch 92/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0519 - accuracy: 0.9953 - val_loss: 0.0660 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00092: val_loss improved from 0.06984 to 0.06603, saving model to ./model/92-0.0660.hdf5\n",
            "Epoch 93/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0786 - accuracy: 0.9648 - val_loss: 0.0763 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.06603\n",
            "Epoch 94/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0612 - accuracy: 0.9756 - val_loss: 0.0710 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.06603\n",
            "Epoch 95/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0673 - accuracy: 0.9738 - val_loss: 0.0992 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.06603\n",
            "Epoch 96/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0738 - accuracy: 0.9705 - val_loss: 0.0657 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00096: val_loss improved from 0.06603 to 0.06571, saving model to ./model/96-0.0657.hdf5\n",
            "Epoch 97/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0506 - accuracy: 0.9866 - val_loss: 0.0883 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.06571\n",
            "Epoch 98/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0424 - accuracy: 0.9884 - val_loss: 0.0640 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00098: val_loss improved from 0.06571 to 0.06399, saving model to ./model/98-0.0640.hdf5\n",
            "Epoch 99/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1248 - accuracy: 0.9371 - val_loss: 0.0664 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.06399\n",
            "Epoch 100/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0523 - accuracy: 0.9755 - val_loss: 0.0622 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00100: val_loss improved from 0.06399 to 0.06225, saving model to ./model/100-0.0622.hdf5\n",
            "1/1 [==============================] - 0s 293ms/step - loss: 0.0622 - accuracy: 1.0000\n",
            "Epoch 1/100\n",
            "27/27 [==============================] - 1s 17ms/step - loss: 0.8881 - accuracy: 0.5630 - val_loss: 0.9244 - val_accuracy: 0.6667\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.92440, saving model to ./model/01-0.9244.hdf5\n",
            "Epoch 2/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.8394 - accuracy: 0.7220 - val_loss: 0.8397 - val_accuracy: 0.6667\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.92440 to 0.83967, saving model to ./model/02-0.8397.hdf5\n",
            "Epoch 3/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.8052 - accuracy: 0.6951 - val_loss: 0.7646 - val_accuracy: 0.6667\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.83967 to 0.76456, saving model to ./model/03-0.7646.hdf5\n",
            "Epoch 4/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.7427 - accuracy: 0.6548 - val_loss: 0.7066 - val_accuracy: 0.6667\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.76456 to 0.70656, saving model to ./model/04-0.7066.hdf5\n",
            "Epoch 5/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.7047 - accuracy: 0.6614 - val_loss: 0.6553 - val_accuracy: 0.6667\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.70656 to 0.65532, saving model to ./model/05-0.6553.hdf5\n",
            "Epoch 6/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.6542 - accuracy: 0.6769 - val_loss: 0.6078 - val_accuracy: 0.6667\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.65532 to 0.60779, saving model to ./model/06-0.6078.hdf5\n",
            "Epoch 7/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.5892 - accuracy: 0.7121 - val_loss: 0.5585 - val_accuracy: 0.7333\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.60779 to 0.55850, saving model to ./model/07-0.5585.hdf5\n",
            "Epoch 8/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.5856 - accuracy: 0.7651 - val_loss: 0.5172 - val_accuracy: 0.7333\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.55850 to 0.51722, saving model to ./model/08-0.5172.hdf5\n",
            "Epoch 9/100\n",
            "27/27 [==============================] - 0s 5ms/step - loss: 0.5936 - accuracy: 0.7786 - val_loss: 0.4785 - val_accuracy: 0.7333\n",
            "\n",
            "Epoch 00009: val_loss improved from 0.51722 to 0.47853, saving model to ./model/09-0.4785.hdf5\n",
            "Epoch 10/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.5125 - accuracy: 0.8712 - val_loss: 0.4432 - val_accuracy: 0.8000\n",
            "\n",
            "Epoch 00010: val_loss improved from 0.47853 to 0.44319, saving model to ./model/10-0.4432.hdf5\n",
            "Epoch 11/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.4415 - accuracy: 0.9031 - val_loss: 0.4129 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00011: val_loss improved from 0.44319 to 0.41294, saving model to ./model/11-0.4129.hdf5\n",
            "Epoch 12/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.3805 - accuracy: 0.9363 - val_loss: 0.3867 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00012: val_loss improved from 0.41294 to 0.38673, saving model to ./model/12-0.3867.hdf5\n",
            "Epoch 13/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.4162 - accuracy: 0.9762 - val_loss: 0.3667 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00013: val_loss improved from 0.38673 to 0.36673, saving model to ./model/13-0.3667.hdf5\n",
            "Epoch 14/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.4001 - accuracy: 0.9324 - val_loss: 0.3422 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00014: val_loss improved from 0.36673 to 0.34225, saving model to ./model/14-0.3422.hdf5\n",
            "Epoch 15/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.3940 - accuracy: 0.9744 - val_loss: 0.3263 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00015: val_loss improved from 0.34225 to 0.32632, saving model to ./model/15-0.3263.hdf5\n",
            "Epoch 16/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.3559 - accuracy: 0.9431 - val_loss: 0.3073 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00016: val_loss improved from 0.32632 to 0.30732, saving model to ./model/16-0.3073.hdf5\n",
            "Epoch 17/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.2948 - accuracy: 0.9733 - val_loss: 0.2923 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00017: val_loss improved from 0.30732 to 0.29231, saving model to ./model/17-0.2923.hdf5\n",
            "Epoch 18/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.3142 - accuracy: 0.9242 - val_loss: 0.2702 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00018: val_loss improved from 0.29231 to 0.27019, saving model to ./model/18-0.2702.hdf5\n",
            "Epoch 19/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.3023 - accuracy: 0.9743 - val_loss: 0.2637 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00019: val_loss improved from 0.27019 to 0.26375, saving model to ./model/19-0.2637.hdf5\n",
            "Epoch 20/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.2682 - accuracy: 0.9784 - val_loss: 0.2350 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00020: val_loss improved from 0.26375 to 0.23501, saving model to ./model/20-0.2350.hdf5\n",
            "Epoch 21/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.2820 - accuracy: 0.9331 - val_loss: 0.2200 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00021: val_loss improved from 0.23501 to 0.22001, saving model to ./model/21-0.2200.hdf5\n",
            "Epoch 22/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.2512 - accuracy: 0.9843 - val_loss: 0.2151 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00022: val_loss improved from 0.22001 to 0.21510, saving model to ./model/22-0.2151.hdf5\n",
            "Epoch 23/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.2398 - accuracy: 0.9690 - val_loss: 0.1840 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00023: val_loss improved from 0.21510 to 0.18399, saving model to ./model/23-0.1840.hdf5\n",
            "Epoch 24/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.2146 - accuracy: 0.9809 - val_loss: 0.1721 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00024: val_loss improved from 0.18399 to 0.17212, saving model to ./model/24-0.1721.hdf5\n",
            "Epoch 25/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.2061 - accuracy: 0.9786 - val_loss: 0.1525 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00025: val_loss improved from 0.17212 to 0.15249, saving model to ./model/25-0.1525.hdf5\n",
            "Epoch 26/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.2086 - accuracy: 0.9764 - val_loss: 0.1511 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00026: val_loss improved from 0.15249 to 0.15109, saving model to ./model/26-0.1511.hdf5\n",
            "Epoch 27/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1766 - accuracy: 0.9863 - val_loss: 0.1416 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00027: val_loss improved from 0.15109 to 0.14157, saving model to ./model/27-0.1416.hdf5\n",
            "Epoch 28/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1769 - accuracy: 0.9749 - val_loss: 0.1494 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.14157\n",
            "Epoch 29/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1586 - accuracy: 0.9702 - val_loss: 0.1149 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00029: val_loss improved from 0.14157 to 0.11494, saving model to ./model/29-0.1149.hdf5\n",
            "Epoch 30/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1645 - accuracy: 0.9496 - val_loss: 0.1085 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00030: val_loss improved from 0.11494 to 0.10854, saving model to ./model/30-0.1085.hdf5\n",
            "Epoch 31/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1520 - accuracy: 0.9663 - val_loss: 0.1439 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.10854\n",
            "Epoch 32/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1587 - accuracy: 0.9804 - val_loss: 0.0943 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00032: val_loss improved from 0.10854 to 0.09434, saving model to ./model/32-0.0943.hdf5\n",
            "Epoch 33/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1415 - accuracy: 0.9503 - val_loss: 0.1160 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.09434\n",
            "Epoch 34/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1499 - accuracy: 0.9732 - val_loss: 0.0895 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00034: val_loss improved from 0.09434 to 0.08949, saving model to ./model/34-0.0895.hdf5\n",
            "Epoch 35/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1880 - accuracy: 0.9485 - val_loss: 0.1036 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.08949\n",
            "Epoch 36/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1478 - accuracy: 0.9654 - val_loss: 0.0874 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00036: val_loss improved from 0.08949 to 0.08736, saving model to ./model/36-0.0874.hdf5\n",
            "Epoch 37/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1505 - accuracy: 0.9552 - val_loss: 0.0954 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.08736\n",
            "Epoch 38/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1319 - accuracy: 0.9764 - val_loss: 0.0689 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00038: val_loss improved from 0.08736 to 0.06885, saving model to ./model/38-0.0689.hdf5\n",
            "Epoch 39/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1335 - accuracy: 0.9565 - val_loss: 0.0764 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.06885\n",
            "Epoch 40/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1243 - accuracy: 0.9755 - val_loss: 0.0626 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00040: val_loss improved from 0.06885 to 0.06260, saving model to ./model/40-0.0626.hdf5\n",
            "Epoch 41/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1409 - accuracy: 0.9686 - val_loss: 0.0614 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00041: val_loss improved from 0.06260 to 0.06136, saving model to ./model/41-0.0614.hdf5\n",
            "Epoch 42/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1175 - accuracy: 0.9590 - val_loss: 0.0827 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.06136\n",
            "Epoch 43/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1283 - accuracy: 0.9546 - val_loss: 0.0532 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00043: val_loss improved from 0.06136 to 0.05320, saving model to ./model/43-0.0532.hdf5\n",
            "Epoch 44/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1209 - accuracy: 0.9590 - val_loss: 0.0841 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.05320\n",
            "Epoch 45/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1135 - accuracy: 0.9799 - val_loss: 0.0500 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00045: val_loss improved from 0.05320 to 0.05005, saving model to ./model/45-0.0500.hdf5\n",
            "Epoch 46/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0873 - accuracy: 0.9869 - val_loss: 0.0447 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00046: val_loss improved from 0.05005 to 0.04467, saving model to ./model/46-0.0447.hdf5\n",
            "Epoch 47/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1537 - accuracy: 0.9396 - val_loss: 0.0499 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.04467\n",
            "Epoch 48/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0926 - accuracy: 0.9725 - val_loss: 0.0482 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.04467\n",
            "Epoch 49/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1139 - accuracy: 0.9790 - val_loss: 0.0474 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.04467\n",
            "Epoch 50/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1013 - accuracy: 0.9635 - val_loss: 0.0454 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.04467\n",
            "Epoch 51/100\n",
            "27/27 [==============================] - 0s 5ms/step - loss: 0.0909 - accuracy: 0.9780 - val_loss: 0.0388 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00051: val_loss improved from 0.04467 to 0.03882, saving model to ./model/51-0.0388.hdf5\n",
            "Epoch 52/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1165 - accuracy: 0.9762 - val_loss: 0.0420 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.03882\n",
            "Epoch 53/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1064 - accuracy: 0.9758 - val_loss: 0.0499 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.03882\n",
            "Epoch 54/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0728 - accuracy: 0.9949 - val_loss: 0.0366 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00054: val_loss improved from 0.03882 to 0.03660, saving model to ./model/54-0.0366.hdf5\n",
            "Epoch 55/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0890 - accuracy: 0.9787 - val_loss: 0.0315 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00055: val_loss improved from 0.03660 to 0.03150, saving model to ./model/55-0.0315.hdf5\n",
            "Epoch 56/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0941 - accuracy: 0.9655 - val_loss: 0.0536 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.03150\n",
            "Epoch 57/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0964 - accuracy: 0.9709 - val_loss: 0.0583 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.03150\n",
            "Epoch 58/100\n",
            "27/27 [==============================] - 0s 5ms/step - loss: 0.1117 - accuracy: 0.9592 - val_loss: 0.0315 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00058: val_loss improved from 0.03150 to 0.03146, saving model to ./model/58-0.0315.hdf5\n",
            "Epoch 59/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0931 - accuracy: 0.9761 - val_loss: 0.0542 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.03146\n",
            "Epoch 60/100\n",
            "27/27 [==============================] - 0s 6ms/step - loss: 0.1279 - accuracy: 0.9586 - val_loss: 0.0267 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00060: val_loss improved from 0.03146 to 0.02667, saving model to ./model/60-0.0267.hdf5\n",
            "Epoch 61/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1015 - accuracy: 0.9408 - val_loss: 0.0490 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.02667\n",
            "Epoch 62/100\n",
            "27/27 [==============================] - 0s 5ms/step - loss: 0.1007 - accuracy: 0.9632 - val_loss: 0.0371 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.02667\n",
            "Epoch 63/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0799 - accuracy: 0.9810 - val_loss: 0.0239 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00063: val_loss improved from 0.02667 to 0.02393, saving model to ./model/63-0.0239.hdf5\n",
            "Epoch 64/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0939 - accuracy: 0.9615 - val_loss: 0.0519 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.02393\n",
            "Epoch 65/100\n",
            "27/27 [==============================] - 0s 5ms/step - loss: 0.0758 - accuracy: 0.9868 - val_loss: 0.0236 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00065: val_loss improved from 0.02393 to 0.02358, saving model to ./model/65-0.0236.hdf5\n",
            "Epoch 66/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0741 - accuracy: 0.9930 - val_loss: 0.0349 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.02358\n",
            "Epoch 67/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0676 - accuracy: 0.9791 - val_loss: 0.0219 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00067: val_loss improved from 0.02358 to 0.02186, saving model to ./model/67-0.0219.hdf5\n",
            "Epoch 68/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0987 - accuracy: 0.9572 - val_loss: 0.0329 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.02186\n",
            "Epoch 69/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0854 - accuracy: 0.9703 - val_loss: 0.0271 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.02186\n",
            "Epoch 70/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0972 - accuracy: 0.9681 - val_loss: 0.0273 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.02186\n",
            "Epoch 71/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0972 - accuracy: 0.9374 - val_loss: 0.0294 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.02186\n",
            "Epoch 72/100\n",
            "27/27 [==============================] - 0s 6ms/step - loss: 0.0619 - accuracy: 0.9765 - val_loss: 0.0251 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.02186\n",
            "Epoch 73/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0914 - accuracy: 0.9733 - val_loss: 0.0348 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.02186\n",
            "Epoch 74/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0958 - accuracy: 0.9742 - val_loss: 0.0276 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.02186\n",
            "Epoch 75/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0605 - accuracy: 0.9865 - val_loss: 0.0316 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.02186\n",
            "Epoch 76/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0737 - accuracy: 0.9642 - val_loss: 0.0454 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.02186\n",
            "Epoch 77/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0752 - accuracy: 0.9791 - val_loss: 0.0163 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00077: val_loss improved from 0.02186 to 0.01633, saving model to ./model/77-0.0163.hdf5\n",
            "Epoch 78/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0858 - accuracy: 0.9670 - val_loss: 0.0174 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.01633\n",
            "Epoch 79/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0994 - accuracy: 0.9807 - val_loss: 0.0234 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.01633\n",
            "Epoch 80/100\n",
            "27/27 [==============================] - 0s 6ms/step - loss: 0.0713 - accuracy: 0.9901 - val_loss: 0.0216 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.01633\n",
            "Epoch 81/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0785 - accuracy: 0.9850 - val_loss: 0.0170 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.01633\n",
            "Epoch 82/100\n",
            "27/27 [==============================] - 0s 5ms/step - loss: 0.0776 - accuracy: 0.9704 - val_loss: 0.0288 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.01633\n",
            "Epoch 83/100\n",
            "27/27 [==============================] - 0s 5ms/step - loss: 0.1092 - accuracy: 0.9404 - val_loss: 0.0208 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.01633\n",
            "Epoch 84/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0807 - accuracy: 0.9718 - val_loss: 0.0187 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.01633\n",
            "Epoch 85/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0844 - accuracy: 0.9710 - val_loss: 0.0176 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.01633\n",
            "Epoch 86/100\n",
            "27/27 [==============================] - 0s 5ms/step - loss: 0.0535 - accuracy: 0.9883 - val_loss: 0.0220 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.01633\n",
            "Epoch 87/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0833 - accuracy: 0.9675 - val_loss: 0.0190 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.01633\n",
            "Epoch 88/100\n",
            "27/27 [==============================] - 0s 5ms/step - loss: 0.0634 - accuracy: 0.9791 - val_loss: 0.0147 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00088: val_loss improved from 0.01633 to 0.01465, saving model to ./model/88-0.0147.hdf5\n",
            "Epoch 89/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0700 - accuracy: 0.9683 - val_loss: 0.0138 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00089: val_loss improved from 0.01465 to 0.01381, saving model to ./model/89-0.0138.hdf5\n",
            "Epoch 90/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0767 - accuracy: 0.9702 - val_loss: 0.0239 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.01381\n",
            "Epoch 91/100\n",
            "27/27 [==============================] - 0s 5ms/step - loss: 0.0437 - accuracy: 0.9925 - val_loss: 0.0145 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.01381\n",
            "Epoch 92/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0861 - accuracy: 0.9641 - val_loss: 0.0130 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00092: val_loss improved from 0.01381 to 0.01300, saving model to ./model/92-0.0130.hdf5\n",
            "Epoch 93/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0981 - accuracy: 0.9664 - val_loss: 0.0168 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.01300\n",
            "Epoch 94/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0840 - accuracy: 0.9725 - val_loss: 0.0163 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.01300\n",
            "Epoch 95/100\n",
            "27/27 [==============================] - 0s 5ms/step - loss: 0.0675 - accuracy: 0.9751 - val_loss: 0.0163 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.01300\n",
            "Epoch 96/100\n",
            "27/27 [==============================] - 0s 5ms/step - loss: 0.0621 - accuracy: 0.9776 - val_loss: 0.0123 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00096: val_loss improved from 0.01300 to 0.01226, saving model to ./model/96-0.0123.hdf5\n",
            "Epoch 97/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1073 - accuracy: 0.9589 - val_loss: 0.0175 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.01226\n",
            "Epoch 98/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0639 - accuracy: 0.9787 - val_loss: 0.0135 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.01226\n",
            "Epoch 99/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0534 - accuracy: 0.9844 - val_loss: 0.0109 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00099: val_loss improved from 0.01226 to 0.01086, saving model to ./model/99-0.0109.hdf5\n",
            "Epoch 100/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0699 - accuracy: 0.9725 - val_loss: 0.0139 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.01086\n",
            "1/1 [==============================] - 0s 270ms/step - loss: 0.0139 - accuracy: 1.0000\n",
            "Epoch 1/100\n",
            "27/27 [==============================] - 1s 14ms/step - loss: 4.1442 - accuracy: 0.3231 - val_loss: 2.2410 - val_accuracy: 0.2000\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 2.24105, saving model to ./model/01-2.2410.hdf5\n",
            "Epoch 2/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 1.9873 - accuracy: 0.2390 - val_loss: 1.5380 - val_accuracy: 0.0000e+00\n",
            "\n",
            "Epoch 00002: val_loss improved from 2.24105 to 1.53799, saving model to ./model/02-1.5380.hdf5\n",
            "Epoch 3/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 1.4464 - accuracy: 0.0000e+00 - val_loss: 1.2217 - val_accuracy: 0.0667\n",
            "\n",
            "Epoch 00003: val_loss improved from 1.53799 to 1.22171, saving model to ./model/03-1.2217.hdf5\n",
            "Epoch 4/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 1.2048 - accuracy: 0.2678 - val_loss: 1.2008 - val_accuracy: 0.2667\n",
            "\n",
            "Epoch 00004: val_loss improved from 1.22171 to 1.20077, saving model to ./model/04-1.2008.hdf5\n",
            "Epoch 5/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 1.1678 - accuracy: 0.3351 - val_loss: 1.1909 - val_accuracy: 0.3333\n",
            "\n",
            "Epoch 00005: val_loss improved from 1.20077 to 1.19088, saving model to ./model/05-1.1909.hdf5\n",
            "Epoch 6/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 1.2158 - accuracy: 0.2178 - val_loss: 1.1692 - val_accuracy: 0.2000\n",
            "\n",
            "Epoch 00006: val_loss improved from 1.19088 to 1.16918, saving model to ./model/06-1.1692.hdf5\n",
            "Epoch 7/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 1.1802 - accuracy: 0.2887 - val_loss: 1.1574 - val_accuracy: 0.2000\n",
            "\n",
            "Epoch 00007: val_loss improved from 1.16918 to 1.15736, saving model to ./model/07-1.1574.hdf5\n",
            "Epoch 8/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 1.1358 - accuracy: 0.2617 - val_loss: 1.1466 - val_accuracy: 0.2667\n",
            "\n",
            "Epoch 00008: val_loss improved from 1.15736 to 1.14661, saving model to ./model/08-1.1466.hdf5\n",
            "Epoch 9/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 1.1153 - accuracy: 0.3906 - val_loss: 1.1355 - val_accuracy: 0.3333\n",
            "\n",
            "Epoch 00009: val_loss improved from 1.14661 to 1.13552, saving model to ./model/09-1.1355.hdf5\n",
            "Epoch 10/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 1.1130 - accuracy: 0.3607 - val_loss: 1.1263 - val_accuracy: 0.3333\n",
            "\n",
            "Epoch 00010: val_loss improved from 1.13552 to 1.12630, saving model to ./model/10-1.1263.hdf5\n",
            "Epoch 11/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 1.1379 - accuracy: 0.2551 - val_loss: 1.1183 - val_accuracy: 0.3333\n",
            "\n",
            "Epoch 00011: val_loss improved from 1.12630 to 1.11829, saving model to ./model/11-1.1183.hdf5\n",
            "Epoch 12/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 1.1279 - accuracy: 0.2993 - val_loss: 1.1090 - val_accuracy: 0.3333\n",
            "\n",
            "Epoch 00012: val_loss improved from 1.11829 to 1.10904, saving model to ./model/12-1.1090.hdf5\n",
            "Epoch 13/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 1.0956 - accuracy: 0.3539 - val_loss: 1.1014 - val_accuracy: 0.3333\n",
            "\n",
            "Epoch 00013: val_loss improved from 1.10904 to 1.10144, saving model to ./model/13-1.1014.hdf5\n",
            "Epoch 14/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 1.0899 - accuracy: 0.3338 - val_loss: 1.0951 - val_accuracy: 0.2667\n",
            "\n",
            "Epoch 00014: val_loss improved from 1.10144 to 1.09513, saving model to ./model/14-1.0951.hdf5\n",
            "Epoch 15/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 1.1018 - accuracy: 0.3531 - val_loss: 1.0895 - val_accuracy: 0.2667\n",
            "\n",
            "Epoch 00015: val_loss improved from 1.09513 to 1.08947, saving model to ./model/15-1.0895.hdf5\n",
            "Epoch 16/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 1.0696 - accuracy: 0.4338 - val_loss: 1.0808 - val_accuracy: 0.3333\n",
            "\n",
            "Epoch 00016: val_loss improved from 1.08947 to 1.08083, saving model to ./model/16-1.0808.hdf5\n",
            "Epoch 17/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 1.0466 - accuracy: 0.3915 - val_loss: 1.0744 - val_accuracy: 0.4000\n",
            "\n",
            "Epoch 00017: val_loss improved from 1.08083 to 1.07439, saving model to ./model/17-1.0744.hdf5\n",
            "Epoch 18/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 1.0549 - accuracy: 0.4205 - val_loss: 1.0615 - val_accuracy: 0.3333\n",
            "\n",
            "Epoch 00018: val_loss improved from 1.07439 to 1.06153, saving model to ./model/18-1.0615.hdf5\n",
            "Epoch 19/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 1.0318 - accuracy: 0.3570 - val_loss: 1.0480 - val_accuracy: 0.3333\n",
            "\n",
            "Epoch 00019: val_loss improved from 1.06153 to 1.04804, saving model to ./model/19-1.0480.hdf5\n",
            "Epoch 20/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 1.0238 - accuracy: 0.3830 - val_loss: 1.0353 - val_accuracy: 0.4000\n",
            "\n",
            "Epoch 00020: val_loss improved from 1.04804 to 1.03535, saving model to ./model/20-1.0353.hdf5\n",
            "Epoch 21/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 1.0159 - accuracy: 0.3421 - val_loss: 1.0187 - val_accuracy: 0.3333\n",
            "\n",
            "Epoch 00021: val_loss improved from 1.03535 to 1.01874, saving model to ./model/21-1.0187.hdf5\n",
            "Epoch 22/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.9879 - accuracy: 0.5649 - val_loss: 0.9992 - val_accuracy: 0.5333\n",
            "\n",
            "Epoch 00022: val_loss improved from 1.01874 to 0.99919, saving model to ./model/22-0.9992.hdf5\n",
            "Epoch 23/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.9855 - accuracy: 0.5459 - val_loss: 0.9794 - val_accuracy: 0.6667\n",
            "\n",
            "Epoch 00023: val_loss improved from 0.99919 to 0.97940, saving model to ./model/23-0.9794.hdf5\n",
            "Epoch 24/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.9468 - accuracy: 0.6921 - val_loss: 0.9578 - val_accuracy: 0.5333\n",
            "\n",
            "Epoch 00024: val_loss improved from 0.97940 to 0.95778, saving model to ./model/24-0.9578.hdf5\n",
            "Epoch 25/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.9224 - accuracy: 0.7262 - val_loss: 0.9321 - val_accuracy: 0.6667\n",
            "\n",
            "Epoch 00025: val_loss improved from 0.95778 to 0.93209, saving model to ./model/25-0.9321.hdf5\n",
            "Epoch 26/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.8914 - accuracy: 0.7025 - val_loss: 0.9082 - val_accuracy: 0.8000\n",
            "\n",
            "Epoch 00026: val_loss improved from 0.93209 to 0.90822, saving model to ./model/26-0.9082.hdf5\n",
            "Epoch 27/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.8756 - accuracy: 0.8302 - val_loss: 0.8791 - val_accuracy: 0.6667\n",
            "\n",
            "Epoch 00027: val_loss improved from 0.90822 to 0.87909, saving model to ./model/27-0.8791.hdf5\n",
            "Epoch 28/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.8518 - accuracy: 0.6881 - val_loss: 0.8533 - val_accuracy: 0.8667\n",
            "\n",
            "Epoch 00028: val_loss improved from 0.87909 to 0.85329, saving model to ./model/28-0.8533.hdf5\n",
            "Epoch 29/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.8011 - accuracy: 0.8702 - val_loss: 0.8163 - val_accuracy: 0.6667\n",
            "\n",
            "Epoch 00029: val_loss improved from 0.85329 to 0.81627, saving model to ./model/29-0.8163.hdf5\n",
            "Epoch 30/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.8033 - accuracy: 0.7656 - val_loss: 0.7859 - val_accuracy: 0.6667\n",
            "\n",
            "Epoch 00030: val_loss improved from 0.81627 to 0.78587, saving model to ./model/30-0.7859.hdf5\n",
            "Epoch 31/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.7420 - accuracy: 0.7383 - val_loss: 0.7570 - val_accuracy: 0.8667\n",
            "\n",
            "Epoch 00031: val_loss improved from 0.78587 to 0.75702, saving model to ./model/31-0.7570.hdf5\n",
            "Epoch 32/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.7119 - accuracy: 0.8547 - val_loss: 0.7195 - val_accuracy: 0.8000\n",
            "\n",
            "Epoch 00032: val_loss improved from 0.75702 to 0.71951, saving model to ./model/32-0.7195.hdf5\n",
            "Epoch 33/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.6909 - accuracy: 0.7591 - val_loss: 0.6875 - val_accuracy: 0.8667\n",
            "\n",
            "Epoch 00033: val_loss improved from 0.71951 to 0.68747, saving model to ./model/33-0.6875.hdf5\n",
            "Epoch 34/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.6437 - accuracy: 0.8714 - val_loss: 0.6552 - val_accuracy: 0.8000\n",
            "\n",
            "Epoch 00034: val_loss improved from 0.68747 to 0.65518, saving model to ./model/34-0.6552.hdf5\n",
            "Epoch 35/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.6044 - accuracy: 0.8181 - val_loss: 0.6235 - val_accuracy: 0.8667\n",
            "\n",
            "Epoch 00035: val_loss improved from 0.65518 to 0.62351, saving model to ./model/35-0.6235.hdf5\n",
            "Epoch 36/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.5723 - accuracy: 0.9167 - val_loss: 0.5946 - val_accuracy: 0.8667\n",
            "\n",
            "Epoch 00036: val_loss improved from 0.62351 to 0.59461, saving model to ./model/36-0.5946.hdf5\n",
            "Epoch 37/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.5293 - accuracy: 0.9573 - val_loss: 0.5657 - val_accuracy: 0.8667\n",
            "\n",
            "Epoch 00037: val_loss improved from 0.59461 to 0.56572, saving model to ./model/37-0.5657.hdf5\n",
            "Epoch 38/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.5343 - accuracy: 0.8773 - val_loss: 0.5421 - val_accuracy: 0.8667\n",
            "\n",
            "Epoch 00038: val_loss improved from 0.56572 to 0.54208, saving model to ./model/38-0.5421.hdf5\n",
            "Epoch 39/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.5303 - accuracy: 0.8903 - val_loss: 0.5174 - val_accuracy: 0.8667\n",
            "\n",
            "Epoch 00039: val_loss improved from 0.54208 to 0.51744, saving model to ./model/39-0.5174.hdf5\n",
            "Epoch 40/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.4879 - accuracy: 0.8916 - val_loss: 0.4941 - val_accuracy: 0.8667\n",
            "\n",
            "Epoch 00040: val_loss improved from 0.51744 to 0.49413, saving model to ./model/40-0.4941.hdf5\n",
            "Epoch 41/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.4906 - accuracy: 0.9047 - val_loss: 0.4719 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00041: val_loss improved from 0.49413 to 0.47193, saving model to ./model/41-0.4719.hdf5\n",
            "Epoch 42/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.4303 - accuracy: 0.9505 - val_loss: 0.4561 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00042: val_loss improved from 0.47193 to 0.45608, saving model to ./model/42-0.4561.hdf5\n",
            "Epoch 43/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.4249 - accuracy: 0.9353 - val_loss: 0.4340 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00043: val_loss improved from 0.45608 to 0.43402, saving model to ./model/43-0.4340.hdf5\n",
            "Epoch 44/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.3668 - accuracy: 0.9509 - val_loss: 0.4268 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00044: val_loss improved from 0.43402 to 0.42685, saving model to ./model/44-0.4268.hdf5\n",
            "Epoch 45/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.3788 - accuracy: 0.9749 - val_loss: 0.3976 - val_accuracy: 0.8667\n",
            "\n",
            "Epoch 00045: val_loss improved from 0.42685 to 0.39755, saving model to ./model/45-0.3976.hdf5\n",
            "Epoch 46/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.3788 - accuracy: 0.9591 - val_loss: 0.3858 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00046: val_loss improved from 0.39755 to 0.38585, saving model to ./model/46-0.3858.hdf5\n",
            "Epoch 47/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.3561 - accuracy: 0.9698 - val_loss: 0.3688 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00047: val_loss improved from 0.38585 to 0.36884, saving model to ./model/47-0.3688.hdf5\n",
            "Epoch 48/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.3369 - accuracy: 0.9694 - val_loss: 0.3585 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00048: val_loss improved from 0.36884 to 0.35851, saving model to ./model/48-0.3585.hdf5\n",
            "Epoch 49/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.3114 - accuracy: 0.9661 - val_loss: 0.3740 - val_accuracy: 0.8667\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.35851\n",
            "Epoch 50/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.3122 - accuracy: 0.9453 - val_loss: 0.3320 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00050: val_loss improved from 0.35851 to 0.33203, saving model to ./model/50-0.3320.hdf5\n",
            "Epoch 51/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.3073 - accuracy: 0.9594 - val_loss: 0.3437 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.33203\n",
            "Epoch 52/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.3011 - accuracy: 0.9585 - val_loss: 0.3183 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00052: val_loss improved from 0.33203 to 0.31828, saving model to ./model/52-0.3183.hdf5\n",
            "Epoch 53/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.2742 - accuracy: 0.9497 - val_loss: 0.3011 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00053: val_loss improved from 0.31828 to 0.30106, saving model to ./model/53-0.3011.hdf5\n",
            "Epoch 54/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.2597 - accuracy: 0.9776 - val_loss: 0.2841 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00054: val_loss improved from 0.30106 to 0.28412, saving model to ./model/54-0.2841.hdf5\n",
            "Epoch 55/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.2738 - accuracy: 0.9463 - val_loss: 0.2707 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00055: val_loss improved from 0.28412 to 0.27066, saving model to ./model/55-0.2707.hdf5\n",
            "Epoch 56/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.2502 - accuracy: 0.9802 - val_loss: 0.2646 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00056: val_loss improved from 0.27066 to 0.26464, saving model to ./model/56-0.2646.hdf5\n",
            "Epoch 57/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.2130 - accuracy: 0.9789 - val_loss: 0.2630 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00057: val_loss improved from 0.26464 to 0.26296, saving model to ./model/57-0.2630.hdf5\n",
            "Epoch 58/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.2292 - accuracy: 0.9758 - val_loss: 0.2528 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00058: val_loss improved from 0.26296 to 0.25277, saving model to ./model/58-0.2528.hdf5\n",
            "Epoch 59/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.2113 - accuracy: 0.9735 - val_loss: 0.2550 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.25277\n",
            "Epoch 60/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1906 - accuracy: 0.9875 - val_loss: 0.2488 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00060: val_loss improved from 0.25277 to 0.24885, saving model to ./model/60-0.2488.hdf5\n",
            "Epoch 61/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.2573 - accuracy: 0.9425 - val_loss: 0.2167 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00061: val_loss improved from 0.24885 to 0.21667, saving model to ./model/61-0.2167.hdf5\n",
            "Epoch 62/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.2048 - accuracy: 0.9653 - val_loss: 0.2288 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.21667\n",
            "Epoch 63/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.2197 - accuracy: 0.9521 - val_loss: 0.2177 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.21667\n",
            "Epoch 64/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1830 - accuracy: 0.9785 - val_loss: 0.2305 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.21667\n",
            "Epoch 65/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.2063 - accuracy: 0.9536 - val_loss: 0.2683 - val_accuracy: 0.8667\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.21667\n",
            "Epoch 66/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1827 - accuracy: 0.9749 - val_loss: 0.1874 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00066: val_loss improved from 0.21667 to 0.18737, saving model to ./model/66-0.1874.hdf5\n",
            "Epoch 67/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1676 - accuracy: 0.9829 - val_loss: 0.1798 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00067: val_loss improved from 0.18737 to 0.17980, saving model to ./model/67-0.1798.hdf5\n",
            "Epoch 68/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1594 - accuracy: 0.9863 - val_loss: 0.1803 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.17980\n",
            "Epoch 69/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1468 - accuracy: 0.9866 - val_loss: 0.1803 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.17980\n",
            "Epoch 70/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1303 - accuracy: 0.9891 - val_loss: 0.1802 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.17980\n",
            "Epoch 71/100\n",
            "27/27 [==============================] - 0s 5ms/step - loss: 0.2147 - accuracy: 0.9353 - val_loss: 0.1557 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00071: val_loss improved from 0.17980 to 0.15569, saving model to ./model/71-0.1557.hdf5\n",
            "Epoch 72/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1673 - accuracy: 0.9597 - val_loss: 0.2333 - val_accuracy: 0.8667\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.15569\n",
            "Epoch 73/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1457 - accuracy: 0.9776 - val_loss: 0.1442 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00073: val_loss improved from 0.15569 to 0.14425, saving model to ./model/73-0.1442.hdf5\n",
            "Epoch 74/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1137 - accuracy: 0.9841 - val_loss: 0.1811 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.14425\n",
            "Epoch 75/100\n",
            "27/27 [==============================] - 0s 5ms/step - loss: 0.1758 - accuracy: 0.9409 - val_loss: 0.1467 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.14425\n",
            "Epoch 76/100\n",
            "27/27 [==============================] - 0s 5ms/step - loss: 0.1080 - accuracy: 0.9792 - val_loss: 0.1589 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.14425\n",
            "Epoch 77/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1498 - accuracy: 0.9680 - val_loss: 0.1328 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00077: val_loss improved from 0.14425 to 0.13281, saving model to ./model/77-0.1328.hdf5\n",
            "Epoch 78/100\n",
            "27/27 [==============================] - 0s 2ms/step - loss: 0.1463 - accuracy: 0.9446 - val_loss: 0.1230 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00078: val_loss improved from 0.13281 to 0.12305, saving model to ./model/78-0.1230.hdf5\n",
            "Epoch 79/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1233 - accuracy: 0.9682 - val_loss: 0.1575 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.12305\n",
            "Epoch 80/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1302 - accuracy: 0.9808 - val_loss: 0.2147 - val_accuracy: 0.8667\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.12305\n",
            "Epoch 81/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1439 - accuracy: 0.9516 - val_loss: 0.1257 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.12305\n",
            "Epoch 82/100\n",
            "27/27 [==============================] - 0s 6ms/step - loss: 0.1215 - accuracy: 0.9659 - val_loss: 0.1138 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00082: val_loss improved from 0.12305 to 0.11377, saving model to ./model/82-0.1138.hdf5\n",
            "Epoch 83/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1422 - accuracy: 0.9472 - val_loss: 0.1325 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.11377\n",
            "Epoch 84/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.0956 - accuracy: 0.9945 - val_loss: 0.1189 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.11377\n",
            "Epoch 85/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1154 - accuracy: 0.9760 - val_loss: 0.1281 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.11377\n",
            "Epoch 86/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1046 - accuracy: 0.9700 - val_loss: 0.1134 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00086: val_loss improved from 0.11377 to 0.11340, saving model to ./model/86-0.1134.hdf5\n",
            "Epoch 87/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1120 - accuracy: 0.9712 - val_loss: 0.1266 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.11340\n",
            "Epoch 88/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1004 - accuracy: 0.9843 - val_loss: 0.2056 - val_accuracy: 0.8667\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.11340\n",
            "Epoch 89/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1049 - accuracy: 0.9946 - val_loss: 0.1194 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.11340\n",
            "Epoch 90/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1175 - accuracy: 0.9738 - val_loss: 0.1491 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.11340\n",
            "Epoch 91/100\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.1228 - accuracy: 0.9542 - val_loss: 0.0914 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00091: val_loss improved from 0.11340 to 0.09140, saving model to ./model/91-0.0914.hdf5\n",
            "Epoch 92/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0954 - accuracy: 0.9833 - val_loss: 0.1389 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.09140\n",
            "Epoch 93/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0954 - accuracy: 0.9697 - val_loss: 0.1278 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.09140\n",
            "Epoch 94/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.1254 - accuracy: 0.9370 - val_loss: 0.1241 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.09140\n",
            "Epoch 95/100\n",
            "27/27 [==============================] - 0s 5ms/step - loss: 0.1313 - accuracy: 0.9521 - val_loss: 0.1669 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.09140\n",
            "Epoch 96/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0871 - accuracy: 0.9749 - val_loss: 0.0897 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00096: val_loss improved from 0.09140 to 0.08967, saving model to ./model/96-0.0897.hdf5\n",
            "Epoch 97/100\n",
            "27/27 [==============================] - 0s 5ms/step - loss: 0.0792 - accuracy: 0.9886 - val_loss: 0.1100 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.08967\n",
            "Epoch 98/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0811 - accuracy: 0.9851 - val_loss: 0.1083 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.08967\n",
            "Epoch 99/100\n",
            "27/27 [==============================] - 0s 5ms/step - loss: 0.0917 - accuracy: 0.9809 - val_loss: 0.0941 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.08967\n",
            "Epoch 100/100\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.0992 - accuracy: 0.9815 - val_loss: 0.1254 - val_accuracy: 0.9333\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.08967\n",
            "1/1 [==============================] - 0s 307ms/step - loss: 0.1254 - accuracy: 0.9333\n",
            "\n",
            " 10 fold accuracy:  ['1.0000', '0.9333', '1.0000', '1.0000', '0.9333', '0.9333', '1.0000', '1.0000', '1.0000', '0.9333']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9oMzXRDvDD20",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64269bbe-ec51-4883-ca56-882b0efa7659"
      },
      "source": [
        "cd /content/drive/MyDrive/인공지능실습/모두의 딥러닝/model/"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/인공지능실습/모두의 딥러닝/model\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "41SaqoagZOYc",
        "outputId": "901c2d7b-9a27-44bf-8316-801ff787f638"
      },
      "source": [
        "!ls #생성된 모델들 확인하기."
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "01-0.9244.hdf5\t 12-0.4023.hdf5  23-0.3015.hdf5  37-0.1160.hdf5  59-0.0490.hdf5\n",
            "01-1.0135.hdf5\t 12-0.4080.hdf5  23-0.9794.hdf5  37-0.1335.hdf5  59-0.0900.hdf5\n",
            "01-1.0789.hdf5\t 12-0.4318.hdf5  24-0.1642.hdf5  37-0.1432.hdf5  60-0.0267.hdf5\n",
            "01-1.1689.hdf5\t 12-0.5246.hdf5  24-0.1721.hdf5  37-0.1906.hdf5  60-0.0518.hdf5\n",
            "01-1.2067.hdf5\t 12-0.5900.hdf5  24-0.1898.hdf5  37-0.1986.hdf5  60-0.0707.hdf5\n",
            "01-1.2164.hdf5\t 12-0.6017.hdf5  24-0.2299.hdf5  37-0.5657.hdf5  60-0.0892.hdf5\n",
            "01-1.4894.hdf5\t 12-1.1090.hdf5  24-0.2302.hdf5  38-0.0689.hdf5  60-0.0898.hdf5\n",
            "01-1.7379.hdf5\t 13-0.3426.hdf5  24-0.2805.hdf5  38-0.0795.hdf5  60-0.2488.hdf5\n",
            "01-2.2410.hdf5\t 13-0.3667.hdf5  24-0.2958.hdf5  38-0.1151.hdf5  61-0.0649.hdf5\n",
            "02-0.8397.hdf5\t 13-0.3814.hdf5  24-0.9578.hdf5  38-0.1363.hdf5  61-0.0885.hdf5\n",
            "02-0.9149.hdf5\t 13-0.3827.hdf5  25-0.1525.hdf5  38-0.1669.hdf5  61-0.1205.hdf5\n",
            "02-0.9406.hdf5\t 13-0.4113.hdf5  25-0.1570.hdf5  38-0.1753.hdf5  61-0.1352.hdf5\n",
            "02-0.9552.hdf5\t 13-0.4868.hdf5  25-0.1878.hdf5  38-0.1811.hdf5  61-0.2167.hdf5\n",
            "02-0.9744.hdf5\t 13-0.5568.hdf5  25-0.2161.hdf5  38-0.5421.hdf5  62-0.0876.hdf5\n",
            "02-1.0584.hdf5\t 13-0.5759.hdf5  25-0.2180.hdf5  39-0.1035.hdf5  63-0.0239.hdf5\n",
            "02-1.0623.hdf5\t 13-1.1014.hdf5  25-0.2568.hdf5  39-0.1076.hdf5  63-0.0629.hdf5\n",
            "02-1.2875.hdf5\t 14-0.3137.hdf5  25-0.2666.hdf5  39-0.1267.hdf5  64-0.0503.hdf5\n",
            "02-1.5380.hdf5\t 14-0.3422.hdf5  25-0.2858.hdf5  39-0.1324.hdf5  64-0.0867.hdf5\n",
            "03-0.7646.hdf5\t 14-0.3477.hdf5  25-0.9321.hdf5  39-0.5174.hdf5  64-0.1139.hdf5\n",
            "03-0.7820.hdf5\t 14-0.3624.hdf5  26-0.1511.hdf5  40-0.0626.hdf5  65-0.0236.hdf5\n",
            "03-0.8533.hdf5\t 14-0.3955.hdf5  26-0.1540.hdf5  40-0.0972.hdf5  65-0.0476.hdf5\n",
            "03-0.8938.hdf5\t 14-0.4534.hdf5  26-0.2008.hdf5  40-0.1027.hdf5  65-0.0779.hdf5\n",
            "03-0.9133.hdf5\t 14-0.5261.hdf5  26-0.2039.hdf5  40-0.1241.hdf5  65-0.0857.hdf5\n",
            "03-0.9657.hdf5\t 14-0.5477.hdf5  26-0.2403.hdf5  40-0.1569.hdf5  66-0.0460.hdf5\n",
            "03-0.9699.hdf5\t 14-1.0951.hdf5  26-0.2555.hdf5  40-0.1675.hdf5  66-0.1874.hdf5\n",
            "03-1.0970.hdf5\t 15-0.2866.hdf5  26-0.2731.hdf5  40-0.4941.hdf5  67-0.0219.hdf5\n",
            "03-1.2217.hdf5\t 15-0.3159.hdf5  26-0.9082.hdf5  41-0.0614.hdf5  67-0.0432.hdf5\n",
            "04-0.6996.hdf5\t 15-0.3263.hdf5  27-0.1416.hdf5  41-0.1251.hdf5  67-0.0823.hdf5\n",
            "04-0.7066.hdf5\t 15-0.3455.hdf5  27-0.1916.hdf5  41-0.1611.hdf5  67-0.1798.hdf5\n",
            "04-0.7646.hdf5\t 15-0.3860.hdf5  27-0.1921.hdf5  41-0.1692.hdf5  68-0.0402.hdf5\n",
            "04-0.8441.hdf5\t 15-0.4175.hdf5  27-0.2337.hdf5  41-0.4719.hdf5  68-0.0741.hdf5\n",
            "04-0.8618.hdf5\t 15-0.4980.hdf5  27-0.2449.hdf5  42-0.1108.hdf5  68-0.1170.hdf5\n",
            "04-0.8907.hdf5\t 15-0.5195.hdf5  27-0.8791.hdf5  42-0.1514.hdf5  69-0.0431.hdf5\n",
            "04-0.8968.hdf5\t 15-1.0895.hdf5  28-0.1416.hdf5  42-0.1582.hdf5  69-0.0580.hdf5\n",
            "04-0.9998.hdf5\t 16-0.2659.hdf5  28-0.1837.hdf5  42-0.4561.hdf5  69-0.0708.hdf5\n",
            "04-1.2008.hdf5\t 16-0.2777.hdf5  28-0.1866.hdf5  43-0.0532.hdf5  70-0.0404.hdf5\n",
            "05-0.6385.hdf5\t 16-0.3073.hdf5  28-0.2179.hdf5  43-0.0909.hdf5  70-0.0570.hdf5\n",
            "05-0.6553.hdf5\t 16-0.3364.hdf5  28-0.2260.hdf5  43-0.1162.hdf5  70-0.0686.hdf5\n",
            "05-0.6931.hdf5\t 16-0.3696.hdf5  28-0.2619.hdf5  43-0.1514.hdf5  70-0.0801.hdf5\n",
            "05-0.7957.hdf5\t 16-0.3968.hdf5  28-0.8533.hdf5  43-0.4340.hdf5  71-0.0670.hdf5\n",
            "05-0.8043.hdf5\t 16-0.4732.hdf5  29-0.1149.hdf5  44-0.0880.hdf5  71-0.1143.hdf5\n",
            "05-0.8263.hdf5\t 16-0.4886.hdf5  29-0.1373.hdf5  44-0.1483.hdf5  71-0.1557.hdf5\n",
            "05-0.8304.hdf5\t 16-1.0808.hdf5  29-0.1410.hdf5  44-0.4268.hdf5  72-0.0388.hdf5\n",
            "05-0.9353.hdf5\t 17-0.2470.hdf5  29-0.1723.hdf5  45-0.0500.hdf5  72-0.0800.hdf5\n",
            "05-1.1909.hdf5\t 17-0.2725.hdf5  29-0.1793.hdf5  45-0.0878.hdf5  73-0.0575.hdf5\n",
            "06-0.5901.hdf5\t 17-0.2923.hdf5  29-0.2056.hdf5  45-0.1019.hdf5  73-0.1128.hdf5\n",
            "06-0.6078.hdf5\t 17-0.3243.hdf5  29-0.2546.hdf5  45-0.1120.hdf5  73-0.1442.hdf5\n",
            "06-0.6340.hdf5\t 17-0.3611.hdf5  29-0.2614.hdf5  45-0.1386.hdf5  74-0.0367.hdf5\n",
            "06-0.7351.hdf5\t 17-0.3642.hdf5  29-0.8163.hdf5  45-0.3976.hdf5  74-0.0631.hdf5\n",
            "06-0.7465.hdf5\t 17-0.4510.hdf5  30-0.1085.hdf5  46-0.0447.hdf5  75-0.0618.hdf5\n",
            "06-0.7703.hdf5\t 17-0.4569.hdf5  30-0.1290.hdf5  46-0.0828.hdf5  76-0.0353.hdf5\n",
            "06-0.7755.hdf5\t 17-1.0744.hdf5  30-0.1689.hdf5  46-0.1551.hdf5  76-0.0611.hdf5\n",
            "06-0.8749.hdf5\t 18-0.2349.hdf5  30-0.1694.hdf5  46-0.3858.hdf5  76-0.0761.hdf5\n",
            "06-1.1692.hdf5\t 18-0.2493.hdf5  30-0.1994.hdf5  47-0.0844.hdf5  77-0.0163.hdf5\n",
            "07-0.5461.hdf5\t 18-0.2702.hdf5  30-0.2157.hdf5  47-0.0948.hdf5  77-0.0346.hdf5\n",
            "07-0.5585.hdf5\t 18-0.3081.hdf5  30-0.2356.hdf5  47-0.1115.hdf5  77-0.0503.hdf5\n",
            "07-0.5807.hdf5\t 18-0.3365.hdf5  30-0.7859.hdf5  47-0.1336.hdf5  77-0.0597.hdf5\n",
            "07-0.6643.hdf5\t 18-0.3478.hdf5  31-0.1230.hdf5  47-0.1497.hdf5  77-0.1328.hdf5\n",
            "07-0.6761.hdf5\t 18-0.4187.hdf5  31-0.1342.hdf5  47-0.3688.hdf5  78-0.1230.hdf5\n",
            "07-0.7228.hdf5\t 18-0.4242.hdf5  31-0.1568.hdf5  48-0.0787.hdf5  79-0.0341.hdf5\n",
            "07-0.7402.hdf5\t 18-1.0615.hdf5  31-0.1681.hdf5  48-0.1105.hdf5  79-0.0738.hdf5\n",
            "07-0.8190.hdf5\t 19-0.2236.hdf5  31-0.1822.hdf5  48-0.3585.hdf5  80-0.0594.hdf5\n",
            "07-1.1574.hdf5\t 19-0.2337.hdf5  31-0.2077.hdf5  49-0.0679.hdf5  80-0.1038.hdf5\n",
            "08-0.5115.hdf5\t 19-0.2637.hdf5  31-0.2293.hdf5  49-0.0760.hdf5  80-0.1085.hdf5\n",
            "08-0.5172.hdf5\t 19-0.2831.hdf5  31-0.2469.hdf5  49-0.1039.hdf5  81-0.0299.hdf5\n",
            "08-0.5401.hdf5\t 19-0.3147.hdf5  31-0.7570.hdf5  49-0.1460.hdf5  81-0.0316.hdf5\n",
            "08-0.5886.hdf5\t 19-0.3371.hdf5  32-0.0943.hdf5  50-0.1204.hdf5  81-0.0585.hdf5\n",
            "08-0.6072.hdf5\t 19-0.3868.hdf5  32-0.1198.hdf5  50-0.3320.hdf5  81-0.0722.hdf5\n",
            "08-0.6772.hdf5\t 19-0.3944.hdf5  32-0.1473.hdf5  51-0.0388.hdf5  82-0.0312.hdf5\n",
            "08-0.7099.hdf5\t 19-1.0480.hdf5  32-0.1742.hdf5  51-0.0692.hdf5  82-0.1138.hdf5\n",
            "08-0.7718.hdf5\t 20-0.2031.hdf5  32-0.1972.hdf5  51-0.0763.hdf5  84-0.0531.hdf5\n",
            "08-1.1466.hdf5\t 20-0.2241.hdf5  32-0.2273.hdf5  51-0.1401.hdf5  84-0.0534.hdf5\n",
            "09-0.4754.hdf5\t 20-0.2350.hdf5  32-0.2288.hdf5  52-0.0616.hdf5  85-0.0295.hdf5\n",
            "09-0.4785.hdf5\t 20-0.2696.hdf5  32-0.7195.hdf5  52-0.0687.hdf5  85-0.0524.hdf5\n",
            "09-0.5060.hdf5\t 20-0.2973.hdf5  33-0.1169.hdf5  52-0.0745.hdf5  86-0.0467.hdf5\n",
            "09-0.5191.hdf5\t 20-0.3362.hdf5  33-0.1218.hdf5  52-0.3183.hdf5  86-0.0517.hdf5\n",
            "09-0.5670.hdf5\t 20-0.3582.hdf5  33-0.1405.hdf5  53-0.0811.hdf5  86-0.0698.hdf5\n",
            "09-0.6349.hdf5\t 20-0.3676.hdf5  33-0.1610.hdf5  53-0.0987.hdf5  86-0.1134.hdf5\n",
            "09-0.6838.hdf5\t 20-1.0353.hdf5  33-0.1724.hdf5  53-0.1085.hdf5  88-0.0147.hdf5\n",
            "09-0.7240.hdf5\t 21-0.1967.hdf5  33-0.1958.hdf5  53-0.1455.hdf5  88-0.0497.hdf5\n",
            "09-1.1355.hdf5\t 21-0.2005.hdf5  33-0.2209.hdf5  53-0.3011.hdf5  89-0.0138.hdf5\n",
            "100-0.0622.hdf5  21-0.2200.hdf5  33-0.6875.hdf5  54-0.0366.hdf5  89-0.0485.hdf5\n",
            "10-0.4432.hdf5\t 21-0.2583.hdf5  34-0.0895.hdf5  54-0.0559.hdf5  90-0.0271.hdf5\n",
            "10-0.4488.hdf5\t 21-0.2770.hdf5  34-0.1059.hdf5  54-0.0641.hdf5  91-0.0914.hdf5\n",
            "10-0.4674.hdf5\t 21-0.3175.hdf5  34-0.1552.hdf5  54-0.0715.hdf5  92-0.0130.hdf5\n",
            "10-0.4749.hdf5\t 21-0.3312.hdf5  34-0.1554.hdf5  54-0.0964.hdf5  92-0.0260.hdf5\n",
            "10-0.4916.hdf5\t 21-0.3454.hdf5  34-0.1803.hdf5  54-0.1067.hdf5  92-0.0660.hdf5\n",
            "10-0.5974.hdf5\t 21-1.0187.hdf5  34-0.2101.hdf5  54-0.2841.hdf5  93-0.0475.hdf5\n",
            "10-0.6559.hdf5\t 22-0.2151.hdf5  34-0.2138.hdf5  55-0.0315.hdf5  94-0.0972.hdf5\n",
            "10-0.6749.hdf5\t 22-0.2464.hdf5  34-0.6552.hdf5  55-0.0735.hdf5  96-0.0123.hdf5\n",
            "10-1.1263.hdf5\t 22-0.2569.hdf5  35-0.1330.hdf5  55-0.2707.hdf5  96-0.0244.hdf5\n",
            "11-0.4129.hdf5\t 22-0.3107.hdf5  35-0.1486.hdf5  56-0.1274.hdf5  96-0.0464.hdf5\n",
            "11-0.4178.hdf5\t 22-0.3134.hdf5  35-0.6235.hdf5  56-0.2646.hdf5  96-0.0657.hdf5\n",
            "11-0.4220.hdf5\t 22-0.3160.hdf5  36-0.0874.hdf5  57-0.0571.hdf5  96-0.0897.hdf5\n",
            "11-0.4443.hdf5\t 22-0.9992.hdf5  36-0.1145.hdf5  57-0.0959.hdf5  97-0.0443.hdf5\n",
            "11-0.4485.hdf5\t 23-0.1752.hdf5  36-0.1412.hdf5  57-0.2630.hdf5  97-0.1022.hdf5\n",
            "11-0.5609.hdf5\t 23-0.1840.hdf5  36-0.1483.hdf5  58-0.0315.hdf5  98-0.0454.hdf5\n",
            "11-0.6286.hdf5\t 23-0.1990.hdf5  36-0.1740.hdf5  58-0.0553.hdf5  98-0.0640.hdf5\n",
            "11-0.6304.hdf5\t 23-0.2360.hdf5  36-0.2001.hdf5  58-0.0555.hdf5  99-0.0109.hdf5\n",
            "11-1.1183.hdf5\t 23-0.2505.hdf5  36-0.2065.hdf5  58-0.0922.hdf5\n",
            "12-0.3737.hdf5\t 23-0.2917.hdf5  36-0.5946.hdf5  58-0.1018.hdf5\n",
            "12-0.3867.hdf5\t 23-0.2925.hdf5  37-0.1140.hdf5  58-0.2528.hdf5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZRtiEXv3ZYKF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}